[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yekta Amirkhalili - Dissertation Website",
    "section": "",
    "text": "Hi! Welcome to my dissertation website. I started my PhD in Management Science & Engineering at University of Waterloo in August 2021. This website will walk you through my entire project, which took me over 3 years to finish. I’m hoping this project can be used by others as inspiration, and hopefully, as a portfolio project for my own! I find a lot of this stuff fun! I’m also hoping to make everything super accessible to everyone, even if you’re coming from no background in Statistics/Math/Computer Science.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My Name is Yekta (read exactly how you write it), my pronouns are She/Her. I was born in Tehran, Iran. Since I was a kid, I’ve always loved learning and adventures. Movies are my first passion! After that, it’s TV shows, music, gadgets and technology, books, writing and studying! Yes! I actually like studying!\nI quickly found out I was much better at Math and Physics than Science and any other field that required memorizing facts (History). I wanted to be an astronaut, then an actress, then a filmmaker, then a writer, then a super model (plans didn’t work out, I stopped growing taller at 14!), then a robotics engineer, then a computer engineer, then an electrical engineer, then a photographer, etc. I thought actor was the one that made the most sense, since I wanted to be so many things! At least with acting, I could pretend to be all of those things!\nWell, I chose mathematics in highschool, and since I was good at it, I continued with mathematics to college. Big Mistake (For Me)! Studying math in undergrad at University of Tehran was like studying Chinese, Greek, Russian and Arabic at the same time. You don’t see numbers after Calculus 2, which you take in the second semester/term. All abstract ideas, proofs, theorems, n-dimensional spaces and curves that curve inside of themselves! “I should have studied Accounting!” (Which I did, btw! I actually took an entire accounting training course outside of university and found it to be sould-crushingly boring, no offense to all accountants.)\nOk, so if math isn’t about numbers and I don’t actually want to work with numbers - then where do I go?! So, thankfully, at UT, I was able to take most of my electives courses from the two other majors in the math department: computer science and statistics. This was the turning point for me. “Here’s where that theory applies, you know, the one you kept asking why you’re even learning its proof”… So I figured I should change my major. Mathematics had taught me nothing but how to think, how to analyze, how to be logical and how important it was to know why stuff made sense… so really, nothing much! I didn’t even realize how much studying mathematics had taught me until years later.\nI still changed my major for graduate school, and decided to try Industrial & Systems Engineering, with a specialization on Macrosystems engineering. I taught myself Python and SQL, I did very well during graduate school with my courses, I worked on research projects and had the best time writing my thesis. So I thought, wow, I actually like all of this! Why not do a PhD? And here I am! I love learning, getting deep in a topic, planning a project from the start to the end."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hi! My Name is Yekta (read exactly how you write it), my pronouns are She/Her. I was born in Tehran, Iran. Since I was a kid, I’ve always loved learning and adventures. Movies are my first passion! After that, it’s TV shows, music, gadgets and technology, books, writing and studying! Yes! I actually like studying!\nI quickly found out I was much better at Math and Physics than Science and any other field that required memorizing facts (History). I wanted to be an astronaut, then an actress, then a filmmaker, then a writer, then a super model (plans didn’t work out, I stopped growing taller at 14!), then a robotics engineer, then a computer engineer, then an electrical engineer, then a photographer, etc. I thought actor was the one that made the most sense, since I wanted to be so many things! At least with acting, I could pretend to be all of those things!\nWell, I chose mathematics in highschool, and since I was good at it, I continued with mathematics to college. Big Mistake (For Me)! Studying math in undergrad at University of Tehran was like studying Chinese, Greek, Russian and Arabic at the same time. You don’t see numbers after Calculus 2, which you take in the second semester/term. All abstract ideas, proofs, theorems, n-dimensional spaces and curves that curve inside of themselves! “I should have studied Accounting!” (Which I did, btw! I actually took an entire accounting training course outside of university and found it to be sould-crushingly boring, no offense to all accountants.)\nOk, so if math isn’t about numbers and I don’t actually want to work with numbers - then where do I go?! So, thankfully, at UT, I was able to take most of my electives courses from the two other majors in the math department: computer science and statistics. This was the turning point for me. “Here’s where that theory applies, you know, the one you kept asking why you’re even learning its proof”… So I figured I should change my major. Mathematics had taught me nothing but how to think, how to analyze, how to be logical and how important it was to know why stuff made sense… so really, nothing much! I didn’t even realize how much studying mathematics had taught me until years later.\nI still changed my major for graduate school, and decided to try Industrial & Systems Engineering, with a specialization on Macrosystems engineering. I taught myself Python and SQL, I did very well during graduate school with my courses, I worked on research projects and had the best time writing my thesis. So I thought, wow, I actually like all of this! Why not do a PhD? And here I am! I love learning, getting deep in a topic, planning a project from the start to the end."
  },
  {
    "objectID": "study1.html#part-2.",
    "href": "study1.html#part-2.",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 2.",
    "text": "Part 2.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-3.",
    "href": "study1.html#part-3.",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 3.",
    "text": "Part 3.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-2.-data-analysis",
    "href": "study1.html#part-2.-data-analysis",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)\n\n\n\n\nNow let’s actually do some analysis. Let’s visualize how the themes of the papers have changed across the years. I will first generate a bar plot that fills the bars at each year (as a categorical factor) with proportions of themes in that year. This is an aggregation that happens under the hood, and using position = \"fill\" will actually make sure all the bars consider things relative to eachother, filling the full 100% of the bar.\n\nggplot(df, aes(x = as.factor(Year), fill = DecisionTheme)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Set3\")\n\nTo see how things move/flow over the years, a line chart is a great idea:\n\ndf %&gt;%\n    dplyr::count(DecisionTheme, Year) %&gt;%\n    ggplot(aes(x = as.factor(Year), y = n, color = DecisionTheme, group = DecisionTheme)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Dark2\")\n\nFor analysis, I will need to convert the data to long format. Since I want to avoid making it too big, I’ll do this separately for each key variable.\n\ntheory_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = THEORY1:THEORY4,\n        names_to = \"THEORY_NAME\", \n        values_to = \"THEORY\"\n    ) \n\nmethod_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"METHODNAME\", \n        values_to = \"METHOD\"\n    ) \n\nlimit_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"LIMITNAME\", \n        values_to = \"LIMIT\"\n    ) \n\nfac_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = F1:F9,\n        names_to = \"FACNAME\",\n        values_to = \"FAC\"\n    )\n\nfac_NS_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = FNS1:FNS4,\n        names_to = \"FAC_NS_NAME\",\n        values_to = \"FAC_NS\"\n    )\n\nfactors_based_on_themes &lt;- df %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"FAC_THEMES_NAMES\",\n    values_to = \"FACTHEME\"\n)\n\nRemove all the empty rows:\n\ntheory_long &lt;- theory_long %&gt;% filter(THEORY != \"\") #\nmethod_long &lt;- method_long %&gt;% filter(METHOD != \"\") #\nlimit_long &lt;- limit_long %&gt;% filter(LIMIT != \"\") #\nfac_long &lt;- fac_long %&gt;% filter(FAC != \"\")\nfac_NS_long &lt;- fac_NS_long %&gt;% filter(FAC_NS != \"\")\n\nAdd factor scores to the long factors and non-signficant factors’ data:\n\nfac_long &lt;- merge(fac_long, factor_scores, by = \"FAC\", all = T)\n\n\nfactor_scores &lt;- factor_scores %&gt;% mutate(FAC_NS = FAC) %&gt;% dplyr::select(FAC_NS,Score_Sig, Score_NOT_Sig)\nfac_NS_long &lt;- merge(fac_NS_long, factor_scores, by = \"FAC_NS\", all = T)\n\n\n\n\nNow, I want to explore the interactions between the following properties: themes, theories, methodologies, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors. Some questions that can be answered from such an analysis are:\n\nAre there notable differences in the distribution of themes, theories, methodologies, limitations, factors, research types, sample sizes, technologies, and non-significant factors.across years?\nAre themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and non-significant factors significantly associated with specific technologies?\nAre there significant differences in sample sizes across themes, theories, methodologies, limitations, factors, years, research types, technologies, and non-significant factors?\nDo research types vary significantly among different themes, theories, methodologies, limitations, factors, years, sample sizes, technologies, and non-significant factors?\nAre there significant differences in methods used across themes, theories, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors?\nAre the significant factors identified notably different among themes, theories, methodologies, limitations, years, research types, sample sizes, technologies, and non-significant factors?\nAre the non-significant factors identified notably different among themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and technologies?\n\nTo do this, I will first decide if further investigation is even worthwhile. First, I will use ANOVA to figure out if there are significant differences between groups of the same variable. That is, are themes, theories, methodologies, limitations, technologies, factors, years, research types, sample sizes, and non-significant factors actually different across the dataset?\n\nbuild_anova &lt;- function(nameOfCol){\n    counts_df &lt;- df %&gt;% count({{nameOfCol}}) %&gt;% arrange(desc(n))\n\n    counts_df_long &lt;- data.frame(\n        Group = rep(as.character(counts_df[[1]]), times = counts_df$n),\n        Value = unlist(lapply(counts_df$n, function(x) seq_len(x)))\n    )\n\n    anova_result &lt;- aov(Value ~ Group, data = counts_df_long)\n    return(summary(anova_result))\n}\n\nThemes are significantly different.\n\nbuild_anova(DecisionTheme)\n\nSo, let’s see how they differ across other factors - starting with the ones that do not require pivoting the dataframe! (Year, Tech, SampleSizeBin, ResearchType). This time, I will use a \\chi^2 test of independence.\n\nbuild_contingency_table &lt;- function(nameOfCol){\n    data_combine &lt;- df %&gt;% group_by(DecisionTheme) %&gt;% count({{nameOfCol}}) \n\n    contingency_table &lt;- xtabs(n ~ DecisionTheme + {{nameOfCol}}, data = data_combine)\n    chi_sq_result &lt;- chisq.test(contingency_table)\n    chi_sq_result\n}\n\n\n#build_contingency_table(SampleSizeBin)\n\nYou can also calculate the Cramer V:\n\ntable(is.na(df$SampleSizeBin))\n\n\n#cramerV(build_contingency_table(SampleSizeBin))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-3.-results",
    "href": "study1.html#part-3.-results",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 3. Results",
    "text": "Part 3. Results",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The proliferation of Mobile banking, referred to in this project as m-banking, has transformed financial services, enabling consumers to conduct transactions conveniently through mobile devices. Despite the advantages, m-banking adoption rates vary significantly. Adoption is the early stage usage or uptake of a technology. My dissertation aims to bridge critical gaps in understanding how different factors (according to literature trends and through novel contributions) and device-specific attributes shape m-banking adoption. The central research question addressed in this dissertation is: “What are the factors that influence mobile banking adoption across different user segments?”\nThe current chapter is a holistic introduction to the dissertation. To answer the main research question, I conducted three distinct yet interconnected studies. These studies, presented in chronological order in the following chapters, are:\nTheoretical Contributions\nThis dissertation makes several contributions to the field of technology adoption and financial services research.\nPractical Contributions",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#the-inner-workings-of-mobile-banking-adoption-a-systematic-literature-review-of-intrinsic-factors",
    "href": "introduction.html#the-inner-workings-of-mobile-banking-adoption-a-systematic-literature-review-of-intrinsic-factors",
    "title": "Introduction",
    "section": "1 ### The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "text": "1 ### The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors\nDespite extensive research on m-banking adoption, a clear framework categorizing user-specific influences is still missing.\nAfter investigating the m-banking adoption literature, I found that uniform definitions for factors 1 influencing m-banking adoption was lacking. Furthermore, there was an over-reliance on only a few factors and theoretical frameworks. This prompted the undertaking of the SLR study in study 1. To address the problem of lack of uniform definitions, I categorized adoption factors based on context and impact. This way, using group/category membership help identify similarities between factors. Thus making definitions more cohesive.\nSeveral studies support the idea that categorization helps in defining and understanding concepts more easily. Research suggests that categorization plays a crucial role in defining concepts, moving from a classical view (fixed definitions) to a probabilistic view where categorization helps in making sense of concepts based on shared characteristics Medin (1989). Categorization enables more effective organization and processing of information which are essential for learning new concepts (zentallCATEGORIZATIONCONCEPTLEARNING2002?).\nTechnology adoption factors are complex and often belong to multiple categories. Because of this, studies have never explicitly categorized factors across the board. I adopt a broad approach to classifying factors, focusing on two key dimensions of decision-making based on how they relate to the user: Intrinsic and Extrinsic. In the context of m-banking adoption, intrinsic factors discuss how individuals evaluate an m-banking application (Often shortened to “app”) internally — based on perceptions, goals, pressure felt from other people’s judgment, and emotions. Extrinsic factors, on the other hand, refer to the apps’ measurable features, such as performance and functionality. Since extrinsic factors are easy to quantify and experienced similarly across the board, they are also straightforward to study. Thus, I focused on intrinsic factors. Because intrinsic factors relate to so many different inner processes (mentioned above), I further categorized them into four main groups:\n\nPerceptive, which are based on beliefs and perceptions.\nPersonal, which are based on individual motivations and traits.\nSocial, which are based on the impact of others on the decision-maker.\nPsychological, which are based on based on cognitive, emotional and mental processes.\n\nThis categorization provides a useful context-specific grouping. Following this, a systematic search of the m-banking literature for intrinsic factors was carried out. Scientific articles gathered were given themes using text-mining techniques – specifically, lda for Topic Modeling. Some of these themes matched my categorization, as well. I also extended prior SLR studies by using statistical techniques – specifically, anova – to mathematically validate my findings. My results provide a strong empirical foundation for future researchers to do context-focused investigation.\nThe Outcome of this study is to help enhance the understanding of intrinsic factors, highlight underutilized methodologies, and identify research gaps. Additionally, I highlight the dominance of certain theoretical models while advocating for greater exploration of under-studied theories and methods. My findings show notable geographical and study-design biases, particularly longitudinal research in developed nations. My categorized framework helps scholars identify intrinsic factors, relevant theories, and research gaps, which promotes targeted future research.\nFor practitioners and policymakers, I recommend designing emotionally engaging apps, ensuring transparent risk communication, and educating users on safe practices. These steps enhance m-banking adoption and effectiveness.\n\n1.1 The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada\nFollowing the work of the previous study, I focused on intrinsic factors. One of the factors I found to be under-explored in the literature was mental health.\nPsychological factors are comparatively less-studied across the m-banking literature Venkatesh et al. (2012), Zou et al. (2023-05, 2023), Tiwari et al. (2021-12, 2021). I verified this further doing a quick preliminary search on Web-of-Science. Using the following search query returned 1,067 studies with no filters set on the results: !30(“mobile banking” OR “mbanking” OR “m-banking”) AND (“adoption”) When changing the search query to find studies specifically on psychological factors, the total number of studies returned were 157, which is about 14\\% of the total. The new search query was: !30\\parbox0.8 (“mobile banking” OR “mbanking” OR “m-banking”) AND \\ (“adoption”) AND (“affective” OR “psychological” OR “affect based” OR “affect-based” OR “emotional” OR “cognitive” OR “mental” OR “personal”)  \\ Using the same search queries in Scopus, 753 results were returned for the first, and 101 in the second search (13.4\\%). It is clear that only a small portion of the literature in m-banking adoption is focused on psychological factors.\nIn this study, I investigated the direct and moderated effect of mh on m-banking adoption. Moderators were extracted from theories in technology adoption or from literature related to mental health: rs, sd, and sns. I used a fixed effect logistic regression model grouped based on Canadian provinces following the cluster sampling design of my dataset. The results showed that mental health significantly and negatively affects m-banking adoption: better mental health outcomes were associated with lower likelihood of m-banking adoption. I did not find sufficient evidence for the moderating effects.\n%———————————————————————- Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices -section-p3 %———————————————————————- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section -section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n\n1.2 References"
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe words factor and construct are used interchangeably throughout most of m-banking adoption literature with no significant distinction between them Oyetade et al. (2020).↩︎",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#the-relationship-between-mental-health-and-mobile-banking-adoption-evidence-from-canada",
    "href": "introduction.html#the-relationship-between-mental-health-and-mobile-banking-adoption-evidence-from-canada",
    "title": "Introduction",
    "section": "2 ### The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada",
    "text": "2 ### The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada\nFollowing the work of the previous study, I focused on intrinsic factors. One of the factors I found to be under-explored in the literature was mental health.\nPsychological factors are comparatively less-studied across the m-banking literature Venkatesh et al. (2012), Zou et al. (2023-05, 2023), Tiwari et al. (2021-12, 2021). I verified this further doing a quick preliminary search on Web-of-Science. Using the following search query returned 1,067 studies with no filters set on the results: !30(“mobile banking” OR “mbanking” OR “m-banking”) AND (“adoption”) When changing the search query to find studies specifically on psychological factors, the total number of studies returned were 157, which is about 14\\% of the total. The new search query was: !30\\parbox0.8 (“mobile banking” OR “mbanking” OR “m-banking”) AND \\ (“adoption”) AND (“affective” OR “psychological” OR “affect based” OR “affect-based” OR “emotional” OR “cognitive” OR “mental” OR “personal”)  \\ Using the same search queries in Scopus, 753 results were returned for the first, and 101 in the second search (13.4\\%). It is clear that only a small portion of the literature in m-banking adoption is focused on psychological factors.\nIn this study, I investigated the direct and moderated effect of mh on m-banking adoption. Moderators were extracted from theories in technology adoption or from literature related to mental health: rs, sd, and sns. I used a fixed effect logistic regression model grouped based on Canadian provinces following the cluster sampling design of my dataset. The results showed that mental health significantly and negatively affects m-banking adoption: better mental health outcomes were associated with lower likelihood of m-banking adoption. I did not find sufficient evidence for the moderating effects.\n%———————————————————————- Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices -section-p3 %———————————————————————- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section -section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n2.1 References"
  },
  {
    "objectID": "introduction.html#the-device-divide-unpacking-mobile-banking-adoption-differences-for-smartphones-and-smart-wearable-devices-labelintro-section-p3",
    "href": "introduction.html#the-device-divide-unpacking-mobile-banking-adoption-differences-for-smartphones-and-smart-wearable-devices-labelintro-section-p3",
    "title": "Introduction",
    "section": "1 The Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices “labelintro-section-p3",
    "text": "1 The Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices “labelintro-section-p3\n%———————————————————————- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section “refintro-section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n1.1 References"
  },
  {
    "objectID": "study1.html",
    "href": "study1.html",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "This study is the second chapter of my Dissertation, “Essays on Mobile Banking Adoption”. This chapter aims to provide a clear framework for categorizing the various factors that influence mobile banking (m-banking) adoption, which have often lacked uniform definitions in previous research. There’s an over-reliance on a limited number of factors and theoretical frameworks in existing m-banking adoption literature.\nTo address these gaps, I introduce a novel categorization of adoption factors into two main dimensions:\n\nIntrinsic\nExtrinsic\n\nThe focus of this study is on intrinsic factors, which relate to how individuals internally evaluate an m-banking application based on their perceptions (Perceptive Factors), goals (Personal Factors), social pressures (Social Factors), and emotions (Psychological Factors). In contrast, extrinsic factors are measurable features of the application, such as performance and functionality, which are generally easier to quantify and study due to their consistent experience across users.\nThe intrinsic factors are further divided into four main sub-categories:\n\nPerceptive (based on beliefs and perceptions),\nPersonal (based on individual motivations and traits),\nSocial (based on the impact of others),\nPsychological (based on cognitive, emotional, and mental processes).\n\nThis structured categorization aims to clarify the meaning of various factors, enable more focused research, and highlight overlooked patterns in the literature. The methodology employed in this chapter involves a systematic literature review of m-banking adoption, utilizing text-mining techniques, specifically Latent Dirichlet Allocation (LDA) for topic modeling, to assign themes to collected scientific articles.\nStatistical techniques, such as Analysis of Variance (ANOVA), are then used to validate the findings empirically. This approach is designed to provide a robust foundation for future context-focused investigations, identify research gaps, and advocate for the exploration of under-studied theories and methods in m-banking adoption.\n\nTopic Modeling\nTopic modeling is a crucial step in analyzing text data to identify patterns and themes within a collection of articles. In this chapter, the process begins with extracting text from PDF files using PyMuPDF and then cleaning the text by converting it to lowercase, removing special characters, extra spaces, and stop words using NLTK. The cleaned text is then broken down into uni-grams (single words) and bi-grams (word pairs) through a process called tokenizing.\nTo identify the most important tokens, the Term Frequency-Inverse Document Frequency (TF-IDF) method is applied, followed by Latent Dirichlet Allocation (LDA) to discover latent topics within the corpus. LDA models documents as mixtures of latent topics, where each topic is represented by a distribution of words. The number of topics for the LDA algorithm is determined by evaluating models with a range of topics (|T|\\in[5,15]) and selecting the number that yields the highest coherence score, which indicates better model performance. For this study, eight topics were found to be optimal when using bi-grams.\nThe LDA-generated topics are then clarified and categorized. Tokens are grouped into categories such as Perceptive (perception-related), Psychological (cognition, emotions, mind), Personal (individual characteristics), Demographic (age, gender, income, education), Cultural (country, region, religion, culture), External (universally experienced, extrinsic factors), Market (technologies, devices), and Generic (unclassified or vague). Generic and external tokens are excluded from theme classification to maintain focus on intrinsic factors.\nFor a walk through of this part, read Topic Modeling.\n\n\nTheme Assignment\nFollowing topic modeling, each paper is assigned themes by the authors, with typically one to three themes per paper. To validate these assignments, a rule-based text mining approach using TF-IDF is employed. This method verifies theme assignments by counting the occurrences and impact of tokens within each article, allowing for up to three themes to be assigned per paper.\nThe algorithm works by using a pre-defined Python dictionary where each key represents a theme (e.g., “Perceptive,” “Psychological”) and its value is a list of factors associated with that theme. The algorithm calculates a score for each theme in an article based on the frequency of its associated factors. The themes corresponding to the top three highest scores are then assigned to the paper. If there is an overlap between author-assigned themes and algorithm-assigned themes, the matching theme with the highest weight is selected as the main theme. In cases where no themes match, the paper is re-reviewed for a decision. The analysis showed a high agreement rate of 90.2% between manual and rule-based assignments.\nFor a walk through of this part, read Theme Assignment.\n\n\nData Analysis\nThe dataset used for this chapter’s analysis comprises 143 articles published between 2018 and 2024, with significant variation in sample sizes across studies. Key information, such as factors of influence, research type, sample size, methods, and foundational theories, was manually extracted from each paper and stored in a CSV file.\nAn exploratory data analysis revealed several insights into the m-banking adoption literature. Most studies are quantitative (80), followed by empirical (27). The Technology Acceptance Model (TAM) and the Unified Theory of Acceptance and Use of Technology (UTAUT) are the most dominant theoretical frameworks, accounting for 70% of the literature, suggesting a potential over-reliance on these models. Structural Equation Modeling (SEM) and Partial Least Squares SEM (PLS-SEM) are the most common methodologies used.\nThe most frequently cited limitations in the studies include limited generalizability due to a focus on specific countries, sample-related issues (such as unrepresentative samples), and the lack of longitudinal studies. Perceived Usefulness, Perceived Ease of Use, and Trust are among the most frequently identified significant factors. The analysis also investigates non-significant factors and explores patterns in how these factors are reported across different study characteristics. Statistical analyses, such as chi-squared tests, are used to examine relationships between themes, technologies, research types, and other variables.\nFor a walk through of this part, read Data Analysis.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-1.-data-collection",
    "href": "study1.html#part-1.-data-collection",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 1. Data Collection",
    "text": "Part 1. Data Collection\nI downloaded the pdf of all the papers (143), reading them and extracting meta data based on the following:\n\nimport numpy as np \n\ndatabase = np.array([\n    {\n        'id': 'string', # unique identifier for the paper following convention P2_#number \n        'title': 'string', # title of the paper\n        'AffiliationCountry': 'string' , #name of country the study was conducted in,\n        'year': 2018-2024, # year of publication a value between 2018 and 2024\n        'journal': 'string', # name of the journal the paper was published in\n        'citations': 0-1000, # number of citations the paper has received - not reported in the paper \n        'year_since': 3, # number of years since publication - not reported in the paper \n        'cpy': 0, # number of citations per year - not reported in the paper \n        'keywords': ['TAM', 'mbanking', 'awareness'], # list of keywords, broken into K1-K10\n        'abstract': 'string', # abstract of the paper \n        'F': ['perceived usefulness'], # factors significant in the study, broken into F1-F9 \n        'FN': ['another factor'], # factors not significant in the study, broken into FNS1-FNS4 \n        'limit': ['geographical context'], # limitations of the study, broken into LIMIT1-LIMIT3 \n        'typeofResearch': 'string', # type of research conducted in the study \n        'methods': ['regression analysis'], # methods used in the study, broken into METHOD1-METHOD4\n        'theory': ['TAM'] # theories used in the study, broken into THEORY1-THEORY4\n        'sampleSize': 100, # sample size of the study \n        'tech': 'string', # main technology studied \n        'man_theme': 'string', # Theme manually assigned by me \n        'algo_theme': 'string', # Theme assigned by the algorithm \n        'decision_Theme': 'string', # Final theme of the paper  \n        'Score_Sig': 0.0, # % of significance for factors \n        'Score_NOT_Sig': 0.0, # % of non-significance for factors\n    }\n])\n\n\n\nIdea for future\n\n🤖 Build an Agentic AI application that automates this process.\n\n\nPart 1.1 Finding Out Themes\nFirst, install the following Python modules:\n\n%%capture \n!pip install nltk\n!pip install gensim\n!pip install itertools\n!pip install spacy\n!pip install langdetect\n!pip install pprint\n!pip install pyLDAvis\n!pip install textract\n!pip install spacy\n!pip install pymupdf\n!pip uninstall matplotlib seaborn -y\n!pip install matplotlib seaborn  \n!pip install --upgrade matplotlib seaborn\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n!pip install scipy==1.12.0 --quiet\n\nSince I am not familiar with Docker, I couldn’t resolve the package dependencies. This took so much time for me and I finally managed to fix it with this specific configuration. The imports look scary:\n\nimport string\nimport os \nimport re # regular expression \nimport pandas as pd\nimport numpy as np\n__requires__= 'scipy==1.12.0'\nimport scipy \nimport itertools\nimport textract # PDF text extraction \nimport math\nimport spacy\nimport fitz #PyMuPDF - another (better) PDF text extraction \n\n#NLP imports\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk.tokenize import RegexpTokenizer\n# from nltk import pos_tag # didn't actually use it \n\n#SKLEARN\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import classification_report\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.neighbors import NearestNeighbors\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n# from sklearn.naive_bayes import (\n# BernoulliNB,\n# ComplementNB,\n# MultinomialNB,\n# )\n#from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.svm import SVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neural_network import MLPClassifier\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics.pairwise import cosine_similarity\n#from sklearn.decomposition import LatentDirichletAllocation\n\n#GENSIMimports\nimport gensim\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.corpora import MmCorpus\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import KeyedVectors\n\n#PyLDAvis imports for visualization of topic modeling results \n# import pyLDAvis\n# import pyLDAvis.gensim_models as gensimvis\n# import pyLDAvis.gensim\n# import pyLDAvis.gensim_models\n\n#MISC imports\nfrom collections import Counter\nfrom collections import defaultdict\nfrom string import punctuation\nfrom pprint import pprint\nfrom numpy import triu\n#from scipy.linalg.special_matrices import triu\nfrom scipy.sparse import csr_matrix\n\n#TRANSFORMERS\n#import torch\n#importtensorflowastf\n#from transformers import BertTokenizer, BertModel\n#from transformers import AutoTokenizer, AutoModel\n#fromtensorflow.keras.modelsimportSequential\n#fromtensorflow.keras.preprocessing.textimportTokenizer\n#fromtensorflow.keras.preprocessing.sequenceimportpad_sequences\n#fromtensorflow.keras.layersimportDense,Embedding,LSTM,SpatialDropout1D\n#fromtensorflow.keras.layersimportLeakyReLU\n\n#MATPLOT\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDownload some of the language support stuff:\n\n# only run once\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\nnltk.download('omw-1.4')  # Optional \n#nltk.download('averaged_perceptron_tagger')  # For POS tagging\n#nltk.download('averaged_perceptron_tagger_eng') # POS tagging \n\nI saved the pdf files’ name in a dictionary like this:\n\nname_of_pdfs = {\n    'p2_01': \"Lonkani et al_2020_A comparative study of trust in mobile banking.pdf\", \n    'p2_02': \"Saprikis et al_2022_A comparative study of users versus non-users' behavioral intention towards.pdf\", \n    'p2_03': \"Malaquias et al_2021_A cross-country study on intention to use mobile banking.pdf\", \n    'p2_04': \"Merhi et al_2019_A cross-cultural study of the intention to use mobile banking between Lebanese.pdf\", \n    'p2_05': \"Frimpong et al. - 2020 - A cross‐national investigation of trait antecedent.pdf\", \n    # and so on ... \n}\n\nAdditionally, I defined a dictionary “look up” for all the factors in the dataset with their related theme that looks like this (shortened for this presentation):\n\ntheme_of_words = {\n    'demographic': \n        list(set(['women', 'woman', 'female', 'men', 'man', 'male', 'sex', 'gender', 'age', 'income', \n            'demographic variables', 'elderly', 'education', 'gender differences', 'generation y', 'millennial generation',\n            'millennial', 'gen y', 'gen Z', 'gen alpha', 'gen X', 'boomer', 'babyboomer', 'generation X', 'generation z',\n            'young consumers', \n            # A lot more factors ...\n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'cultural': \n        list(set(['developing countries','malaysia','transition country','pakistan',\n            'zakat','developing country','ghana','USA','srilanka', 'sri lanka',\n            'india','maldives','saudi-arabia','saudi arabia', 'nigeria','thailand','united states',\n            'yemen','citizenship','zimbabwe','palestine','culture',\n            'Country perspective', \n            # ... \n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'psychological':\n        list(set(['anxiety','satisfaction','behavior','behaviour','attitudes','attitude','awareness',\n            'technology anxiety','consumer-behavior','trust','benv','consumer behaviour',\n            'covid-19 related psychological distress','psychological distress','psychological','distress',\n            'behavioral','computer anxiety','customer satisfaction', 'cognitive resistance',\n            # A LOT more ... \n            ]))\n            , \n            # ... few other key value pairs corresponding to themes \n\n}\n\nI also needed to delete some stop words, and decided to add more words that I knew would be frequently repeated. I also define the lemmer and stemmer.\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"online\", \"e\", \"e-bank\", \"ebank\", \"mobile banking\", \"mobile bank\", \n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", \"doi\", \"http\", \"https\", \"et al\", \"issue\",\n                   \"technology\", \"internet\", \"information system\", \"international information\",\n                   \"information technology\", \"computer human\", \"mis quarterly\", \"electornic commerce\",\n                   \"j market\", \"telematics and informatics\", \"telematics informatics\", \"retail consumer\",\n                   \"international volume\", \"international business\", \"global information\",\n                   \"et\", \"al\", \"al.\", \"tam\", \"sem\", \"pls\", \"utaut\", \"tpb\",\n                   \".com\", \"management\", \"marketing\", \"published\", \"study\",\n                   \"research\", \"literature\", \"model\", #from journal information \n                   \"app\", \"application\", \"usage\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\nSet up.\nSo, I need a few functions as set up for cleaning the text. Function extract_text_from_pdf() is using PyMuPDF to extract text from a PDF file.\n\n#version one using PyMuPDF \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\nThis function is just one of the data cleaning functions:\n\ndef preprocess_Dict(dct):\n    for k, v in dct.items():\n        if isinstance(v, list):\n            processed_list = []\n            for item in v:\n                item = item.lower()\n                item = re.sub(r'http\\S+|www\\S+|@\\S+', '', item)\n                item = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', item)\n                item = re.sub(r'[^a-z0-9\\s\\n]', '', item)\n                item = re.sub(r'\\s+', ' ', item).strip()\n                item = re.sub(r'\\d+', '', item).strip()\n\n                # replacing abbreviations \n                item = item.replace('structural equation model', 'sem')\n                item = item.replace('technology acceptance model', 'tam')\n                item = item.replace('unified theory of acceptance and use of technology', 'utaut')\n                item = item.replace('diffusion of innovation', 'doi')\n                item = item.replace('partial least squares', 'pls')\n                item = item.replace('theory of planned behavior', 'tpb')\n                item = re.sub(\"perceived usefulness\", \"pu\", item)\n                item = re.sub(\"perceived ease of use\", \"peou\", item)\n                item = re.sub(\"perceived privacy\", \"priv\", item)\n                item = re.sub(\"perceived aesthetics\", \"p_aest\", item)\n                item = re.sub(\"perceived relative advantage\", \"p_rel_adv\", item)\n                item = re.sub(\"perceived risk\", \"prisk\", item)\n                item = re.sub(\"perceived enjoyment\",\"penjy\", item)\n                item = re.sub(\"perceived intelligence\",\"pintlj\", item)\n                item = re.sub(\"perceived security\",\"psec\", item)\n                item = re.sub(\"perceived trust\",\"ptrst\", item)\n                item = re.sub(\"perceived anthropomorphism\",\"panthro\", item)\n                item = re.sub(\"perceived value\",\"pval\", item)\n                item = re.sub(\"perceived compatibility\",\"pcompat\", item)\n                item = re.sub(\"perceived detterants\",\"pdet\", item)\n                item = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", item)\n                item = re.sub(\"perceived credibility\",\"pcred\", item)\n                item = re.sub(\"perceived cost\",\"pcost\", item)\n                item = re.sub(\"perceived benefit\",\"pbenef\", item)\n                item = re.sub(\"perceived convenience\",\"pconv\", item)\n                item = re.sub(\"perceived usability\",\"pusbl\", item)\n                item = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", item)\n                \n                # belief base \n                item = re.sub(\"performance expectancy\", \"peex\", item)\n                item = re.sub(\"convenience\", \"conv\", item)\n                item = re.sub(\"effort expectancy\",\"efex\", item)\n                item = re.sub(\"access convenience\",\"acc_conv\", item)\n                item = re.sub(\"reliability\", \"rely\", item)\n                item = re.sub(\"behavioral control\", \"bhv_ctrl\", item)\n                item = re.sub(\"compatibility\", \"compat\", item)\n                item = re.sub(\"normative beliefs\", \"norm_blf\", item)\n                item = re.sub(\"normative belief\", \"norm_blf\", item)\n                item = re.sub(\"transaction convenience\",\"trans_conv\", item)\n                item = re.sub(\"post use trust\", \"post_trst\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"benefit convenience\",\"ben_conv\", item)\n                item = re.sub(\"search convenience\", \"srch_conv\", item)\n                item = re.sub(\"utilitarian expectation\", \"util_exp\", item)\n                item = re.sub(\"evaluation convenience\", \"eval_conv\", item)\n                item = re.sub(\"expectation\", \"expect\", item)\n                item = re.sub(\"possession convenience\", \"poss_conv\", item)\n                item = re.sub(\"expected advantage\", \"exp_adv\", item)\n               \n                # intention \n                item = re.sub(\"intention\", \"intnt\", item)\n                item = re.sub(\"motivation\", \"motiv\", item)\n                item = re.sub(\"automative motivation\",\"auto_motiv\", item)\n                item = re.sub(\"behavioral intention\",\"bhv_intnt\", item)\n                item = re.sub(\"control motivation\",\"ctrl_motiv\", item)\n                item = re.sub(\"controlled motivation\", \"ctrl_motiv\",  item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\", item)\n                item = re.sub(\"intention to use\",\"intnt_use\", item)\n                \n                # personal \n                item = re.sub(\"habit\", \"habt\", item)\n                item = re.sub(\"personality\",\"prsnl\",item)\n                item = re.sub(\"personal factors\",\"prsnl\",item)\n                item = re.sub(\"personal factor\", \"prsnl\", item)\n                item = re.sub(\"digital literacy\",\"dig_lit\",item)\n                item = re.sub(\"digital capability\",\"dig_cabl\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"financial literacy\",\"fin_lit\",item)\n                item = re.sub(\"previous experience\",\"prv_exp\",item)\n                item = re.sub(\"life compatibility\",\"life_compat\",item)\n                item = re.sub(\"lifestyle\", \"life\", item)\n                item = re.sub(\"knowledge\",\"know\",item)\n                item = re.sub(\"functional value\",\"fun_val\",item)\n                item = re.sub(\"fun value\", \"fun_val\", item)\n                item = re.sub(\"utalitarian value\",\"util_val\",item)\n                item = re.sub(\"epistemic value\",\"epi_val\",item)\n                item = re.sub(\"monetary value\",\"mon_val\",item)\n                item = re.sub(\"money value\",\"mon_val\",item)\n                item = re.sub(\"hedonic value\", \"hed_val\", item)\n                item = re.sub(\"emotional value\",\"emo_val\",item)\n                item = re.sub(\"quality value\",\"qual_val\",item)\n                item = re.sub(\"value barriers\", \"val_bar\", item)\n                item = re.sub(\"value barrier\",\"val_bar\",item)\n                item = re.sub(\"customer experience about usability\",\"exp_use\",item)\n                item = re.sub(\"experience\",\"exp\",item)\n                item = re.sub(\"self employment\",\"semp\",item)\n                item = re.sub(\"self-employment\",\"semp\",item)\n                item = re.sub(\"valence\",\"val\",item)\n                item = re.sub(\"religiosity\",\"religis\",item)\n                item = re.sub(\"task technology fit\",\"ttf\",item)\n                item = re.sub(\"lifestyle fit\",\"life_fit\",item)\n                    \n                # social \n                item = re.sub(\"social interactions on platforms\",\"soc_int_plt\", item)\n                item = re.sub(\"coercive pressures\", \"coe_prsr\",  item)\n                item = re.sub(\"coercive pressure\", \"coe_prsr\",  item)\n                item = re.sub(\"human human interaction\",\"hh_int\", item)\n                item = re.sub(\"human-human interaction\",\"hh_int\", item)\n                item = re.sub(\"social influence\", \"socinf\", item)\n                item = re.sub(\"collectivist cultural practices\", \"colcul\",  item)\n                item = re.sub(\"collectivist cultural practice\",\"colcul\", item)\n                item = re.sub(\"social media influence\",\"snsinf\", item)\n                item = re.sub(\"normative belief\",\"norm_blf\", item)\n                item = re.sub(\"interaction\",\"interac\", item)\n                item = re.sub(\"subjective norm\", \"sbj_nrm\",  item)\n                item = re.sub(\"subjective norms\", \"sbj_nrm\",  item)\n                item = re.sub(\"social factors\", \"soc_fac\",  item)\n                item = re.sub(\"social factor\" ,\"soc_fac\" , item)\n                item = re.sub(\"normative pressure\",\"nrm_prsr\", item)\n                item = re.sub(\"CSR economical responsibility\",\"csr_econ\", item)\n                item = re.sub(\"social norms\", \"soc_nrm\",  item)\n                item = re.sub(\"family influence\",\"fam_inf\", item)\n                item = re.sub(\"people\",\"people\", item)\n                item = re.sub(\"herd\",\"herd\", item)\n                item = re.sub(\"CSR social responsibility\",\"csr_soc\", item)\n                item = re.sub(\"mimetic pressure\", \"mim_prsr\",  item)\n                item = re.sub(\"CSR environmental responsibility\", \"csr_env\",  item)\n                item = re.sub(\"social value\",\"soc_val\", item)\n                item = re.sub(\"social values\",\"soc_val\", item)\n                item = re.sub(\"employee customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"employee-customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"social isolation\", \"soc_iso\",  item)\n                item = re.sub(\"normative pressures\",\"norm_prsr\", item)\n                item = re.sub(\"social proof social media\", \"soc_prf_sns\",  item)\n                item = re.sub(\"social proof\", \"soc_prf\",  item)\n                item = re.sub(\"social media\",\"sns\", item)\n                item = re.sub(\"word of mouth\", \"wom\",  item)\n                item = re.sub(\"wom\",\"wom\", item)\n                item = re.sub(\"word-of-mouth\", \"wom\",  item)\n                item = re.sub(\"w-o-m\", \"wom\",  item)\n                item = re.sub(\"wordmouth\", \"wom\",  item)\n                item = re.sub(\"environment\", \"env\",  item)\n                \n                \n                # psychological \n                item = re.sub(\"computer self efficacy\",\"self\",item)\n                item = re.sub(\"self efficacy\",\"self\",item)\n                item = re.sub(\"self-efficacy\", \"self\", item)\n                item = re.sub(\"attitude\", \"attd\", item)\n                item = re.sub(\"attitudes\",\"attd\",item)\n                item = re.sub(\"trust\",\"trst\",item)\n                item = re.sub(\"pragmatic\",\"prgt\",item)\n                item = re.sub(\"security concern\", \"sec_cn\",item)\n                item = re.sub(\"security concerns\",\"sec_cn\",item)\n                item = re.sub(\"self-image\", \"self_cong\", item)\n                item = re.sub(\"self image\",\"self_cong\",item)\n                item = re.sub(\"congruence\", \"cong\", item)\n                item = re.sub(\"self-congruence\",\"self_cong\",item)\n                item = re.sub(\"self congruence\", \"self_cong\", item)\n                item = re.sub(\"self-image congruence\",\"self_cong\",item)\n                item = re.sub(\"self image congruence\", \"self_cong\", item)\n                item = re.sub(\"selfimage congruence\", \"self_cong\", item)\n                item = re.sub(\"awareness\", \"awar\", item)\n                item = re.sub(\"satisfaction\",\"satis\",item)\n                item = re.sub(\"consumer satisfaction\",\"satis\",item)\n                item = re.sub(\"customer satisfaction\",\"satis\",item)\n                item = re.sub(\"restiant to change\",\"resist_chng\",item)\n                item = re.sub(\"resistance to change\",\"resist_chng\",item)\n                item = re.sub(\"risk aversion\", \"risk_avrs\", item)\n                item = re.sub(\"risk averse\",\"risk_avrs\",item)\n                item = re.sub(\"novelty\",\"new_seek\",item)\n                item = re.sub(\"novelty-seeking\", \"new_seek\", item)\n                item = re.sub(\"novelty seeking\", \"new_seek\", item)\n                item = re.sub(\"consciousnesnness\", \"conscn\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"post use trust\",\"post_trst\",item)\n                item = re.sub(\"postuse trust\",\"post_trst\",item)\n                item = re.sub(\"emotional experience\",\"emo_exp\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"privacy concerns\",\"priv_cn\",item)\n                item = re.sub(\"privacy concern\",\"priv_cn\",item)\n                item = re.sub(\"cognitive decline\",\"cog_dec\",item)\n                item = re.sub(\"benevolent convenince\",\"ben_conv\",item)\n                item = re.sub(\"enjoyment\",\"enjy\",item)\n                item = re.sub(\"enjoy\", \"enjy\", item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\",item)\n                item = re.sub(\"oppenness\", \"open\", item)\n                item = re.sub(\"loyal\", \"loyal\", item)\n                item = re.sub(\"loyalty\",\"loyal\",item)\n                item = re.sub(\"confirmation\",\"confrm\",item)\n                item = re.sub(\"optimism\",\"optim\",item)\n                item = re.sub(\"safety concerns\",\"safe_cn\",item)\n                item = re.sub(\"safety concern\",\"safe_cn\",item)\n                item = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"green concerns\",\"green_cn\",item)\n                item = re.sub(\"technology anxiety\", \"anxiety\", item)\n                item = re.sub(\"anxiety\",\"anxiety\",item)\n                item = re.sub(\"obedience\", \"obed\", item)\n                item = re.sub(\"empathy\",\"empath\",item)\n                item = re.sub(\"decision comfort\",\"dec_comfrt\",item)\n                item = re.sub(\"confidence\",\"confdnc\",item)\n                item = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", item)\n                item = re.sub(\"comfort\", \"cmfrt\", item)\n                item = re.sub(\"discomfort\",\"discmfrt\",item)\n                item = re.sub(\"insecurity\", \"insec\", item)\n                item = re.sub(\"insecurities\", \"insec\", item)\n                item = re.sub(\"benevolence\",\"benv\",item)\n                item = re.sub(\"technology stress\",\"tech_strss\",item)\n                item = re.sub(\"stress\", \"tech_strss\", item)\n                item = re.sub(\"techno-stress\",\"tech_strss\",item)\n                item = re.sub(\"technostress\",\"tech_strss\",item)\n                item = re.sub(\"techno stress\", \"tech_strss\", item)\n                item = re.sub(\"cognitive resistence\",\"cog_resist\",item)\n                        \n                # demographic \n                item = re.sub(\"age\",\"age\",item)\n                item = re.sub(\"sex\",\"sex\",item)\n                item = re.sub(\"education\",\"edu\",item)\n                item = re.sub(\"income\",\"income\",item)\n                item = re.sub(\"islamic religiosity\",\"religios\",item)\n                item = re.sub(\"culture\",\"cltr\",item)\n                \n                item = \" \".join([word for word in item.split() if word not in stop_words])\n                item = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in item.split()])\n                #item = \" \".join([stemmer.stem(word) for word in item.split()])\n                processed_list.append(item)\n            dct[k] = \" \".join(processed_list)\n        else:\n            v = v.lower()\n            v = re.sub(r'http\\S+|www\\S+|@\\S+', '', v)\n            v = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', v)\n            v = re.sub(r'[^a-z0-9\\s\\n]', '', v)\n            v = re.sub(r'\\s+', ' ', v).strip()\n            v = re.sub(r'\\d+', '', v).strip()\n\n            # replacing abbreviations \n            v = v.replace('structural equation model', 'sem')\n            v = v.replace('technology acceptance model', 'tam')\n            v = v.replace('unified theory of acceptance and use of technology', 'utaut')\n            v = v.replace('diffusion of innovation', 'doi')\n            v = v.replace('partial least squares', 'pls')\n            v = v.replace('theory of planned behavior', 'tpb')\n            v = re.sub(\"perceived usefulness\", \"pu\", v)\n            v = re.sub(\"perceived ease of use\", \"peou\", v)\n            v = re.sub(\"perceived privacy\", \"priv\", v)\n            v = re.sub(\"perceived aesthetics\", \"p_aest\", v)\n            v = re.sub(\"perceived relative advantage\", \"p_rel_adv\", v)\n            v = re.sub(\"perceived risk\", \"prisk\", v)\n            v = re.sub(\"perceived enjoyment\",\"penjy\", v)\n            v = re.sub(\"perceived intelligence\",\"pintlj\", v)\n            v = re.sub(\"perceived security\",\"psec\", v)\n            v = re.sub(\"perceived trust\",\"ptrst\", v)\n            v = re.sub(\"perceived anthropomorphism\",\"panthro\", v)\n            v = re.sub(\"perceived value\",\"pval\", v)\n            v = re.sub(\"perceived compatibility\",\"pcompat\", v)\n            v = re.sub(\"perceived detterants\",\"pdet\", v)\n            v = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", v)\n            v = re.sub(\"perceived credibility\",\"pcred\", v)\n            v = re.sub(\"perceived cost\",\"pcost\", v)\n            v = re.sub(\"perceived benefit\",\"pbenef\", v)\n            v = re.sub(\"perceived convenience\",\"pconv\", v)\n            v = re.sub(\"perceived usability\",\"pusbl\", v)\n            v = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", v)\n                \n            v = re.sub(\"performance expectancy\", \"peex\", v)\n            v = re.sub(\"convenience\", \"conv\", v)\n            v = re.sub(\"effort expectancy\",\"efex\", v)\n            v = re.sub(\"access convenience\",\"acc_conv\", v)\n            v = re.sub(\"reliability\", \"rely\", v)\n            v = re.sub(\"behavioral control\", \"bhv_ctrl\", v)\n            v = re.sub(\"compatibility\", \"compat\", v)\n            v = re.sub(\"normative beliefs\", \"norm_blf\", v)\n            v = re.sub(\"normative belief\", \"norm_blf\", v)\n            v = re.sub(\"transaction convenience\",\"trans_conv\", v)\n            v = re.sub(\"post use trust\", \"post_trst\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"benefit convenience\",\"ben_conv\", v)\n            v = re.sub(\"search convenience\", \"srch_conv\", v)\n            v = re.sub(\"utilitarian expectation\", \"util_exp\", v)\n            v = re.sub(\"evaluation convenience\", \"eval_conv\", v)\n            v = re.sub(\"expectation\", \"expect\", v)\n            v = re.sub(\"possession convenience\", \"poss_conv\", v)\n            v = re.sub(\"expected advantage\", \"exp_adv\", v)\n            \n            # intention \n            v = re.sub(\"intention\", \"intnt\", v)\n            v = re.sub(\"motivation\", \"motiv\", v)\n            v = re.sub(\"automative motivation\",\"auto_motiv\", v)\n            v = re.sub(\"behavioral intention\",\"bhv_intnt\", v)\n            v = re.sub(\"control motivation\",\"ctrl_motiv\", v)\n            v = re.sub(\"controlled motivation\", \"ctrl_motiv\",  v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\", v)\n            v = re.sub(\"intention to use\",\"intnt_use\", v)\n                \n                # personal \n            v = re.sub(\"habit\", \"habt\", v)\n            v = re.sub(\"personality\",\"prsnl\",v)\n            v = re.sub(\"personal factors\",\"prsnl\",v)\n            v = re.sub(\"personal factor\", \"prsnl\", v)\n            v = re.sub(\"digital literacy\",\"dig_lit\",v)\n            v = re.sub(\"digital capability\",\"dig_cabl\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"financial literacy\",\"fin_lit\",v)\n            v = re.sub(\"previous experience\",\"prv_exp\",v)\n            v = re.sub(\"life compatibility\",\"life_compat\",v)\n            v = re.sub(\"lifestyle\", \"life\", v)\n            v = re.sub(\"knowledge\",\"know\",v)\n            v = re.sub(\"functional value\",\"fun_val\",v)\n            v = re.sub(\"fun value\", \"fun_val\", v)\n            v = re.sub(\"utalitarian value\",\"util_val\",v)\n            v = re.sub(\"epistemic value\",\"epi_val\",v)\n            v = re.sub(\"monetary value\",\"mon_val\",v)\n            v = re.sub(\"money value\",\"mon_val\",v)\n            v = re.sub(\"hedonic value\", \"hed_val\", v)\n            v = re.sub(\"emotional value\",\"emo_val\",v)\n            v = re.sub(\"quality value\",\"qual_val\",v)\n            v = re.sub(\"value barriers\", \"val_bar\", v)\n            v = re.sub(\"value barrier\",\"val_bar\",v)\n            v = re.sub(\"customer experience about usability\",\"exp_use\",v)\n            v = re.sub(\"experience\",\"exp\",v)\n            v = re.sub(\"self employment\",\"semp\",v)\n            v = re.sub(\"self-employment\",\"semp\",v)\n            v = re.sub(\"valence\",\"val\",v)\n            v = re.sub(\"religiosity\",\"religis\",v)\n            v = re.sub(\"task technology fit\",\"ttf\",v)\n            v = re.sub(\"lifestyle fit\",\"life_fit\",v)\n                    \n                # social \n            v = re.sub(\"social interactions on platforms\",\"soc_int_plt\", v)\n            v = re.sub(\"coercive pressures\", \"coe_prsr\",  v)\n            v = re.sub(\"coercive pressure\", \"coe_prsr\",  v)\n            v = re.sub(\"human human interaction\",\"hh_int\", v)\n            v = re.sub(\"human-human interaction\",\"hh_int\", v)\n            v = re.sub(\"social influence\", \"socinf\", v)\n            v = re.sub(\"collectivist cultural practices\", \"colcul\",  v)\n            v = re.sub(\"collectivist cultural practice\",\"colcul\", v)\n            v = re.sub(\"social media influence\",\"snsinf\", v)\n            v = re.sub(\"normative belief\",\"norm_blf\", v)\n            v = re.sub(\"interaction\",\"interac\", v)\n            v = re.sub(\"subjective norm\", \"sbj_nrm\",  v)\n            v = re.sub(\"subjective norms\", \"sbj_nrm\",  v)\n            v = re.sub(\"social factors\", \"soc_fac\",  v)\n            v = re.sub(\"social factor\" ,\"soc_fac\" , v)\n            v = re.sub(\"normative pressure\",\"nrm_prsr\", v)\n            v = re.sub(\"CSR economical responsibility\",\"csr_econ\", v)\n            v = re.sub(\"social norms\", \"soc_nrm\",  v)\n            v = re.sub(\"family influence\",\"fam_inf\", v)\n            v = re.sub(\"people\",\"people\", v)\n            v = re.sub(\"herd\",\"herd\", v)\n            v = re.sub(\"CSR social responsibility\",\"csr_soc\", v)\n            v = re.sub(\"mimetic pressure\", \"mim_prsr\",  v)\n            v = re.sub(\"CSR environmental responsibility\", \"csr_env\",  v)\n            v = re.sub(\"social value\",\"soc_val\", v)\n            v = re.sub(\"social values\",\"soc_val\", v)\n            v = re.sub(\"employee customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"employee-customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"social isolation\", \"soc_iso\",  v)\n            v = re.sub(\"normative pressures\",\"norm_prsr\", v)\n            v = re.sub(\"social proof social media\", \"soc_prf_sns\",  v)\n            v = re.sub(\"social proof\", \"soc_prf\",  v)\n            v = re.sub(\"social media\",\"sns\", v)\n            v = re.sub(\"word of mouth\", \"wom\",  v)\n            v = re.sub(\"wom\",\"wom\", v)\n            v = re.sub(\"word-of-mouth\", \"wom\",  v)\n            v = re.sub(\"w-o-m\", \"wom\",  v)\n            v = re.sub(\"wordmouth\", \"wom\",  v)\n            v = re.sub(\"environment\", \"env\",  v)\n                \n                \n                # psychological \n            v = re.sub(\"computer self efficacy\",\"self\",v)\n            v = re.sub(\"self efficacy\",\"self\",v)\n            v = re.sub(\"self-efficacy\", \"self\", v)\n            v = re.sub(\"attitude\", \"attd\", v)\n            v = re.sub(\"attitudes\",\"attd\",v)\n            v = re.sub(\"trust\",\"trst\",v)\n            v = re.sub(\"pragmatic\",\"prgt\",v)\n            v = re.sub(\"security concern\", \"sec_cn\",v)\n            v = re.sub(\"security concerns\",\"sec_cn\",v)\n            v = re.sub(\"self-image\", \"self_cong\", v)\n            v = re.sub(\"self image\",\"self_cong\",v)\n            v = re.sub(\"congruence\", \"cong\", v)\n            v = re.sub(\"self-congruence\",\"self_cong\",v)\n            v = re.sub(\"self congruence\", \"self_cong\", v)\n            v = re.sub(\"self-image congruence\",\"self_cong\",v)\n            v = re.sub(\"self image congruence\", \"self_cong\", v)\n            v = re.sub(\"selfimage congruence\", \"self_cong\", v)\n            v = re.sub(\"awareness\", \"awar\", v)\n            v = re.sub(\"satisfaction\",\"satis\",v)\n            v = re.sub(\"consumer satisfaction\",\"satis\",v)\n            v = re.sub(\"customer satisfaction\",\"satis\",v)\n            v = re.sub(\"restiant to change\",\"resist_chng\",v)\n            v = re.sub(\"resistance to change\",\"resist_chng\",v)\n            v = re.sub(\"risk aversion\", \"risk_avrs\", v)\n            v = re.sub(\"risk averse\",\"risk_avrs\",v)\n            v = re.sub(\"novelty\",\"new_seek\",v)\n            v = re.sub(\"novelty-seeking\", \"new_seek\", v)\n            v = re.sub(\"novelty seeking\", \"new_seek\", v)\n            v = re.sub(\"consciousnesnness\", \"conscn\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"post use trust\",\"post_trst\",v)\n            v = re.sub(\"postuse trust\",\"post_trst\",v)\n            v = re.sub(\"emotional experience\",\"emo_exp\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"privacy concerns\",\"priv_cn\",v)\n            v = re.sub(\"privacy concern\",\"priv_cn\",v)\n            v = re.sub(\"cognitive decline\",\"cog_dec\",v)\n            v = re.sub(\"benevolent convenince\",\"ben_conv\",v)\n            v = re.sub(\"enjoyment\",\"enjy\",v)\n            v = re.sub(\"enjoy\", \"enjy\", v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\",v)\n            v = re.sub(\"oppenness\", \"open\", v)\n            v = re.sub(\"loyal\", \"loyal\", v)\n            v = re.sub(\"loyalty\",\"loyal\",v)\n            v = re.sub(\"confirmation\",\"confrm\",v)\n            v = re.sub(\"optimism\",\"optim\",v)\n            v = re.sub(\"safety concerns\",\"safe_cn\",v)\n            v = re.sub(\"safety concern\",\"safe_cn\",v)\n            v = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"green concerns\",\"green_cn\",v)\n            v = re.sub(\"technology anxiety\", \"anxiety\", v)\n            v = re.sub(\"anxiety\",\"anxiety\",v)\n            v = re.sub(\"obedience\", \"obed\", v)\n            v = re.sub(\"empathy\",\"empath\",v)\n            v = re.sub(\"decision comfort\",\"dec_comfrt\",v)\n            v = re.sub(\"confidence\",\"confdnc\",v)\n            v = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", v)\n            v = re.sub(\"comfort\", \"cmfrt\", v)\n            v = re.sub(\"discomfort\",\"discmfrt\",v)\n            v = re.sub(\"insecurity\", \"insec\", v)\n            v = re.sub(\"insecurities\", \"insec\", v)\n            v = re.sub(\"benevolence\",\"benv\",v)\n            v = re.sub(\"technology stress\",\"tech_strss\",v)\n            v = re.sub(\"stress\", \"tech_strss\", v)\n            v = re.sub(\"techno-stress\",\"tech_strss\",v)\n            v = re.sub(\"technostress\",\"tech_strss\",v)\n            v = re.sub(\"techno stress\", \"tech_strss\", v)\n            v = re.sub(\"cognitive resistence\",\"cog_resist\",v)\n                    \n            # demographic \n            v = re.sub(\"age\",\"age\",v)\n            v = re.sub(\"sex\",\"sex\",v)\n            v = re.sub(\"education\",\"edu\",v)\n            v = re.sub(\"income\",\"income\",v)\n            v = re.sub(\"islamic religiosity\",\"religios\",v)\n            v = re.sub(\"culture\",\"cltr\",v)\n                \n            v = \" \".join([word for word in v.split() if word not in stop_words])\n            v = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in v.split()])\n            #v = \" \".join([stemmer.stem(word) for word in v.split()])\n            dct[k] = v\n    return dct\n\ndef tokenizeToSentences(doc):\n    for k, v in doc.items():\n        \n        if isinstance(v, bytes):\n            v = v.decode('utf-8')\n          \n        v = v.lower()\n        v = v.replace('\\n', ' ')\n        v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n        #v = \" \".join([str(s) for s in v])\n\n        v = sent_tokenize(v)\n        doc[k] = v\n        \n    return doc\n\nFor Topic modeling, I write a function to generate dictionaries and save them in a .mm file format.\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n\n    return dictionary, corpus\n\nAdditionally, I want a function that prints the top 50 most frequently appearing words in the corpus:\n\n# ---------------------- START OF CHATGPT CODE\ndef print_top_50_words(corpus, dictionary):\n    total_word_count = defaultdict(int)\n    word_weights = defaultdict(float)\n\n    for word_id, word_count in itertools.chain.from_iterable(corpus):\n        total_word_count[word_id] += word_count\n\n    sorted_tota_words_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n\n    tfidf = TfidfModel(corpus)\n\n    for doc in corpus:\n        tfidf_weights = tfidf[doc]  # Calculate TF-IDF for the review\n        for term_id, weight in tfidf_weights:\n            word_weights[term_id] += weight  # Aggregate the weight for the term\n\n    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n\n    # Print the top 50 terms with their weights\n    top_50_words = [(dictionary.get(term_id), weight) for term_id, weight in sorted_word_weights[:50]]\n\n    for word, weight in top_50_words:\n        print(word, weight)\n\n# ---------------------- END OF CHATGPT CODE \n\nI also plan on seeing how python clusters the words (as in, finds similar words) vs me:\n\ndef print_clusters(n_clusters, list_of_words):\n    clusters = {i: [] for i in range(n_clusters)}\n    for word, label in zip(list_of_words, labels):\n        clusters[label].append(word)\n\n    for label, words in clusters.items():\n        print(f\"Cluster {label}:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n    # Explain clusters\n    print(\"Cluster explanations based on semantics and ideas:\")\n    for label, words in clusters.items():\n        print(f\"Cluster {label} might be related to:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\nThis is a function for if you want to use a word embedding (requires some effort, time and machine power!):\n\ndef get_embedding(text):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model_bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\nAnd then you use this to get semantically similar words:\n\ndef get_semantically_similar_words(words, threshold=0.7):\n    similar_words = set(words)\n    for word in words:\n        token = nlp(word)\n        for vocab_word in nlp.vocab:\n            if vocab_word.has_vector and vocab_word.is_alpha:\n                similarity = token.similarity(nlp(vocab_word.text))\n                if similarity &gt;= threshold:\n                    similar_words.add(vocab_word.text)\n    return similar_words\n\nSo, how do I find the themes? Essentially, I just tweaked TF-IDF:\n\nclass CustomTfidfVectorizer(TfidfVectorizer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        #self.general_keywords = set(general_keywords)\n        \n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)] #if w not in self.general_keywords]\n    \n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        # Penalize general keywords\n        #penalized_idf = self.idf_ * (1 - 0.8 * np.isin(self.get_feature_names_out(), list(self.general_keywords)))\n        \n        # Apply penalized IDF\n        augmented_tfidf = augmented_tf * penalized_idf\n\n        return csr_matrix(augmented_tfidf)\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\n\n\nKeyword Analytics\n\ntry:\n    df = pd.read_csv(\"P2_AR_04.csv\", encoding='utf-8')\nexcept UnicodeDecodeError:\n    try:\n        df = pd.read_csv(\"P2_AR_04.csv\", encoding='latin-1')\n    except Exception as e:\n        error_message = str(e)\n        df = None\n\nClean all the data you’ve gathered the same way the PDF’s have been cleaned (the preprocess_text() function looks very similar to the cleaning function above!):\n\ndf2 = df.copy()\n\ncolumns_to_preprocess = ['Man_Theme',\n                        'K1','K2','K3','K4','K5','K6','K7','K8','K9','K10',\n                        'F1','F2','F3','F4','F5','F6','F7','F8','F9',\n                        'FNS1','FNS2','FNS3','FNS4',\n                        'METHOD1','METHOD2','METHOD3','METHOD4',\n                        'THEORY1','THEORY2','THEORY3','THEORY4',\n                        'LIMIT1' ,'LIMIT2' ,'LIMIT3', 'Abstract'\n                        ]\n\nfor col in columns_to_preprocess:\n    df2[col] = df2[col].apply(preprocess_text)\n\n\npapers = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    papers[paper_id] = text\n\npapers_df = pd.DataFrame.from_dict(papers, orient = 'index', columns = ['paperText'])\npapers_df = papers_df.reset_index(names = ['paperID'])\npapers_df.to_csv('papers_unclean.csv')\npapers_df.head()\n\nThese look like this: \n\n# keep a copy (a habit of mine)\npapers_uncleaned = papers.copy() \ntheme_of_words_uncleaned = theme_of_words.copy() \n\n# clean up all papers \npapers_cleaned = preprocess_Dict(papers)\npapersClean_df = pd.DataFrame.from_dict(papers_cleaned, orient = 'index', columns = ['paperText'])\npapersClean_df = papersClean_df.reset_index(names = ['paperID'])\npapersClean_df.to_csv('papers_clean3.csv')\npapersClean_df.head()\n\n\nClean the theme of words dictionary so the words match:\n\ntheme_of_words_cleaned = {}\n\nfor k, v in theme_of_words.items():\n    theme_of_words_cleaned[k] = preprocess_list(v)\n\ntheme_of_words_cleaned['psychological'][:10]\n\n\n\n\n[‘initial trst’,‘simcong’,‘hedmotiv’,‘selfefficacy’,‘strss’,‘controlled motiv’,‘cong’,‘selfcong cong’,‘trst systems’,‘custom loyalti’]\n\n\n\nMake sure to drop all NA’s and empty values:\n\nfor k, v in theme_of_words_cleaned.items():\n    theme_of_words_cleaned[k] = [x for x in v if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n\n\nThemes Based on Count of Words in Each Group\nI will skip the parts on BERT and word embeddings that’s in the jupyter notebook as these were not used for my project. The reason is that it was taking so long and my computer simply did not have the capacity to handle it. I also didn’t have the time to find a fix for it, but there are a bunch of commented-out code from chatGPT that I was playing around with.\nSo, we now have a dictionary with the factors and their theme, and a corpus of text of all papers in the dataset. What I’m going to do here is: * Count all the words total_words in each paper * Count the instances of each word (factor) in theme_of_words_cleaned in each paper * Each word’s count adds 1 point to its corresponding theme’s “weight score” * Take for example the word emotion and I said the theme for this word is psychological. If emotion shows up 10 times in paper 1, paper 1’s dictionary of weights has a weight of 10 for (divided by the total number of words) psychological.\n\nresults = []\ncount_words_df = pd.DataFrame(results)\n\n\nfor doc_id, text in papers_cleaned.items():\n    doc = nlp(text)\n    word_counts = defaultdict(int)\n\n    for token in doc:\n        for group, keywords in theme_of_words_cleaned.items():\n            if token.text.lower() in keywords:\n                word_counts[group] += 1\n\n    total_words = len(doc)\n    group_weights = {f\"{group}_w\": count / total_words for group, count in word_counts.items()}\n    max_weight = max(group_weights.values(), default=0)\n    theme = max(group_weights, key=group_weights.get).replace(\"_w\", \"\") if max_weight &gt; 0 else None\n\n    result = {\"doc_id\": doc_id, **word_counts, **group_weights, \"theme\": theme, \"max_weight\": max_weight}\n    results.append(result)\n\n\nfor group in theme_of_words_cleaned.keys():\n    if group not in count_words_df.columns:\n        count_words_df[group] = 0\n    if f\"{group}_w\" not in count_words_df.columns:\n        count_words_df[f\"{group}_w\"] = 0.0\n\ncount_words_df.head()\n\n\nThis is still very basic, though because it’s only based on the factors which may not be fully representative. So, I do this again using keywords in addition to the factors. These are keywords that were selected by the authors as well as information extracted from Web of Science \\BibTeX file.\n\ncols_toPick = ['K1','K2','K3','K4','K5','K6','K7','K8','K9','K10','F1','F2','F3','F4','F5','F6','F7','F8','F9']\n\nkeywordsDf = df2.loc[:,cols_toPick]\n\n# flatten the dataframe to a list \nkeywords_across_db = keywordsDf.values.flatten().tolist()\n\n# there are 2,717 words here \nprint(\"number of words (factors and keywords) in total \", len(keywords_across_db))\n\n# making sure there are no empty/NaN/Null values \nkeywords_across_db = [x for x in keywords_across_db if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n# making sure there are no duplicates (set takes care of this)\nkeywords_across_db_nodup_cleaned = list(set(keywords_across_db))\n\n# convert the list into a dictionary temporarily, then convert it to a dataframe \ntemp = {'Keyword': keywords_across_db_nodup_cleaned}\nkeywords_themes_df = pd.DataFrame(temp, columns=['Keyword'])\n\n# go back to the theme_of_words_cleaned and find each keyword's theme \nkeywords_themes_dic = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\n# This is just a dataframe view of the keywords with their respective theme \nkeywords_themes_df['Theme'] = keywords_themes_df['Keyword'].map(keywords_themes_dic)\n\n# if the theme is empty, give it \"Generic\" - that means these keywords weren't in the list of important words that we picked themes for \nkeywords_themes_df['Theme'] = keywords_themes_df['Theme'].apply(lambda x: 'Generic' if pd.isna(x) or x == ' ' else x)\n\nkeywords_themes_df.head()",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-0.-jupyter-notebook",
    "href": "study1.html#part-0.-jupyter-notebook",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1_theme.html",
    "href": "study1_theme.html",
    "title": "Algorithmic Approach to Finding Themes",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Algorithmic Approach to Finding Themes"
    ]
  },
  {
    "objectID": "study1_theme.html#part-0.-jupyter-notebook",
    "href": "study1_theme.html#part-0.-jupyter-notebook",
    "title": "Algorithmic Approach to Finding Themes",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Algorithmic Approach to Finding Themes"
    ]
  },
  {
    "objectID": "study1_theme.html#part-1.-data-collection",
    "href": "study1_theme.html#part-1.-data-collection",
    "title": "Algorithmic Approach to Finding Themes",
    "section": "Part 1. Data Collection",
    "text": "Part 1. Data Collection\nI downloaded the pdf of all the papers (143), reading them and extracting meta data based on the following:\n\nimport numpy as np \n\ndatabase = np.array([\n    {\n        'id': 'string', # unique identifier for the paper following convention P2_#number \n        'title': 'string', # title of the paper\n        'AffiliationCountry': 'string' , #name of country the study was conducted in,\n        'year': 2018-2024, # year of publication a value between 2018 and 2024\n        'journal': 'string', # name of the journal the paper was published in\n        'citations': 0-1000, # number of citations the paper has received - not reported in the paper \n        'year_since': 3, # number of years since publication - not reported in the paper \n        'cpy': 0, # number of citations per year - not reported in the paper \n        'keywords': ['TAM', 'mbanking', 'awareness'], # list of keywords, broken into K1-K10\n        'abstract': 'string', # abstract of the paper \n        'F': ['perceived usefulness'], # factors significant in the study, broken into F1-F9 \n        'FN': ['another factor'], # factors not significant in the study, broken into FNS1-FNS4 \n        'limit': ['geographical context'], # limitations of the study, broken into LIMIT1-LIMIT3 \n        'typeofResearch': 'string', # type of research conducted in the study \n        'methods': ['regression analysis'], # methods used in the study, broken into METHOD1-METHOD4\n        'theory': ['TAM'] # theories used in the study, broken into THEORY1-THEORY4\n        'sampleSize': 100, # sample size of the study \n        'tech': 'string', # main technology studied \n        'man_theme': 'string', # Theme manually assigned by me \n        'algo_theme': 'string', # Theme assigned by the algorithm \n        'decision_Theme': 'string', # Final theme of the paper  \n        'Score_Sig': 0.0, # % of significance for factors \n        'Score_NOT_Sig': 0.0, # % of non-significance for factors\n    }\n])\n\n\n\nIdea for future\n\n🤖 Build an Agentic AI application that automates this process.\n\n\nPart 1.1 Finding Out Themes\nFirst, install the following Python modules. Since I am not familiar with Docker, I couldn’t resolve the package dependencies. This took so much time for me and I finally managed to fix it with this specific configuration. The imports look scary:\n\nimport string\nimport os \nimport re # regular expression \nimport pandas as pd\nimport numpy as np\n__requires__= 'scipy==1.12.0'\nimport scipy \nimport itertools\nimport textract # PDF text extraction \nimport math\nimport spacy\nimport fitz #PyMuPDF - another (better) PDF text extraction \n\n#NLP imports\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk.tokenize import RegexpTokenizer\n\n#SKLEARN\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#GENSIMimports\nimport gensim\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.corpora import MmCorpus\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import KeyedVectors\n\n#PyLDAvis imports for visualization of topic modeling results \n# import pyLDAvis\n# import pyLDAvis.gensim_models as gensimvis\n# import pyLDAvis.gensim\n# import pyLDAvis.gensim_models\n\n#MISC imports\nfrom collections import Counter\nfrom collections import defaultdict\nfrom string import punctuation\nfrom pprint import pprint\nfrom numpy import triu\n#from scipy.linalg.special_matrices import triu\nfrom scipy.sparse import csr_matrix\n\n#TRANSFORMERS\n#import torch\n#importtensorflowastf\n#from transformers import BertTokenizer, BertModel\n#from transformers import AutoTokenizer, AutoModel\n#fromtensorflow.keras.modelsimportSequential\n#fromtensorflow.keras.preprocessing.textimportTokenizer\n#fromtensorflow.keras.preprocessing.sequenceimportpad_sequences\n#fromtensorflow.keras.layersimportDense,Embedding,LSTM,SpatialDropout1D\n#fromtensorflow.keras.layersimportLeakyReLU\n\n#MATPLOT\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDownload some of the language support stuff:\n\n# only run once\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\n#nltk.download('omw-1.4')  # Optional \n#nltk.download('averaged_perceptron_tagger')  # For POS tagging\n#nltk.download('averaged_perceptron_tagger_eng') # POS tagging \n\nI saved the pdf files’ name in a dictionary like this:\n\nname_of_pdfs = {\n    'p2_01': \"Lonkani et al_2020_A comparative study of trust in mobile banking.pdf\", \n    'p2_02': \"Saprikis et al_2022_A comparative study of users versus non-users' behavioral intention towards.pdf\", \n    'p2_03': \"Malaquias et al_2021_A cross-country study on intention to use mobile banking.pdf\", \n    'p2_04': \"Merhi et al_2019_A cross-cultural study of the intention to use mobile banking between Lebanese.pdf\", \n    'p2_05': \"Frimpong et al. - 2020 - A cross‐national investigation of trait antecedent.pdf\", \n    # and so on ... \n}\n\nAdditionally, I defined a dictionary “look up” for all the factors in the dataset with their related theme that looks like this (shortened for this presentation):\n\ntheme_of_words = {\n    'demographic': \n        list(set(['women', 'woman', 'female', 'men', 'man', 'male', 'sex', 'gender', 'age', 'income', \n            'demographic variables', 'elderly', 'education', 'gender differences', 'generation y', 'millennial generation',\n            'millennial', 'gen y', 'gen Z', 'gen alpha', 'gen X', 'boomer', 'babyboomer', 'generation X', 'generation z',\n            'young consumers', \n            # A lot more factors ...\n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'cultural': \n        list(set(['developing countries','malaysia','transition country','pakistan',\n            'zakat','developing country','ghana','USA','srilanka', 'sri lanka',\n            'india','maldives','saudi-arabia','saudi arabia', 'nigeria','thailand','united states',\n            'yemen','citizenship','zimbabwe','palestine','culture',\n            'Country perspective', \n            # ... \n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'psychological':\n        list(set(['anxiety','satisfaction','behavior','behaviour','attitudes','attitude','awareness',\n            'technology anxiety','consumer-behavior','trust','benv','consumer behaviour',\n            'covid-19 related psychological distress','psychological distress','psychological','distress',\n            'behavioral','computer anxiety','customer satisfaction', 'cognitive resistance',\n            # A LOT more ... \n            ]))\n            , \n            # ... few other key value pairs corresponding to themes \n\n}\n\nI also needed to delete some stop words, and decided to add more words that I knew would be frequently repeated. I also define the lemmer and stemmer.\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"online\", \"e\", \"e-bank\", \"ebank\", \"mobile banking\", \"mobile bank\", \n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", \"doi\", \"http\", \"https\", \"et al\", \"issue\",\n                   \"technology\", \"internet\", \"information system\", \"international information\",\n                   \"information technology\", \"computer human\", \"mis quarterly\", \"electornic commerce\",\n                   \"j market\", \"telematics and informatics\", \"telematics informatics\", \"retail consumer\",\n                   \"international volume\", \"international business\", \"global information\",\n                   \"et\", \"al\", \"al.\", \"tam\", \"sem\", \"pls\", \"utaut\", \"tpb\",\n                   \".com\", \"management\", \"marketing\", \"published\", \"study\",\n                   \"research\", \"literature\", \"model\", #from journal information \n                   \"app\", \"application\", \"usage\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\nSet up.\nSo, I need a few functions as set up for cleaning the text. Function extract_text_from_pdf() is using PyMuPDF to extract text from a PDF file.\n\n#version one using PyMuPDF \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\nThis function is just one of the data cleaning functions:\n\ndef preprocess_Dict(dct):\n    for k, v in dct.items():\n        if isinstance(v, list):\n            processed_list = []\n            for item in v:\n                item = item.lower()\n                item = re.sub(r'http\\S+|www\\S+|@\\S+', '', item)\n                item = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', item)\n                item = re.sub(r'[^a-z0-9\\s\\n]', '', item)\n                item = re.sub(r'\\s+', ' ', item).strip()\n                item = re.sub(r'\\d+', '', item).strip()\n\n                # replacing abbreviations \n                item = item.replace('structural equation model', 'sem')\n                item = item.replace('technology acceptance model', 'tam')\n                item = item.replace('unified theory of acceptance and use of technology', 'utaut')\n                item = item.replace('diffusion of innovation', 'doi')\n                item = item.replace('partial least squares', 'pls')\n                item = item.replace('theory of planned behavior', 'tpb')\n                item = re.sub(\"perceived usefulness\", \"pu\", item)\n                item = re.sub(\"perceived ease of use\", \"peou\", item)\n                item = re.sub(\"perceived privacy\", \"priv\", item)\n                item = re.sub(\"perceived aesthetics\", \"p_aest\", item)\n                item = re.sub(\"perceived relative advantage\", \"p_rel_adv\", item)\n                item = re.sub(\"perceived risk\", \"prisk\", item)\n                item = re.sub(\"perceived enjoyment\",\"penjy\", item)\n                item = re.sub(\"perceived intelligence\",\"pintlj\", item)\n                item = re.sub(\"perceived security\",\"psec\", item)\n                item = re.sub(\"perceived trust\",\"ptrst\", item)\n                item = re.sub(\"perceived anthropomorphism\",\"panthro\", item)\n                item = re.sub(\"perceived value\",\"pval\", item)\n                item = re.sub(\"perceived compatibility\",\"pcompat\", item)\n                item = re.sub(\"perceived detterants\",\"pdet\", item)\n                item = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", item)\n                item = re.sub(\"perceived credibility\",\"pcred\", item)\n                item = re.sub(\"perceived cost\",\"pcost\", item)\n                item = re.sub(\"perceived benefit\",\"pbenef\", item)\n                item = re.sub(\"perceived convenience\",\"pconv\", item)\n                item = re.sub(\"perceived usability\",\"pusbl\", item)\n                item = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", item)\n                \n                # belief base \n                item = re.sub(\"performance expectancy\", \"peex\", item)\n                item = re.sub(\"convenience\", \"conv\", item)\n                item = re.sub(\"effort expectancy\",\"efex\", item)\n                item = re.sub(\"access convenience\",\"acc_conv\", item)\n                item = re.sub(\"reliability\", \"rely\", item)\n                item = re.sub(\"behavioral control\", \"bhv_ctrl\", item)\n                item = re.sub(\"compatibility\", \"compat\", item)\n                item = re.sub(\"normative beliefs\", \"norm_blf\", item)\n                item = re.sub(\"normative belief\", \"norm_blf\", item)\n                item = re.sub(\"transaction convenience\",\"trans_conv\", item)\n                item = re.sub(\"post use trust\", \"post_trst\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"benefit convenience\",\"ben_conv\", item)\n                item = re.sub(\"search convenience\", \"srch_conv\", item)\n                item = re.sub(\"utilitarian expectation\", \"util_exp\", item)\n                item = re.sub(\"evaluation convenience\", \"eval_conv\", item)\n                item = re.sub(\"expectation\", \"expect\", item)\n                item = re.sub(\"possession convenience\", \"poss_conv\", item)\n                item = re.sub(\"expected advantage\", \"exp_adv\", item)\n               \n                # intention \n                item = re.sub(\"intention\", \"intnt\", item)\n                item = re.sub(\"motivation\", \"motiv\", item)\n                item = re.sub(\"automative motivation\",\"auto_motiv\", item)\n                item = re.sub(\"behavioral intention\",\"bhv_intnt\", item)\n                item = re.sub(\"control motivation\",\"ctrl_motiv\", item)\n                item = re.sub(\"controlled motivation\", \"ctrl_motiv\",  item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\", item)\n                item = re.sub(\"intention to use\",\"intnt_use\", item)\n                \n                # personal \n                item = re.sub(\"habit\", \"habt\", item)\n                item = re.sub(\"personality\",\"prsnl\",item)\n                item = re.sub(\"personal factors\",\"prsnl\",item)\n                item = re.sub(\"personal factor\", \"prsnl\", item)\n                item = re.sub(\"digital literacy\",\"dig_lit\",item)\n                item = re.sub(\"digital capability\",\"dig_cabl\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"financial literacy\",\"fin_lit\",item)\n                item = re.sub(\"previous experience\",\"prv_exp\",item)\n                item = re.sub(\"life compatibility\",\"life_compat\",item)\n                item = re.sub(\"lifestyle\", \"life\", item)\n                item = re.sub(\"knowledge\",\"know\",item)\n                item = re.sub(\"functional value\",\"fun_val\",item)\n                item = re.sub(\"fun value\", \"fun_val\", item)\n                item = re.sub(\"utalitarian value\",\"util_val\",item)\n                item = re.sub(\"epistemic value\",\"epi_val\",item)\n                item = re.sub(\"monetary value\",\"mon_val\",item)\n                item = re.sub(\"money value\",\"mon_val\",item)\n                item = re.sub(\"hedonic value\", \"hed_val\", item)\n                item = re.sub(\"emotional value\",\"emo_val\",item)\n                item = re.sub(\"quality value\",\"qual_val\",item)\n                item = re.sub(\"value barriers\", \"val_bar\", item)\n                item = re.sub(\"value barrier\",\"val_bar\",item)\n                item = re.sub(\"customer experience about usability\",\"exp_use\",item)\n                item = re.sub(\"experience\",\"exp\",item)\n                item = re.sub(\"self employment\",\"semp\",item)\n                item = re.sub(\"self-employment\",\"semp\",item)\n                item = re.sub(\"valence\",\"val\",item)\n                item = re.sub(\"religiosity\",\"religis\",item)\n                item = re.sub(\"task technology fit\",\"ttf\",item)\n                item = re.sub(\"lifestyle fit\",\"life_fit\",item)\n                    \n                # social \n                item = re.sub(\"social interactions on platforms\",\"soc_int_plt\", item)\n                item = re.sub(\"coercive pressures\", \"coe_prsr\",  item)\n                item = re.sub(\"coercive pressure\", \"coe_prsr\",  item)\n                item = re.sub(\"human human interaction\",\"hh_int\", item)\n                item = re.sub(\"human-human interaction\",\"hh_int\", item)\n                item = re.sub(\"social influence\", \"socinf\", item)\n                item = re.sub(\"collectivist cultural practices\", \"colcul\",  item)\n                item = re.sub(\"collectivist cultural practice\",\"colcul\", item)\n                item = re.sub(\"social media influence\",\"snsinf\", item)\n                item = re.sub(\"normative belief\",\"norm_blf\", item)\n                item = re.sub(\"interaction\",\"interac\", item)\n                item = re.sub(\"subjective norm\", \"sbj_nrm\",  item)\n                item = re.sub(\"subjective norms\", \"sbj_nrm\",  item)\n                item = re.sub(\"social factors\", \"soc_fac\",  item)\n                item = re.sub(\"social factor\" ,\"soc_fac\" , item)\n                item = re.sub(\"normative pressure\",\"nrm_prsr\", item)\n                item = re.sub(\"CSR economical responsibility\",\"csr_econ\", item)\n                item = re.sub(\"social norms\", \"soc_nrm\",  item)\n                item = re.sub(\"family influence\",\"fam_inf\", item)\n                item = re.sub(\"people\",\"people\", item)\n                item = re.sub(\"herd\",\"herd\", item)\n                item = re.sub(\"CSR social responsibility\",\"csr_soc\", item)\n                item = re.sub(\"mimetic pressure\", \"mim_prsr\",  item)\n                item = re.sub(\"CSR environmental responsibility\", \"csr_env\",  item)\n                item = re.sub(\"social value\",\"soc_val\", item)\n                item = re.sub(\"social values\",\"soc_val\", item)\n                item = re.sub(\"employee customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"employee-customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"social isolation\", \"soc_iso\",  item)\n                item = re.sub(\"normative pressures\",\"norm_prsr\", item)\n                item = re.sub(\"social proof social media\", \"soc_prf_sns\",  item)\n                item = re.sub(\"social proof\", \"soc_prf\",  item)\n                item = re.sub(\"social media\",\"sns\", item)\n                item = re.sub(\"word of mouth\", \"wom\",  item)\n                item = re.sub(\"wom\",\"wom\", item)\n                item = re.sub(\"word-of-mouth\", \"wom\",  item)\n                item = re.sub(\"w-o-m\", \"wom\",  item)\n                item = re.sub(\"wordmouth\", \"wom\",  item)\n                item = re.sub(\"environment\", \"env\",  item)\n                \n                \n                # psychological \n                item = re.sub(\"computer self efficacy\",\"self\",item)\n                item = re.sub(\"self efficacy\",\"self\",item)\n                item = re.sub(\"self-efficacy\", \"self\", item)\n                item = re.sub(\"attitude\", \"attd\", item)\n                item = re.sub(\"attitudes\",\"attd\",item)\n                item = re.sub(\"trust\",\"trst\",item)\n                item = re.sub(\"pragmatic\",\"prgt\",item)\n                item = re.sub(\"security concern\", \"sec_cn\",item)\n                item = re.sub(\"security concerns\",\"sec_cn\",item)\n                item = re.sub(\"self-image\", \"self_cong\", item)\n                item = re.sub(\"self image\",\"self_cong\",item)\n                item = re.sub(\"congruence\", \"cong\", item)\n                item = re.sub(\"self-congruence\",\"self_cong\",item)\n                item = re.sub(\"self congruence\", \"self_cong\", item)\n                item = re.sub(\"self-image congruence\",\"self_cong\",item)\n                item = re.sub(\"self image congruence\", \"self_cong\", item)\n                item = re.sub(\"selfimage congruence\", \"self_cong\", item)\n                item = re.sub(\"awareness\", \"awar\", item)\n                item = re.sub(\"satisfaction\",\"satis\",item)\n                item = re.sub(\"consumer satisfaction\",\"satis\",item)\n                item = re.sub(\"customer satisfaction\",\"satis\",item)\n                item = re.sub(\"restiant to change\",\"resist_chng\",item)\n                item = re.sub(\"resistance to change\",\"resist_chng\",item)\n                item = re.sub(\"risk aversion\", \"risk_avrs\", item)\n                item = re.sub(\"risk averse\",\"risk_avrs\",item)\n                item = re.sub(\"novelty\",\"new_seek\",item)\n                item = re.sub(\"novelty-seeking\", \"new_seek\", item)\n                item = re.sub(\"novelty seeking\", \"new_seek\", item)\n                item = re.sub(\"consciousnesnness\", \"conscn\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"post use trust\",\"post_trst\",item)\n                item = re.sub(\"postuse trust\",\"post_trst\",item)\n                item = re.sub(\"emotional experience\",\"emo_exp\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"privacy concerns\",\"priv_cn\",item)\n                item = re.sub(\"privacy concern\",\"priv_cn\",item)\n                item = re.sub(\"cognitive decline\",\"cog_dec\",item)\n                item = re.sub(\"benevolent convenince\",\"ben_conv\",item)\n                item = re.sub(\"enjoyment\",\"enjy\",item)\n                item = re.sub(\"enjoy\", \"enjy\", item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\",item)\n                item = re.sub(\"oppenness\", \"open\", item)\n                item = re.sub(\"loyal\", \"loyal\", item)\n                item = re.sub(\"loyalty\",\"loyal\",item)\n                item = re.sub(\"confirmation\",\"confrm\",item)\n                item = re.sub(\"optimism\",\"optim\",item)\n                item = re.sub(\"safety concerns\",\"safe_cn\",item)\n                item = re.sub(\"safety concern\",\"safe_cn\",item)\n                item = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"green concerns\",\"green_cn\",item)\n                item = re.sub(\"technology anxiety\", \"anxiety\", item)\n                item = re.sub(\"anxiety\",\"anxiety\",item)\n                item = re.sub(\"obedience\", \"obed\", item)\n                item = re.sub(\"empathy\",\"empath\",item)\n                item = re.sub(\"decision comfort\",\"dec_comfrt\",item)\n                item = re.sub(\"confidence\",\"confdnc\",item)\n                item = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", item)\n                item = re.sub(\"comfort\", \"cmfrt\", item)\n                item = re.sub(\"discomfort\",\"discmfrt\",item)\n                item = re.sub(\"insecurity\", \"insec\", item)\n                item = re.sub(\"insecurities\", \"insec\", item)\n                item = re.sub(\"benevolence\",\"benv\",item)\n                item = re.sub(\"technology stress\",\"tech_strss\",item)\n                item = re.sub(\"stress\", \"tech_strss\", item)\n                item = re.sub(\"techno-stress\",\"tech_strss\",item)\n                item = re.sub(\"technostress\",\"tech_strss\",item)\n                item = re.sub(\"techno stress\", \"tech_strss\", item)\n                item = re.sub(\"cognitive resistence\",\"cog_resist\",item)\n                        \n                # demographic \n                item = re.sub(\"age\",\"age\",item)\n                item = re.sub(\"sex\",\"sex\",item)\n                item = re.sub(\"education\",\"edu\",item)\n                item = re.sub(\"income\",\"income\",item)\n                item = re.sub(\"islamic religiosity\",\"religios\",item)\n                item = re.sub(\"culture\",\"cltr\",item)\n                \n                item = \" \".join([word for word in item.split() if word not in stop_words])\n                item = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in item.split()])\n                #item = \" \".join([stemmer.stem(word) for word in item.split()])\n                processed_list.append(item)\n            dct[k] = \" \".join(processed_list)\n        else:\n            v = v.lower()\n            v = re.sub(r'http\\S+|www\\S+|@\\S+', '', v)\n            v = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', v)\n            v = re.sub(r'[^a-z0-9\\s\\n]', '', v)\n            v = re.sub(r'\\s+', ' ', v).strip()\n            v = re.sub(r'\\d+', '', v).strip()\n\n            # replacing abbreviations \n            v = v.replace('structural equation model', 'sem')\n            v = v.replace('technology acceptance model', 'tam')\n            v = v.replace('unified theory of acceptance and use of technology', 'utaut')\n            v = v.replace('diffusion of innovation', 'doi')\n            v = v.replace('partial least squares', 'pls')\n            v = v.replace('theory of planned behavior', 'tpb')\n            v = re.sub(\"perceived usefulness\", \"pu\", v)\n            v = re.sub(\"perceived ease of use\", \"peou\", v)\n            v = re.sub(\"perceived privacy\", \"priv\", v)\n            v = re.sub(\"perceived aesthetics\", \"p_aest\", v)\n            v = re.sub(\"perceived relative advantage\", \"p_rel_adv\", v)\n            v = re.sub(\"perceived risk\", \"prisk\", v)\n            v = re.sub(\"perceived enjoyment\",\"penjy\", v)\n            v = re.sub(\"perceived intelligence\",\"pintlj\", v)\n            v = re.sub(\"perceived security\",\"psec\", v)\n            v = re.sub(\"perceived trust\",\"ptrst\", v)\n            v = re.sub(\"perceived anthropomorphism\",\"panthro\", v)\n            v = re.sub(\"perceived value\",\"pval\", v)\n            v = re.sub(\"perceived compatibility\",\"pcompat\", v)\n            v = re.sub(\"perceived detterants\",\"pdet\", v)\n            v = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", v)\n            v = re.sub(\"perceived credibility\",\"pcred\", v)\n            v = re.sub(\"perceived cost\",\"pcost\", v)\n            v = re.sub(\"perceived benefit\",\"pbenef\", v)\n            v = re.sub(\"perceived convenience\",\"pconv\", v)\n            v = re.sub(\"perceived usability\",\"pusbl\", v)\n            v = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", v)\n                \n            v = re.sub(\"performance expectancy\", \"peex\", v)\n            v = re.sub(\"convenience\", \"conv\", v)\n            v = re.sub(\"effort expectancy\",\"efex\", v)\n            v = re.sub(\"access convenience\",\"acc_conv\", v)\n            v = re.sub(\"reliability\", \"rely\", v)\n            v = re.sub(\"behavioral control\", \"bhv_ctrl\", v)\n            v = re.sub(\"compatibility\", \"compat\", v)\n            v = re.sub(\"normative beliefs\", \"norm_blf\", v)\n            v = re.sub(\"normative belief\", \"norm_blf\", v)\n            v = re.sub(\"transaction convenience\",\"trans_conv\", v)\n            v = re.sub(\"post use trust\", \"post_trst\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"benefit convenience\",\"ben_conv\", v)\n            v = re.sub(\"search convenience\", \"srch_conv\", v)\n            v = re.sub(\"utilitarian expectation\", \"util_exp\", v)\n            v = re.sub(\"evaluation convenience\", \"eval_conv\", v)\n            v = re.sub(\"expectation\", \"expect\", v)\n            v = re.sub(\"possession convenience\", \"poss_conv\", v)\n            v = re.sub(\"expected advantage\", \"exp_adv\", v)\n            \n            # intention \n            v = re.sub(\"intention\", \"intnt\", v)\n            v = re.sub(\"motivation\", \"motiv\", v)\n            v = re.sub(\"automative motivation\",\"auto_motiv\", v)\n            v = re.sub(\"behavioral intention\",\"bhv_intnt\", v)\n            v = re.sub(\"control motivation\",\"ctrl_motiv\", v)\n            v = re.sub(\"controlled motivation\", \"ctrl_motiv\",  v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\", v)\n            v = re.sub(\"intention to use\",\"intnt_use\", v)\n                \n                # personal \n            v = re.sub(\"habit\", \"habt\", v)\n            v = re.sub(\"personality\",\"prsnl\",v)\n            v = re.sub(\"personal factors\",\"prsnl\",v)\n            v = re.sub(\"personal factor\", \"prsnl\", v)\n            v = re.sub(\"digital literacy\",\"dig_lit\",v)\n            v = re.sub(\"digital capability\",\"dig_cabl\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"financial literacy\",\"fin_lit\",v)\n            v = re.sub(\"previous experience\",\"prv_exp\",v)\n            v = re.sub(\"life compatibility\",\"life_compat\",v)\n            v = re.sub(\"lifestyle\", \"life\", v)\n            v = re.sub(\"knowledge\",\"know\",v)\n            v = re.sub(\"functional value\",\"fun_val\",v)\n            v = re.sub(\"fun value\", \"fun_val\", v)\n            v = re.sub(\"utalitarian value\",\"util_val\",v)\n            v = re.sub(\"epistemic value\",\"epi_val\",v)\n            v = re.sub(\"monetary value\",\"mon_val\",v)\n            v = re.sub(\"money value\",\"mon_val\",v)\n            v = re.sub(\"hedonic value\", \"hed_val\", v)\n            v = re.sub(\"emotional value\",\"emo_val\",v)\n            v = re.sub(\"quality value\",\"qual_val\",v)\n            v = re.sub(\"value barriers\", \"val_bar\", v)\n            v = re.sub(\"value barrier\",\"val_bar\",v)\n            v = re.sub(\"customer experience about usability\",\"exp_use\",v)\n            v = re.sub(\"experience\",\"exp\",v)\n            v = re.sub(\"self employment\",\"semp\",v)\n            v = re.sub(\"self-employment\",\"semp\",v)\n            v = re.sub(\"valence\",\"val\",v)\n            v = re.sub(\"religiosity\",\"religis\",v)\n            v = re.sub(\"task technology fit\",\"ttf\",v)\n            v = re.sub(\"lifestyle fit\",\"life_fit\",v)\n                    \n                # social \n            v = re.sub(\"social interactions on platforms\",\"soc_int_plt\", v)\n            v = re.sub(\"coercive pressures\", \"coe_prsr\",  v)\n            v = re.sub(\"coercive pressure\", \"coe_prsr\",  v)\n            v = re.sub(\"human human interaction\",\"hh_int\", v)\n            v = re.sub(\"human-human interaction\",\"hh_int\", v)\n            v = re.sub(\"social influence\", \"socinf\", v)\n            v = re.sub(\"collectivist cultural practices\", \"colcul\",  v)\n            v = re.sub(\"collectivist cultural practice\",\"colcul\", v)\n            v = re.sub(\"social media influence\",\"snsinf\", v)\n            v = re.sub(\"normative belief\",\"norm_blf\", v)\n            v = re.sub(\"interaction\",\"interac\", v)\n            v = re.sub(\"subjective norm\", \"sbj_nrm\",  v)\n            v = re.sub(\"subjective norms\", \"sbj_nrm\",  v)\n            v = re.sub(\"social factors\", \"soc_fac\",  v)\n            v = re.sub(\"social factor\" ,\"soc_fac\" , v)\n            v = re.sub(\"normative pressure\",\"nrm_prsr\", v)\n            v = re.sub(\"CSR economical responsibility\",\"csr_econ\", v)\n            v = re.sub(\"social norms\", \"soc_nrm\",  v)\n            v = re.sub(\"family influence\",\"fam_inf\", v)\n            v = re.sub(\"people\",\"people\", v)\n            v = re.sub(\"herd\",\"herd\", v)\n            v = re.sub(\"CSR social responsibility\",\"csr_soc\", v)\n            v = re.sub(\"mimetic pressure\", \"mim_prsr\",  v)\n            v = re.sub(\"CSR environmental responsibility\", \"csr_env\",  v)\n            v = re.sub(\"social value\",\"soc_val\", v)\n            v = re.sub(\"social values\",\"soc_val\", v)\n            v = re.sub(\"employee customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"employee-customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"social isolation\", \"soc_iso\",  v)\n            v = re.sub(\"normative pressures\",\"norm_prsr\", v)\n            v = re.sub(\"social proof social media\", \"soc_prf_sns\",  v)\n            v = re.sub(\"social proof\", \"soc_prf\",  v)\n            v = re.sub(\"social media\",\"sns\", v)\n            v = re.sub(\"word of mouth\", \"wom\",  v)\n            v = re.sub(\"wom\",\"wom\", v)\n            v = re.sub(\"word-of-mouth\", \"wom\",  v)\n            v = re.sub(\"w-o-m\", \"wom\",  v)\n            v = re.sub(\"wordmouth\", \"wom\",  v)\n            v = re.sub(\"environment\", \"env\",  v)\n                \n                \n                # psychological \n            v = re.sub(\"computer self efficacy\",\"self\",v)\n            v = re.sub(\"self efficacy\",\"self\",v)\n            v = re.sub(\"self-efficacy\", \"self\", v)\n            v = re.sub(\"attitude\", \"attd\", v)\n            v = re.sub(\"attitudes\",\"attd\",v)\n            v = re.sub(\"trust\",\"trst\",v)\n            v = re.sub(\"pragmatic\",\"prgt\",v)\n            v = re.sub(\"security concern\", \"sec_cn\",v)\n            v = re.sub(\"security concerns\",\"sec_cn\",v)\n            v = re.sub(\"self-image\", \"self_cong\", v)\n            v = re.sub(\"self image\",\"self_cong\",v)\n            v = re.sub(\"congruence\", \"cong\", v)\n            v = re.sub(\"self-congruence\",\"self_cong\",v)\n            v = re.sub(\"self congruence\", \"self_cong\", v)\n            v = re.sub(\"self-image congruence\",\"self_cong\",v)\n            v = re.sub(\"self image congruence\", \"self_cong\", v)\n            v = re.sub(\"selfimage congruence\", \"self_cong\", v)\n            v = re.sub(\"awareness\", \"awar\", v)\n            v = re.sub(\"satisfaction\",\"satis\",v)\n            v = re.sub(\"consumer satisfaction\",\"satis\",v)\n            v = re.sub(\"customer satisfaction\",\"satis\",v)\n            v = re.sub(\"restiant to change\",\"resist_chng\",v)\n            v = re.sub(\"resistance to change\",\"resist_chng\",v)\n            v = re.sub(\"risk aversion\", \"risk_avrs\", v)\n            v = re.sub(\"risk averse\",\"risk_avrs\",v)\n            v = re.sub(\"novelty\",\"new_seek\",v)\n            v = re.sub(\"novelty-seeking\", \"new_seek\", v)\n            v = re.sub(\"novelty seeking\", \"new_seek\", v)\n            v = re.sub(\"consciousnesnness\", \"conscn\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"post use trust\",\"post_trst\",v)\n            v = re.sub(\"postuse trust\",\"post_trst\",v)\n            v = re.sub(\"emotional experience\",\"emo_exp\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"privacy concerns\",\"priv_cn\",v)\n            v = re.sub(\"privacy concern\",\"priv_cn\",v)\n            v = re.sub(\"cognitive decline\",\"cog_dec\",v)\n            v = re.sub(\"benevolent convenince\",\"ben_conv\",v)\n            v = re.sub(\"enjoyment\",\"enjy\",v)\n            v = re.sub(\"enjoy\", \"enjy\", v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\",v)\n            v = re.sub(\"oppenness\", \"open\", v)\n            v = re.sub(\"loyal\", \"loyal\", v)\n            v = re.sub(\"loyalty\",\"loyal\",v)\n            v = re.sub(\"confirmation\",\"confrm\",v)\n            v = re.sub(\"optimism\",\"optim\",v)\n            v = re.sub(\"safety concerns\",\"safe_cn\",v)\n            v = re.sub(\"safety concern\",\"safe_cn\",v)\n            v = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"green concerns\",\"green_cn\",v)\n            v = re.sub(\"technology anxiety\", \"anxiety\", v)\n            v = re.sub(\"anxiety\",\"anxiety\",v)\n            v = re.sub(\"obedience\", \"obed\", v)\n            v = re.sub(\"empathy\",\"empath\",v)\n            v = re.sub(\"decision comfort\",\"dec_comfrt\",v)\n            v = re.sub(\"confidence\",\"confdnc\",v)\n            v = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", v)\n            v = re.sub(\"comfort\", \"cmfrt\", v)\n            v = re.sub(\"discomfort\",\"discmfrt\",v)\n            v = re.sub(\"insecurity\", \"insec\", v)\n            v = re.sub(\"insecurities\", \"insec\", v)\n            v = re.sub(\"benevolence\",\"benv\",v)\n            v = re.sub(\"technology stress\",\"tech_strss\",v)\n            v = re.sub(\"stress\", \"tech_strss\", v)\n            v = re.sub(\"techno-stress\",\"tech_strss\",v)\n            v = re.sub(\"technostress\",\"tech_strss\",v)\n            v = re.sub(\"techno stress\", \"tech_strss\", v)\n            v = re.sub(\"cognitive resistence\",\"cog_resist\",v)\n                    \n            # demographic \n            v = re.sub(\"age\",\"age\",v)\n            v = re.sub(\"sex\",\"sex\",v)\n            v = re.sub(\"education\",\"edu\",v)\n            v = re.sub(\"income\",\"income\",v)\n            v = re.sub(\"islamic religiosity\",\"religios\",v)\n            v = re.sub(\"culture\",\"cltr\",v)\n                \n            v = \" \".join([word for word in v.split() if word not in stop_words])\n            v = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in v.split()])\n            #v = \" \".join([stemmer.stem(word) for word in v.split()])\n            dct[k] = v\n    return dct\n\ndef tokenizeToSentences(doc):\n    for k, v in doc.items():\n        \n        if isinstance(v, bytes):\n            v = v.decode('utf-8')\n          \n        v = v.lower()\n        v = v.replace('\\n', ' ')\n        v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n        #v = \" \".join([str(s) for s in v])\n\n        v = sent_tokenize(v)\n        doc[k] = v\n        \n    return doc\n\nFor Topic modeling, I write a function to generate dictionaries and save them in a .mm file format.\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n\n    return dictionary, corpus\n\nAdditionally, I want a function that prints the top 50 most frequently appearing words in the corpus:\n\n# ---------------------- START OF CHATGPT CODE\ndef print_top_50_words(corpus, dictionary):\n    total_word_count = defaultdict(int)\n    word_weights = defaultdict(float)\n\n    for word_id, word_count in itertools.chain.from_iterable(corpus):\n        total_word_count[word_id] += word_count\n\n    sorted_tota_words_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n\n    tfidf = TfidfModel(corpus)\n\n\n    for doc in corpus:\n        tfidf_weights = tfidf[doc]  # Calculate TF-IDF for the review\n        for term_id, weight in tfidf_weights:\n            word_weights[term_id] += weight  # Aggregate the weight for the term\n\n    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n\n    # Print the top 50 terms with their weights\n    top_50_words = [(dictionary.get(term_id), weight) for term_id, weight in sorted_word_weights[:50]]\n\n    for word, weight in top_50_words:\n        print(word, weight)\n\n# ---------------------- END OF CHATGPT CODE \n\nI also plan on seeing how python clusters the words (as in, finds similar words) vs me:\n\ndef print_clusters(n_clusters, list_of_words):\n    clusters = {i: [] for i in range(n_clusters)}\n    for word, label in zip(list_of_words, labels):\n        clusters[label].append(word)\n\n    for label, words in clusters.items():\n        print(f\"Cluster {label}:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n    # Explain clusters\n    print(\"Cluster explanations based on semantics and ideas:\")\n    for label, words in clusters.items():\n        print(f\"Cluster {label} might be related to:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\nThis is a function for if you want to use a word embedding (requires some effort, time and machine power!):\n\ndef get_embedding(text):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model_bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\nAnd then you use this to get semantically similar words:\n\ndef get_semantically_similar_words(words, threshold=0.7):\n    similar_words = set(words)\n    for word in words:\n        token = nlp(word)\n        for vocab_word in nlp.vocab:\n            if vocab_word.has_vector and vocab_word.is_alpha:\n                similarity = token.similarity(nlp(vocab_word.text))\n                if similarity &gt;= threshold:\n                    similar_words.add(vocab_word.text)\n    return similar_words\n\nSo, how do I find the themes? Essentially, I just tweaked TF-IDF:\n\nclass CustomTfidfVectorizer(TfidfVectorizer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        #self.general_keywords = set(general_keywords)\n        \n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)] #if w not in self.general_keywords]\n    \n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        # Penalize general keywords\n        #penalized_idf = self.idf_ * (1 - 0.8 * np.isin(self.get_feature_names_out(), list(self.general_keywords)))\n        \n        # Apply penalized IDF\n        augmented_tfidf = augmented_tf * penalized_idf\n\n        return csr_matrix(augmented_tfidf)\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\n\n\nKeyword Analytics\n\ntry:\n    df = pd.read_csv(\"P2_AR_04.csv\", encoding='utf-8')\nexcept UnicodeDecodeError:\n    try:\n        df = pd.read_csv(\"P2_AR_04.csv\", encoding='latin-1')\n    except Exception as e:\n        error_message = str(e)\n        df = None\n\nClean all the data you’ve gathered the same way the PDF’s have been cleaned (the preprocess_text() function looks very similar to the cleaning function above!):\n\ndf2 = df.copy()\n\ncolumns_to_preprocess = ['Man_Theme',\n                        'K1','K2','K3','K4','K5','K6','K7','K8','K9','K10',\n                        'F1','F2','F3','F4','F5','F6','F7','F8','F9',\n                        'FNS1','FNS2','FNS3','FNS4',\n                        'METHOD1','METHOD2','METHOD3','METHOD4',\n                        'THEORY1','THEORY2','THEORY3','THEORY4',\n                        'LIMIT1' ,'LIMIT2' ,'LIMIT3', 'Abstract'\n                        ]\n\nfor col in columns_to_preprocess:\n    df2[col] = df2[col].apply(preprocess_text)\n\n\npapers = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    papers[paper_id] = text\n\npapers_df = pd.DataFrame.from_dict(papers, orient = 'index', columns = ['paperText'])\npapers_df = papers_df.reset_index(names = ['paperID'])\npapers_df.to_csv('papers_unclean.csv')\npapers_df.head()\n\nThese look like this: \n\n# keep a copy (a habit of mine)\npapers_uncleaned = papers.copy() \ntheme_of_words_uncleaned = theme_of_words.copy() \n\n# clean up all papers \npapers_cleaned = preprocess_Dict(papers)\npapersClean_df = pd.DataFrame.from_dict(papers_cleaned, orient = 'index', columns = ['paperText'])\npapersClean_df = papersClean_df.reset_index(names = ['paperID'])\npapersClean_df.to_csv('papers_clean3.csv')\npapersClean_df.head()\n\n\nClean the theme of words dictionary so the words match:\n\ntheme_of_words_cleaned = {}\n\nfor k, v in theme_of_words.items():\n    theme_of_words_cleaned[k] = preprocess_list(v)\n\ntheme_of_words_cleaned['psychological'][:10]\n\n\n\n\n[‘initial trst’,‘simcong’,‘hedmotiv’,‘selfefficacy’,‘strss’,‘controlled motiv’,‘cong’,‘selfcong cong’,‘trst systems’,‘custom loyalti’]\n\n\n\nMake sure to drop all NA’s and empty values:\n\nfor k, v in theme_of_words_cleaned.items():\n    theme_of_words_cleaned[k] = [x for x in v if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n\n\nThemes Based on Count of Words in Each Group\nI will skip the parts on BERT and word embeddings that’s in the jupyter notebook as these were not used for my project. The reason is that it was taking so long and my computer simply did not have the capacity to handle it. I also didn’t have the time to find a fix for it, but there are a bunch of commented-out code from chatGPT that I was playing around with.\nSo, we now have a dictionary with the factors and their theme, and a corpus of text of all papers in the dataset. What I’m going to do here is: * Count all the words total_words in each paper * Count the instances of each word (factor) in theme_of_words_cleaned in each paper * Each word’s count adds 1 point to its corresponding theme’s “weight score” * Take for example the word emotion and I said the theme for this word is psychological. If emotion shows up 10 times in paper 1, paper 1’s dictionary of weights has a weight of 10 for (divided by the total number of words) psychological.\n\nresults = []\ncount_words_df = pd.DataFrame(results)\n\n\nfor doc_id, text in papers_cleaned.items():\n    doc = nlp(text)\n    word_counts = defaultdict(int)\n\n    for token in doc:\n        for group, keywords in theme_of_words_cleaned.items():\n            if token.text.lower() in keywords:\n                word_counts[group] += 1\n\n    total_words = len(doc)\n    group_weights = {f\"{group}_w\": count / total_words for group, count in word_counts.items()}\n    max_weight = max(group_weights.values(), default=0)\n    theme = max(group_weights, key=group_weights.get).replace(\"_w\", \"\") if max_weight &gt; 0 else None\n\n    result = {\"doc_id\": doc_id, **word_counts, **group_weights, \"theme\": theme, \"max_weight\": max_weight}\n    results.append(result)\n\n\nfor group in theme_of_words_cleaned.keys():\n    if group not in count_words_df.columns:\n        count_words_df[group] = 0\n    if f\"{group}_w\" not in count_words_df.columns:\n        count_words_df[f\"{group}_w\"] = 0.0\n\ncount_words_df.head()\n\n\nThis is still very basic, though because it’s only based on the factors which may not be fully representative. So, I do this again using keywords in addition to the factors. These are keywords that were selected by the authors as well as information extracted from Web of Science \\BibTeX file.\n\ncols_toPick = ['K1','K2','K3','K4','K5','K6','K7','K8','K9','K10','F1','F2','F3','F4','F5','F6','F7','F8','F9']\n\nkeywordsDf = df2.loc[:,cols_toPick]\n\n# flatten the dataframe to a list \nkeywords_across_db = keywordsDf.values.flatten().tolist()\n\n# there are 2,717 words here \nprint(\"number of words (factors and keywords) in total \", len(keywords_across_db))\n\n# making sure there are no empty/NaN/Null values \nkeywords_across_db = [x for x in keywords_across_db if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n# making sure there are no duplicates (set takes care of this)\nkeywords_across_db_nodup_cleaned = list(set(keywords_across_db))\n\n# convert the list into a dictionary temporarily, then convert it to a dataframe \ntemp = {'Keyword': keywords_across_db_nodup_cleaned}\nkeywords_themes_df = pd.DataFrame(temp, columns=['Keyword'])\n\n# go back to the theme_of_words_cleaned and find each keyword's theme \nkeywords_themes_dic = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\n# This is just a dataframe view of the keywords with their respective theme \nkeywords_themes_df['Theme'] = keywords_themes_df['Keyword'].map(keywords_themes_dic)\n\n# if the theme is empty, give it \"Generic\" - that means these keywords weren't in the list of important words that we picked themes for \nkeywords_themes_df['Theme'] = keywords_themes_df['Theme'].apply(lambda x: 'Generic' if pd.isna(x) or x == ' ' else x)\n\nkeywords_themes_df.head()\n\n\nI’m gonna get rid of all the Generic keywords, so keeping a copy of this dataframe:\n\nkeywords_themes_df_withGenerics = keywords_themes_df.copy()\n\nkeywords_themes_df['Theme'] = keywords_themes_df['Theme'].apply(lambda x: 'Generic' if pd.isna(x) or x == ' ' else x)\n\n# everything but generic \nkeywords_themes_df = keywords_themes_df.loc[keywords_themes_df['Theme'] != 'Generic']\n\n# flatten it to build a vocabulary \nwords_acrossAll_nonGeneric = keywords_themes_df['Keyword'].values.flatten().tolist()\n\n# making sure no null values were generated \nwords_acrossAll_nonGeneric = [x for x in words_acrossAll_nonGeneric if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n# making sure there are no dulicates (233 words total)\nwords_acrossAll_nonGeneric = list(set(words_acrossAll_nonGeneric))\n\n\n\nTheme Assignment\nI will now use the custom TF-IDF class to generate a TF-IDF matrix. This is similar to what I did by hand a bit further above. Basically, all TF-IDF is doing is counting the frequency of words across the document.\n\nvectorizer_keys = CustomTfidfVectorizer(vocabulary = words_acrossAll_nonGeneric)\n\ntfidf_matrix = vectorizer_keys.fit_transform(papers_cleaned.values())\n\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index = papers_cleaned.keys(), columns=vectorizer_keys.get_feature_names_out())\n\ntfidf_df.head()\n\n\nNow using TF-IDF scores, finding the weights for themes for each paper:\n\nkeyword_to_theme = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\ntheme_weights = pd.DataFrame(0, index=tfidf_df.index, columns=theme_of_words_cleaned.keys())\n\nfor keyword, theme in keyword_to_theme.items():\n    if keyword in tfidf_df.columns:\n        theme_weights[theme] += tfidf_df[keyword]\n\n\nfor _, row in df2.iterrows():\n    paper_id = row['ID']\n    keywords = words_acrossAll_nonGeneric \n    \n    for keyword in keywords:\n        if keyword in tfidf_df.columns:\n            theme = keyword_to_theme.get(keyword, None)\n            if theme:\n                #if theme in tfidf_df.index:\n                theme_weights.at[paper_id, theme] += tfidf_df.at[paper_id, keyword] * 5  \n\n# this is picking just 1 theme per paper - find the theme with the maximum weight as the main theme of the paper \nmain_theme_df = theme_weights.apply(lambda row: (row == row.max()).astype(int), axis=1)\n\nmain_theme_df.head()\n\n\nVisualizing this:\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n\n# Heatmap for theme weights\nsns.heatmap(theme_weights.T, ax=axes[0], cmap=\"YlGnBu_r\", cbar_kws={'label': 'Weight'})\naxes[0].set_xlabel(\"Paper ID\")\naxes[0].set_ylabel(\"Theme\")\naxes[0].set_title(\"Theme Weights per Paper\")\n\n# Heatmap for main themes\nsns.heatmap(main_theme_df.T, ax=axes[1], cmap=\"YlGnBu_r\", cbar_kws={'label': 'Theme Presence'})\naxes[1].set_xlabel(\"Paper ID\")\naxes[1].set_ylabel(\"Theme\")\naxes[1].set_title(\"Main Theme per Paper\")\n\nplt.tight_layout()\nplt.show()\nplt.savefig('main_themes_heatmap_1.png')\n\n\nTo extract the themes easily:\n\n# Extract themes for each paper\nthemes_for_papers = {\n    paper_id: main_theme_df.columns[row.astype(bool)].tolist()\n    for paper_id, row in main_theme_df.iterrows()\n}\n\n# Print the themes for each paper\nfor paper_id, themes in themes_for_papers.items():\n    print(f\"Paper ID: {paper_id}, Themes: {', '.join(themes)}\")\n\n\nPaper ID: p2_01, Themes: psychological Paper ID: p2_02, Themes: psychological Paper ID: p2_03, Themes: psychological Paper ID: p2_04, Themes: psychological Paper ID: p2_05, Themes: psychological Paper ID: p2_06, Themes: perceptive Paper ID: p2_07, Themes: personal Paper ID: p2_08, Themes: personal Paper ID: p2_09, Themes: perceptive Paper ID: p2_10, Themes: psychological\n\nSince Topic Modeling is Multimembership, papers can have more than just 1 theme. Since I didn’t use a word embedding and may not get the best representatitve theme here, I decided to allow for up to 3 themes. To do this, I changed the CustomTfidfVectorizer class.\n\n\n\nALLOWING FOR MULTIPLE THEMES\n\nclass CustomTfidfVectorizerUpdateClass(TfidfVectorizer):\n    def __init__(self, theme_keywords, threshold=0.8, **kwargs):\n        # Generate vocabulary from theme_keywords\n        vocabulary = list(set(word for words in theme_keywords.values() for word in words))\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        self.threshold = threshold  # Threshold for determining multiple themes\n        self.theme_keywords = theme_keywords  # Store theme_keywords for later use\n\n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)]\n\n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        augmented_tfidf = augmented_tf  # No penalized IDF applied here\n\n        return csr_matrix(augmented_tfidf)\n\n    def determine_themes(self, documents_dict):\n        \"\"\"\n        Determines the themes for each document based on TF-IDF scores.\n        A paper can have multiple themes if the scores are within the threshold.\n\n        :param documents_dict: Dictionary of documents (keys: IDs, values: text)\n        :return: Dictionary where keys are document IDs and values are lists of themes\n        \"\"\"\n        document_ids = list(documents_dict.keys())\n        raw_documents = list(documents_dict.values())\n        \n        X = self.transform(raw_documents).toarray()\n        feature_name_to_index = {name: i for i, name in enumerate(self.get_feature_names_out())}\n\n        theme_scores = {}\n        for doc_index, doc_vector in enumerate(X):\n            doc_id = document_ids[doc_index]\n            # Calculate scores for each theme\n            scores = {\n                theme: sum(doc_vector[feature_name_to_index[word]]\n                           for word in keywords if word in feature_name_to_index)\n                for theme, keywords in self.theme_keywords.items()\n            }\n            max_score = max(scores.values()) if scores else 0\n            \n            # Determine themes within the threshold\n            selected_themes = [\n                theme for theme, score in scores.items()\n                if score &gt;= self.threshold * max_score\n            ]\n            theme_scores[doc_id] = selected_themes\n\n        return theme_scores\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\nSimilar to the above task, I generate the vectorizer from theme_of_words_cleaned vocabulary, but this time, allow for a few more themes (threshold = 0.75).\n\nvectorizer_keys2 = CustomTfidfVectorizerUpdateClass(theme_keywords = theme_of_words_cleaned, threshold = 0.75) \n\ntfidf_matrix2 = vectorizer_keys2.fit_transform(papers_cleaned.values())\n\ntfidf_df2 = pd.DataFrame(tfidf_matrix2.toarray(), index=papers_cleaned.keys(), columns=vectorizer_keys2.get_feature_names_out())\ntfidf_df2.head()\n\n\nYou can still pick a main theme for each paper, but I want to see the top 3 themes:\n\nkeyword_to_theme2 = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\ntheme_weights2 = pd.DataFrame(0, index=tfidf_df2.index, columns=theme_of_words_cleaned.keys())\n\nfor keyword, theme in keyword_to_theme2.items():\n    if keyword in tfidf_df2.columns:\n        theme_weights2[theme] += tfidf_df2[keyword]\n\nfor _, row in df2.iterrows():\n    paper_id = row['ID']\n    #for keyword in words_acrossAll_nonGeneric:\n    for keyword in vectorizer_keys2.get_feature_names_out():\n        if keyword in tfidf_df2.columns:\n            theme = keyword_to_theme2.get(keyword, None)\n            if theme:\n                # the 5 I'm adding here is just to make the weights a bit larger for visualization. It's a simple scaling and won't change the results \n                theme_weights2.at[paper_id, theme] += tfidf_df2.at[paper_id, keyword] * 5\n\n\ntop_3_themes_for_papers = {\n    paper_id: theme_weights2.loc[paper_id]\n    .sort_values(ascending=False)[:3]  \n    .index.tolist()\n    for paper_id in theme_weights2.index\n}\n\nfor paper_id, themes in top_3_themes_for_papers.items():\n    print(f\"Paper ID: {paper_id}, Top 3 Themes: {', '.join(themes)}\")\n\n\nPaper ID: p2_01, Top 3 Themes: psychological, cultural, demographic Paper ID: p2_02, Top 3 Themes: psychological, market, personal Paper ID: p2_03, Top 3 Themes: psychological, personal, perceptive Paper ID: p2_04, Top 3 Themes: psychological, personal, perceptive Paper ID: p2_05, Top 3 Themes: cultural, personal, psychological Paper ID: p2_06, Top 3 Themes: perceptive, cultural, psychological Paper ID: p2_07, Top 3 Themes: personal, psychological, demographic Paper ID: p2_08, Top 3 Themes: psychological, personal, perceptive Paper ID: p2_09, Top 3 Themes: perceptive, psychological, personal Paper ID: p2_10, Top 3 Themes: psychological, cultural, personal\n\nKeep a copy and save this to file:\n\ndf3 = df2.copy()\n\ntop_3_themes_for_papers = {\n    paper_id: \", \".join(theme_weights2.loc[paper_id]\n                         .sort_values(ascending=False)[:3]\n                         .index.tolist())\n    for paper_id in theme_weights2.index\n}\n\ndf3[\"Algo_Theme\"] = df2[\"ID\"].map(top_3_themes_for_papers)\n\ndf3.to_csv(\"NewWithThemes.csv\")",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Algorithmic Approach to Finding Themes"
    ]
  },
  {
    "objectID": "study1_DA.html",
    "href": "study1_DA.html",
    "title": "Data Analysis",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-3.-results",
    "href": "study1_DA.html#part-3.-results",
    "title": "Data Analysis",
    "section": "Part 3. Results",
    "text": "Part 3. Results",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-1-data-cleaning-and-prep",
    "href": "study1_DA.html#part-1-data-cleaning-and-prep",
    "title": "Data Analysis",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nRows: 143\nColumns: 62\n$ Reason_Theme       &lt;chr&gt; \"comparative\", \"behavioral intention\", \"keywords\", …\n$ CiteKey            &lt;chr&gt; \"LonkaniAcomparativeStudyOfTrust2020\", \"SaprikisACo…\n$ ID                 &lt;chr&gt; \"p2_01\", \"p2_02\", \"p2_03\", \"p2_04\", \"p2_05\", \"p2_06…\n$ Algo_Theme         &lt;chr&gt; \"psychological, cultural, demographic\", \"psychologi…\n$ DecisionTheme      &lt;chr&gt; \"cultural\", \"psychological\", \"psychological\", \"cult…\n$ Man_Theme          &lt;chr&gt; \"cultural, psychological\", \"social, psychological\",…\n$ Match              &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Title              &lt;chr&gt; \"a comparative study of trust in mobile banking: an…\n$ Year               &lt;int&gt; 2020, 2022, 2021, 2019, 2020, 2020, 2024, 2020, 201…\n$ PublicationTitle   &lt;chr&gt; \"journal of global information management\", \"inform…\n$ Creators           &lt;chr&gt; \"ravi lonkani, chuleeporn changchit, tim klaus, jom…\n$ Publisher          &lt;chr&gt; \"igi global\", \"mdpi\", \"igi global\", \"elsevier\", \"wi…\n$ AffiliationCountry &lt;chr&gt; \"thailand\", \"greece\", \"brazil, south korea, usa\", \"…\n$ K1                 &lt;chr&gt; \"technology acceptance model\", \"word-of-mouth\", \"em…\n$ K2                 &lt;chr&gt; \"initial trust\", \"mobile-banking\", \"consumer adopti…\n$ K3                 &lt;chr&gt; \"consumer acceptance\", \"acceptance model\", \"initial…\n$ K4                 &lt;chr&gt; \"gender-differences\", \"information-technology\", \"in…\n$ K5                 &lt;chr&gt; \"normative beliefs\", \"consumer adoption\", \"anxiety\"…\n$ K6                 &lt;chr&gt; \"national culture\", \"moderating role\", \"acceptance\"…\n$ K7                 &lt;chr&gt; \"usage intentions\", \"utaut model\", \"commerce\", \"emp…\n$ K8                 &lt;chr&gt; \"privacy concerns\", \"services\", \"traits\", \"acceptan…\n$ K9                 &lt;chr&gt; \"internet users\", \"internet\", \"usage\", \"mediating r…\n$ K10                &lt;chr&gt; \"online trust\", \"determinants\", \"satisfaction\", \"ad…\n$ Num_Factors        &lt;int&gt; 5, 8, 5, 6, 4, 6, 4, 5, 4, 4, 8, 4, 5, 3, 6, 8, 6, …\n$ F1                 &lt;chr&gt; \"cltr\", \"peex\", \"self\", \"habt\", \"attd\", \"pu\", \"prgt…\n$ F2                 &lt;chr&gt; \"norm_blf\", \"socinf\", \"trst\", \"psec\", \"intrac\", \"pe…\n$ F3                 &lt;chr&gt; \"prv_exp\", \"fac_cond\", \"peou\", \"ppriv\", \"cltr\", \"so…\n$ F4                 &lt;chr&gt; \"competnc\", \"risk\", \"pu\", \"trst\", \"innov\", \"fac_con…\n$ F5                 &lt;chr&gt; \"trst\", \"anxiety\", \"intnt_use\", \"peex\", \"\", \"trst\",…\n$ F6                 &lt;chr&gt; \"\", \"rwrd\", \"\", \"price\", \"\", \"prisk\", \"\", \"\", \"\", \"…\n$ F7                 &lt;chr&gt; \"\", \"sec\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"habt\", …\n$ F8                 &lt;chr&gt; \"\", \"recom\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"price…\n$ F9                 &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ F1_THEME           &lt;chr&gt; \"social\", \"personal\", \"psychological\", \"personal\", …\n$ F2_THEME           &lt;chr&gt; \"social\", \"social\", \"psychological\", \"perceptive\", …\n$ F3_THEME           &lt;chr&gt; \"personal\", \"external\", \"perceptive\", \"perceptive\",…\n$ F4_THEME           &lt;chr&gt; \"external\", \"external\", \"perceptive\", \"psychologica…\n$ F5_THEME           &lt;chr&gt; \"psychological\", \"psychological\", \"psychological\", …\n$ F6_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"external\", \"\", \"perceptive\", \"…\n$ F7_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"pe…\n$ F8_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"ex…\n$ F9_THEME           &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ NUM_FAC_NOTSIG     &lt;int&gt; 2, 2, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, …\n$ FNS1               &lt;chr&gt; \"age\", \"efex\", \"\", \"peex\", \"\", \"\", \"\", \"\", \"\", \"soc…\n$ FNS2               &lt;chr&gt; \"sex\", \"fac_cond\", \"\", \"pval\", \"\", \"\", \"\", \"\", \"\", …\n$ FNS3               &lt;chr&gt; \"\", \"\", \"\", \"socinf\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ FNS4               &lt;chr&gt; \"\", \"\", \"\", \"hed_motiv\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n$ ResearchType       &lt;chr&gt; \"comparative\", \"empirical\", \"empirical\", \"empirical…\n$ SampleSize         &lt;int&gt; 560, 837, 458, 901, 1340, 755, 418, 203, 384, 127, …\n$ METHOD1            &lt;chr&gt; \"regression\", \"cfa\", \"cfa\", \"sem\", \"multigroup sem\"…\n$ METHOD2            &lt;chr&gt; \"\", \"sem\", \"sem\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ METHOD3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ METHOD4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ Tech               &lt;chr&gt; \"mobile banking\", \"mobile banking\", \"mobile banking…\n$ THEORY1            &lt;chr&gt; \"hcd\", \"utaut\", \"tam\", \"utaut2\", \"tam\", \"tam\", \"dct…\n$ THEORY2            &lt;chr&gt; \"\", \"\", \"\", \"\", \"tpb\", \"\", \"\", \"\", \"tpb\", \"\", \"utau…\n$ THEORY3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ THEORY4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ LIMIT1             &lt;chr&gt; \"sample\", \"sample\", \"sample\", \"sample\", \"no_long\", …\n$ LIMIT2             &lt;chr&gt; \"\", \"cross_cult\", \"bias\", \"no_mod\", \"bias\", \"sample…\n$ LIMIT3             &lt;chr&gt; \"\", \"incomp_fac\", \"question\", \"incomp_fac\", \"no_mod…\n$ Abstract           &lt;chr&gt; \"with the rapid growth of mobile phone usage, mobil…\n\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)\n\n\n\nNow let’s actually do some analysis. Let’s visualize how the themes of the papers have changed across the years. I will first generate a bar plot that fills the bars at each year (as a categorical factor) with proportions of themes in that year. This is an aggregation that happens under the hood, and using position = \"fill\" will actually make sure all the bars consider things relative to eachother, filling the full 100% of the bar.\n\nggplot(df, aes(x = as.factor(Year), fill = DecisionTheme)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Set3\")\n\nTo see how things move/flow over the years, a line chart is a great idea:\n\ndf %&gt;%\n    dplyr::count(DecisionTheme, Year) %&gt;%\n    ggplot(aes(x = as.factor(Year), y = n, color = DecisionTheme, group = DecisionTheme)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Dark2\")\n\nFor analysis, I will need to convert the data to long format. Since I want to avoid making it too big, I’ll do this separately for each key variable.\n\ntheory_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = THEORY1:THEORY4,\n        names_to = \"THEORY_NAME\", \n        values_to = \"THEORY\"\n    ) \n\nmethod_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"METHODNAME\", \n        values_to = \"METHOD\"\n    ) \n\nlimit_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"LIMITNAME\", \n        values_to = \"LIMIT\"\n    ) \n\nfac_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = F1:F9,\n        names_to = \"FACNAME\",\n        values_to = \"FAC\"\n    )\n\nfac_NS_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = FNS1:FNS4,\n        names_to = \"FAC_NS_NAME\",\n        values_to = \"FAC_NS\"\n    )\n\nfactors_based_on_themes &lt;- df %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"FAC_THEMES_NAMES\",\n    values_to = \"FACTHEME\"\n)\n\nRemove all the empty rows:\n\ntheory_long &lt;- theory_long %&gt;% filter(THEORY != \"\") #\nmethod_long &lt;- method_long %&gt;% filter(METHOD != \"\") #\nlimit_long &lt;- limit_long %&gt;% filter(LIMIT != \"\") #\nfac_long &lt;- fac_long %&gt;% filter(FAC != \"\")\nfac_NS_long &lt;- fac_NS_long %&gt;% filter(FAC_NS != \"\")\n\nAdd factor scores to the long factors and non-signficant factors’ data:\n\nfac_long &lt;- merge(fac_long, factor_scores, by = \"FAC\", all = T)\n\n\nfactor_scores &lt;- factor_scores %&gt;% mutate(FAC_NS = FAC) %&gt;% dplyr::select(FAC_NS,Score_Sig, Score_NOT_Sig)\nfac_NS_long &lt;- merge(fac_NS_long, factor_scores, by = \"FAC_NS\", all = T)\n\n\n\n\nNow, I want to explore the interactions between the following properties: themes, theories, methodologies, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors. Some questions that can be answered from such an analysis are:\n\nAre there notable differences in the distribution of themes, theories, methodologies, limitations, factors, research types, sample sizes, technologies, and non-significant factors.across years?\nAre themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and non-significant factors significantly associated with specific technologies?\nAre there significant differences in sample sizes across themes, theories, methodologies, limitations, factors, years, research types, technologies, and non-significant factors?\nDo research types vary significantly among different themes, theories, methodologies, limitations, factors, years, sample sizes, technologies, and non-significant factors?\nAre there significant differences in methods used across themes, theories, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors?\nAre the significant factors identified notably different among themes, theories, methodologies, limitations, years, research types, sample sizes, technologies, and non-significant factors?\nAre the non-significant factors identified notably different among themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and technologies?\n\nTo do this, I will first decide if further investigation is even worthwhile. First, I will use ANOVA to figure out if there are significant differences between groups of the same variable. That is, are themes, theories, methodologies, limitations, technologies, factors, years, research types, sample sizes, and non-significant factors actually different across the dataset?\n\nbuild_anova &lt;- function(nameOfCol){\n    counts_df &lt;- df %&gt;% count({{nameOfCol}}) %&gt;% arrange(desc(n))\n\n    counts_df_long &lt;- data.frame(\n        Group = rep(as.character(counts_df[[1]]), times = counts_df$n),\n        Value = unlist(lapply(counts_df$n, function(x) seq_len(x)))\n    )\n\n    anova_result &lt;- aov(Value ~ Group, data = counts_df_long)\n    return(summary(anova_result))\n}\n\nThemes are significantly different.\n\nbuild_anova(DecisionTheme)\n\nSo, let’s see how they differ across other factors - starting with the ones that do not require pivoting the dataframe! (Year, Tech, SampleSizeBin, ResearchType). This time, I will use a \\chi^2 test of independence.\n\nbuild_contingency_table &lt;- function(nameOfCol){\n    data_combine &lt;- df %&gt;% group_by(DecisionTheme) %&gt;% count({{nameOfCol}}) \n\n    contingency_table &lt;- xtabs(n ~ DecisionTheme + {{nameOfCol}}, data = data_combine)\n    chi_sq_result &lt;- chisq.test(contingency_table)\n    chi_sq_result\n}\n\n\n#build_contingency_table(SampleSizeBin)\n\nYou can also calculate the Cramer V:\n\ntable(is.na(df$SampleSizeBin))\n\n\n#cramerV(build_contingency_table(SampleSizeBin))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-1.-data-cleaning-and-prep",
    "href": "study1_DA.html#part-1.-data-cleaning-and-prep",
    "title": "Data Analysis",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\nlibrary(ggcorrplot)\nlibrary(ggnewscale)\nlibrary(ggeffects)\nlibrary(ggeffects)\nlibrary(pheatmap)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(treemapify)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nRows: 143\nColumns: 62\n$ Reason_Theme       &lt;chr&gt; \"comparative\", \"behavioral intention\", \"keywords\", …\n$ CiteKey            &lt;chr&gt; \"LonkaniAcomparativeStudyOfTrust2020\", \"SaprikisACo…\n$ ID                 &lt;chr&gt; \"p2_01\", \"p2_02\", \"p2_03\", \"p2_04\", \"p2_05\", \"p2_06…\n$ Algo_Theme         &lt;chr&gt; \"psychological, cultural, demographic\", \"psychologi…\n$ DecisionTheme      &lt;chr&gt; \"cultural\", \"psychological\", \"psychological\", \"cult…\n$ Man_Theme          &lt;chr&gt; \"cultural, psychological\", \"social, psychological\",…\n$ Match              &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Title              &lt;chr&gt; \"a comparative study of trust in mobile banking: an…\n$ Year               &lt;int&gt; 2020, 2022, 2021, 2019, 2020, 2020, 2024, 2020, 201…\n$ PublicationTitle   &lt;chr&gt; \"journal of global information management\", \"inform…\n$ Creators           &lt;chr&gt; \"ravi lonkani, chuleeporn changchit, tim klaus, jom…\n$ Publisher          &lt;chr&gt; \"igi global\", \"mdpi\", \"igi global\", \"elsevier\", \"wi…\n$ AffiliationCountry &lt;chr&gt; \"thailand\", \"greece\", \"brazil, south korea, usa\", \"…\n$ K1                 &lt;chr&gt; \"technology acceptance model\", \"word-of-mouth\", \"em…\n$ K2                 &lt;chr&gt; \"initial trust\", \"mobile-banking\", \"consumer adopti…\n$ K3                 &lt;chr&gt; \"consumer acceptance\", \"acceptance model\", \"initial…\n$ K4                 &lt;chr&gt; \"gender-differences\", \"information-technology\", \"in…\n$ K5                 &lt;chr&gt; \"normative beliefs\", \"consumer adoption\", \"anxiety\"…\n$ K6                 &lt;chr&gt; \"national culture\", \"moderating role\", \"acceptance\"…\n$ K7                 &lt;chr&gt; \"usage intentions\", \"utaut model\", \"commerce\", \"emp…\n$ K8                 &lt;chr&gt; \"privacy concerns\", \"services\", \"traits\", \"acceptan…\n$ K9                 &lt;chr&gt; \"internet users\", \"internet\", \"usage\", \"mediating r…\n$ K10                &lt;chr&gt; \"online trust\", \"determinants\", \"satisfaction\", \"ad…\n$ Num_Factors        &lt;int&gt; 5, 8, 5, 6, 4, 6, 4, 5, 4, 4, 8, 4, 5, 3, 6, 8, 6, …\n$ F1                 &lt;chr&gt; \"cltr\", \"peex\", \"self\", \"habt\", \"attd\", \"pu\", \"prgt…\n$ F2                 &lt;chr&gt; \"norm_blf\", \"socinf\", \"trst\", \"psec\", \"intrac\", \"pe…\n$ F3                 &lt;chr&gt; \"prv_exp\", \"fac_cond\", \"peou\", \"ppriv\", \"cltr\", \"so…\n$ F4                 &lt;chr&gt; \"competnc\", \"risk\", \"pu\", \"trst\", \"innov\", \"fac_con…\n$ F5                 &lt;chr&gt; \"trst\", \"anxiety\", \"intnt_use\", \"peex\", \"\", \"trst\",…\n$ F6                 &lt;chr&gt; \"\", \"rwrd\", \"\", \"price\", \"\", \"prisk\", \"\", \"\", \"\", \"…\n$ F7                 &lt;chr&gt; \"\", \"sec\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"habt\", …\n$ F8                 &lt;chr&gt; \"\", \"recom\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"price…\n$ F9                 &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ F1_THEME           &lt;chr&gt; \"social\", \"personal\", \"psychological\", \"personal\", …\n$ F2_THEME           &lt;chr&gt; \"social\", \"social\", \"psychological\", \"perceptive\", …\n$ F3_THEME           &lt;chr&gt; \"personal\", \"external\", \"perceptive\", \"perceptive\",…\n$ F4_THEME           &lt;chr&gt; \"external\", \"external\", \"perceptive\", \"psychologica…\n$ F5_THEME           &lt;chr&gt; \"psychological\", \"psychological\", \"psychological\", …\n$ F6_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"external\", \"\", \"perceptive\", \"…\n$ F7_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"pe…\n$ F8_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"ex…\n$ F9_THEME           &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ NUM_FAC_NOTSIG     &lt;int&gt; 2, 2, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, …\n$ FNS1               &lt;chr&gt; \"age\", \"efex\", \"\", \"peex\", \"\", \"\", \"\", \"\", \"\", \"soc…\n$ FNS2               &lt;chr&gt; \"sex\", \"fac_cond\", \"\", \"pval\", \"\", \"\", \"\", \"\", \"\", …\n$ FNS3               &lt;chr&gt; \"\", \"\", \"\", \"socinf\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ FNS4               &lt;chr&gt; \"\", \"\", \"\", \"hed_motiv\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n$ ResearchType       &lt;chr&gt; \"comparative\", \"empirical\", \"empirical\", \"empirical…\n$ SampleSize         &lt;int&gt; 560, 837, 458, 901, 1340, 755, 418, 203, 384, 127, …\n$ METHOD1            &lt;chr&gt; \"regression\", \"cfa\", \"cfa\", \"sem\", \"multigroup sem\"…\n$ METHOD2            &lt;chr&gt; \"\", \"sem\", \"sem\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ METHOD3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ METHOD4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ Tech               &lt;chr&gt; \"mobile banking\", \"mobile banking\", \"mobile banking…\n$ THEORY1            &lt;chr&gt; \"hcd\", \"utaut\", \"tam\", \"utaut2\", \"tam\", \"tam\", \"dct…\n$ THEORY2            &lt;chr&gt; \"\", \"\", \"\", \"\", \"tpb\", \"\", \"\", \"\", \"tpb\", \"\", \"utau…\n$ THEORY3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ THEORY4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ LIMIT1             &lt;chr&gt; \"sample\", \"sample\", \"sample\", \"sample\", \"no_long\", …\n$ LIMIT2             &lt;chr&gt; \"\", \"cross_cult\", \"bias\", \"no_mod\", \"bias\", \"sample…\n$ LIMIT3             &lt;chr&gt; \"\", \"incomp_fac\", \"question\", \"incomp_fac\", \"no_mod…\n$ Abstract           &lt;chr&gt; \"with the rapid growth of mobile phone usage, mobil…\n\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\n               vars   n    mean      sd median  min   max\nYear              1 143 2021.18    1.84   2021 2018  2024\nMatch             2 143    0.94    0.23      1    0     1\nNum_Factors       3 143    5.31    1.60      5    2     9\nNUM_FAC_NOTSIG    4 143    0.46    0.87      0    0     4\nSampleSize        5 103  894.50 3189.24    384   26 25000\n\n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\n     ID\n1 p2_77\n                                                                            Title\n1 financial literacy, behavioral traits, and epayment adoption and usage in japan\n  SampleSize\n1      25000\n\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\n               vars   n    mean      sd median  min   max\nYear              1 102 2021.14    1.86   2021 2018  2024\nMatch             2 102    0.94    0.24      1    0     1\nNum_Factors       3 102    5.15    1.65      5    2     9\nNUM_FAC_NOTSIG    4 102    0.54    0.95      0    0     4\nSampleSize        5 102  658.18 2112.42    384   26 21526\n\n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\n     ID\n1 p2_59\n                                                                                                                                                                     Title\n1 exploring mobile banking adoption and service quality features through user-generated content: the application of a topic modeling approach to google play store reviews\n  SampleSize\n1      21526\n\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\n               vars   n    mean     sd median  min  max\nID*               1 141   71.00  40.85     71    1  141\nYear              2 141 2021.16   1.85   2021 2018 2024\nMatch             3 141    0.94   0.23      1    0    1\nNum_Factors       4 141    5.33   1.60      5    2    9\nNUM_FAC_NOTSIG    5 141    0.47   0.87      0    0    4\nSampleSize        6 101  451.56 330.40    384   26 2202\n\n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nNumber of Unique Values in ID:  143\nNumber of Unique Values in Title:  143\nNumber of Unique Values in PublicationTitles:  54\nNumber of Unique Values in Publisher:  15\nNumber of Unique Values in AffiliationCountry:  43\nNumber of Unique Values in Factors:  233\nNumber of Unique Values in Not Sig:  32\nNumber of Unique Values in Methods:  42\nNumber of Unique Values in Theory:  44\nNumber of Unique Values in Limits:  16\nNumber of Unique Values in ResearchType:  16\nNumber of Unique Values in Authors:  137\nNumber of Unique Values in Keywords:  413\nNumber of Unique Values in Tech:  9\nNumber of Unique Values in Themes:  7\n\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n             n   mean    sd median min  max\nSampleSize 101 451.56 330.4    384  26 2202\n\n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\n  SampleSizeBin  n\n1     NotStated 42\n2           SQ1 26\n3           SQ2 28\n4           SQ3 23\n5           SQ4 24\n\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)\n\n        FAC F_count FNS_counts Total_count Score_Sig Score_NOT_Sig\n1       acc       1          0           1       1.0           0.0\n2  acc_conv       2          0           2       1.0           0.0\n3 accbility       3          0           3       1.0           0.0\n4       age       3          3           6       0.5           0.5\n5    agrbns       1          0           1       1.0           0.0\n6      alts       1          0           1       1.0           0.0",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-2.-analysis",
    "href": "study1_DA.html#part-2.-analysis",
    "title": "Data Analysis",
    "section": "Part 2. Analysis",
    "text": "Part 2. Analysis\nNow let’s actually do some analysis. Let’s visualize how the themes of the papers have changed across the years. I will first generate a bar plot that fills the bars at each year (as a categorical factor) with proportions of themes in that year. This is an aggregation that happens under the hood, and using position = \"fill\" will actually make sure all the bars consider things relative to eachother, filling the full 100% of the bar.\n\nggplot(df, aes(x = as.factor(Year), fill = DecisionTheme)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Set3\")\n\n\n\n\n\n\n\n\nTo see how things move/flow over the years, a line chart is a great idea:\n\ndf %&gt;%\n    dplyr::count(DecisionTheme, Year) %&gt;%\n    ggplot(aes(x = as.factor(Year), y = n, color = DecisionTheme, group = DecisionTheme)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Dark2\")\n\n\n\n\n\n\n\n\nFor analysis, I will need to convert the data to long format. Since I want to avoid making it too big, I’ll do this separately for each key variable.\n\ntheory_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = THEORY1:THEORY4,\n        names_to = \"THEORY_NAME\", \n        values_to = \"THEORY\"\n    ) \n\nmethod_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"METHODNAME\", \n        values_to = \"METHOD\"\n    ) \n\nlimit_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"LIMITNAME\", \n        values_to = \"LIMIT\"\n    ) \n\nfac_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = F1:F9,\n        names_to = \"FACNAME\",\n        values_to = \"FAC\"\n    )\n\nfac_NS_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = FNS1:FNS4,\n        names_to = \"FAC_NS_NAME\",\n        values_to = \"FAC_NS\"\n    )\n\nfactors_based_on_themes &lt;- df %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"FAC_THEMES_NAMES\",\n    values_to = \"FACTHEME\"\n)\n\nRemove all the empty rows:\n\ntheory_long &lt;- theory_long %&gt;% filter(THEORY != \"\") #\nmethod_long &lt;- method_long %&gt;% filter(METHOD != \"\") #\nlimit_long &lt;- limit_long %&gt;% filter(LIMIT != \"\") #\nfac_long &lt;- fac_long %&gt;% filter(FAC != \"\")\nfac_NS_long &lt;- fac_NS_long %&gt;% filter(FAC_NS != \"\")\n\nAdd factor scores to the long factors and non-signficant factors’ data:\n\nfac_long &lt;- merge(fac_long, factor_scores, by = \"FAC\", all = T)\n\n\nfactor_scores &lt;- factor_scores %&gt;% mutate(FAC_NS = FAC) %&gt;% dplyr::select(FAC_NS,Score_Sig, Score_NOT_Sig)\nfac_NS_long &lt;- merge(fac_NS_long, factor_scores, by = \"FAC_NS\", all = T)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-3-statistical-analysis",
    "href": "study1_DA.html#part-3-statistical-analysis",
    "title": "Data Analysis",
    "section": "Part 3 Statistical Analysis",
    "text": "Part 3 Statistical Analysis\nNow, I want to explore the interactions between the following properties: themes, theories, methodologies, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors. Some questions that can be answered from such an analysis are:\n\nAre there notable differences in the distribution of themes, theories, methodologies, limitations, factors, research types, sample sizes, technologies, and non-significant factors.across years?\nAre themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and non-significant factors significantly associated with specific technologies?\nAre there significant differences in sample sizes across themes, theories, methodologies, limitations, factors, years, research types, technologies, and non-significant factors?\nDo research types vary significantly among different themes, theories, methodologies, limitations, factors, years, sample sizes, technologies, and non-significant factors?\nAre there significant differences in methods used across themes, theories, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors?\nAre the significant factors identified notably different among themes, theories, methodologies, limitations, years, research types, sample sizes, technologies, and non-significant factors?\nAre the non-significant factors identified notably different among themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and technologies?\n\nTo do this, I will first decide if further investigation is even worthwhile. First, I will use ANOVA to figure out if there are significant differences between groups of the same variable. That is, are themes, theories, methodologies, limitations, technologies, factors, years, research types, sample sizes, and non-significant factors actually different across the dataset?\n\nbuild_anova &lt;- function(nameOfCol){\n    counts_df &lt;- df %&gt;% count({{nameOfCol}}) %&gt;% arrange(desc(n))\n\n    counts_df_long &lt;- data.frame(\n        Group = rep(as.character(counts_df[[1]]), times = counts_df$n),\n        Value = unlist(lapply(counts_df$n, function(x) seq_len(x)))\n    )\n\n    anova_result &lt;- aov(Value ~ Group, data = counts_df_long)\n    return(summary(anova_result))\n}\n\n\nPaper Themes\nThemes are significantly different.\n\nbuild_anova(DecisionTheme)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup         6   7489  1248.2   15.05 3.69e-13 ***\nResiduals   136  11280    82.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, let’s see how they differ across other factors - starting with the ones that do not require pivoting the dataframe! (Year, Tech, SampleSizeBin, ResearchType). This time, I will use a \\chi^2 test of independence.\n\ncombine_data &lt;- function(nameOfCol){\n    data_combine &lt;- df %&gt;% \n    group_by(DecisionTheme) %&gt;% \n    count({{nameOfCol}}) \n\n    return(data_combine)\n}\n\n\nchisq.test(\n    xtabs(n ~ DecisionTheme + Year, \n         data = combine_data(Year))\n)\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ DecisionTheme + Year, data = combine_data(Year))\nX-squared = 44.414, df = 36, p-value = 0.1585\n\n\nI do this for SampleSizeBin, ResearchType, and Tech too. The results are:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nYear\n44.414\n36\n0.159\n\n\nTech\n102.59\n48\n0.002^{**}\n\n\nSampleSizeBin\n8.6582\n18\n0.967\n\n\nResearchType\n127.06\n90\n0.006^{**}\n\n\n\nFor the variables that have multiple columns, I first pivot the dataframe and then do the test. For example, this is how I do it for non-significant factors:\n\ncombine_data_longFormat &lt;- function(startCol, stopCol){\n    longlong &lt;- df %&gt;% pivot_longer(\n        cols = {{startCol}}:{{stopCol}},\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n    filter(THINGS != \"\" & THINGS != \" \") # no null values \n\n    combined_df &lt;- longlong %&gt;% \n        group_by(DecisionTheme) %&gt;%\n        count(THINGS) \n\n    return(combined_df)\n}\n\ncombined_df &lt;- combine_data_longFormat(FNS1, FNS4)\nchisq.test(xtabs(n ~ DecisionTheme + THINGS,\n           data = combined_df))\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ DecisionTheme + THINGS, data = combined_df)\nX-squared = 207.87, df = 180, p-value = 0.0758\n\n\nDoing this for Theory, Methods, Limitatiosn, Factors, as well as factor Themes I have:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nTheories\n251.84\n252\n0.491\n\n\nMethods\n272.02\n240\n0.076\n\n\nLimits\n89.327\n84\n0.325\n\n\nFactors\n1461.4\n1386\n0.078\n\n\nFactorThemes\n139.26\n36\n&lt; 0.001^{***}\n\n\nNS.Factors\n207.87\n180\n0.076\n\n\n\nWhich basically means only technology, research type and factor themes are kind of related to overall paper themes. Let’s dig deeper.\n\nPaper Themes vs Technology\nI first make sure the rows where Tech is empty are filled with mobile banking (as the most frequent technology, because the papers were specifically m-banking related).\n\ndf %&gt;% group_by(DecisionTheme) %&gt;% \n    mutate(Tech = ifelse(\n        Tech == \"\",\n        \"mobile banking\",\n        Tech\n    )) %&gt;% count(Tech) %&gt;% ggplot(aes(x = as.factor(DecisionTheme), fill = Tech)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Tech\",\n       x = \"Themes\",\n       y = \" \") +\n  fill_palette(\"Set3\") + \n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"), \n  ) \n\n\n\n\n\n\n\n\nMarket studies cover the broadest range of technologies. This is because studies on variations of m-banking like m-wallets and smart wearables fall under the Market category, influencing the theme distribution across technologies. To confirm that this effect is driven by Market studies, I exclude them and rerun the above test:\n\ntemp_combine_data &lt;- df %&gt;% \n    filter(DecisionTheme != 'market') %&gt;% \n    group_by(DecisionTheme) %&gt;% \n    count(Tech) \n\nchisq.test(xtabs(\n    n ~ DecisionTheme + Tech,\n    data = temp_combine_data\n))\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ DecisionTheme + Tech, data = temp_combine_data)\nX-squared = 19.629, df = 20, p-value = 0.4813\n\n\nThe p-value of 0.481 means we cannot reject the null hypothesis of \\chi^2 test of independence. That means, Paper Themes and Technology studied are independent when we set aside Market studies. How about research type? First, a little house keeping for the data with respect to research type: I simply rename a few columns for easier visualization.\n\n\nPaper Themes vs Research Type\n\ndf %&gt;% group_by(DecisionTheme) %&gt;% \n    mutate(\n        ResearchType = case_when(\n               ResearchType == \"survey quantitative\" ~ \"survey analysis\",\n               ResearchType == \"survey, empirical\" ~ \"survey analysis\",\n               ResearchType == \"survey\" ~ \"survey analysis\",\n               ResearchType == \"conceptual qualitative\" ~ \"conceptual\",\n               ResearchType == \"mixed method qualquant\" ~ \"mixed method\",\n               .default = ResearchType\n    )) %&gt;% \n    filter(!is.na(ResearchType) & ResearchType != \"\") %&gt;% \n    count(ResearchType) %&gt;% \n    ggplot(aes(x = as.factor(DecisionTheme), fill = ResearchType)) +\n          geom_bar(position = \"fill\") +\n          theme_minimal() +\n          labs(fill = \"Research Type\",\n               x = \"Themes\",\n               y = \" \") +\n          fill_palette(\"Set3\") + \n          theme(\n            axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"), \n          )\n\n\n\n\n\n\n\n\nPatterns between research types and themes are less distinct. Market and Personal studies show the least research type diversity, while Psychological studies exhibit the most. Comparative research appears only in Cultural and Demographic studies. For themes of factors, I will use a different type of visualization (not filling the bars, but wanting to see the difference separately):\n\n\nPaper Themes vs Sig/Non-Significant Factors\n\nlonglong &lt;- df %&gt;% pivot_longer(\n        cols = F1_THEME:F9_THEME,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        # don't include external variables \n    filter(THINGS != \"\" & THINGS != \" \" & THINGS != \"external\") %&gt;% # these changes are for visualization \n    mutate(THINGS = str_to_title(THINGS), DecisionTheme = str_to_title(DecisionTheme))\n\nlonglongGroup &lt;- longlong %&gt;% group_by(DecisionTheme) %&gt;% count(THINGS) %&gt;% arrange(desc(n))\n\nggplot(longlongGroup, aes(x = as.factor(DecisionTheme), y = n, fill = as.factor(THINGS))) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(fill = \"Theme of Factors\", x = \"Theme\", y = \"Proportions\") +\n  fill_palette(\"Accent\") + \n  theme(axis.text.y = element_text(angle = 0, hjust = 1, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\")) \n\n\n\n\n\n\n\n\nMarket studies contain the most diverse set of factors influencing m-banking, likely due to device differences. This suggests that variations of the same technology (e.g., m-wallets, m-payment devices) introduce different influencing factors. A comparative study between devices could provide valuable insights. Market-specific factors appear only in Social and Perceptive themes (aside from Market itself). This suggests that Social and Perceptive studies are the only studies that are considering device differences. Therefore, future research should expand the understanding of device differences in m-banking adoption by conducting thematically different studies. Cultural and Personal themes have the least diverse factor sets. Within Cultural studies, personal and perceptive factors dominate, indicating that cultural influences shape individual perceptions (opinions) and values (personal factors). In personal studies, social factors are overshadowed by others, suggesting that individual preferences take precedence over group opinions. Most studies on individual differences do not consider societal influences, signaling an unexplored area for future research. Although no significant relationship exists between factors and themes, it is still useful to look at some patterns.\nOk, I’m not interested in seeing if there are differences in how many times factors appear significant and non-significant across themes. First, let’s merge the long data on factor and non-significant factors with factor_scores datasets. The factor scores were the counts of how many times each factor appeared across the literature as significant/non-significant.\n\nlonglong_fac &lt;- df %&gt;% pivot_longer(\n    cols = F1:F9,\n    names_to = \"NAMES\",\n    values_to = \"FAC_NS\"\n) %&gt;% filter(FAC_NS != \"\" & FAC_NS != \" \" & FAC_NS != \"external\")\n\nlonglong_nonsigfac &lt;- df %&gt;% pivot_longer(\n    cols = FNS1:FNS4,\n    names_to = \"NAMES\",\n    values_to = \"FAC_NS\"\n) %&gt;% filter(FAC_NS != \"\" & FAC_NS != \" \" & FAC_NS != \"external\") \n\ndf_merged_fac &lt;- longlong_fac %&gt;% left_join(factor_scores, by = \"FAC_NS\")\ndf_merged_nonsigfac &lt;- longlong_nonsigfac %&gt;% left_join(factor_scores, by = \"FAC_NS\")\n\nThe significance score across themes:\n\nggplot(df_merged_fac, \n    aes(x = DecisionTheme, y = Score_Sig, \n        fill = DecisionTheme)) +\n  geom_boxplot() +\n  labs(x = \"Theme\",\n       y = \"Factor's Significance Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1,\n   face = \"bold\"))\n\n\n\n\n\n\n\n\nMost themes have a high median significant score. The median for all themes is close to 1.0, meaning most factors are frequently significant, at least relative to their total occurrences. Variation exists, but it’s minor. Some themes (e.g., market, cultural and demographic) have slightly lower median than others. The IQR is small, indicating that most factor scores are clustered near the top. Some data points fall below 0.5, suggesting a few factors appear non-significant more so than they are significant. These outliers might indicate less relevant factors for certain themes. Since most medians are high and IQR’s are similar, no theme strongly dominates in terms of factor scores. Market studies might have slightly lower scores than others, indicating device and technology specific factors may be needed. I can also find out which factors have a low significance score for each theme:\n\nhead(df_merged_fac %&gt;% filter(Score_Sig &lt;= 0.5) %&gt;% dplyr::select(FAC_NS, DecisionTheme))\n\n# A tibble: 6 × 2\n  FAC_NS  DecisionTheme\n  &lt;chr&gt;   &lt;chr&gt;        \n1 age     psychological\n2 soc_val demographic  \n3 edu     market       \n4 trial   social       \n5 age     demographic  \n6 edu     demographic  \n\n\nThe outliers (Score_Sig \\leq 0.5) and the related theme they appeared as an outlier in are: Age (Psychological), Education (Market), Triability (Social), Hedonic and Social Value (Perceptive), and Age, Education and Social Value (Demographic). The reason for these outliers are:\n\nthese factors either do not appear frequently in those themes, or\nthey do appear, but they are not frequently found to be significant.\n\nFor example, I expected age and education to appear in many Demographic studies. Therefore, it is surprising that these factors have low scores. This indicates that age and education are either not frequently used in Demographic studies or they are frequently found to be non-significant variables in m-banking adoption.\nThe non-significance score across themes:\n\nggplot(df_merged_nonsigfac,\n    aes(x = DecisionTheme, y = Score_NOT_Sig, \n        fill = DecisionTheme)) +\n  geom_boxplot() +\n  labs(x = \"Theme\",\n       y = \"Factor's Non Significance Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, \n  face = \"bold\"))\n\n\n\n\n\n\n\n\nPerceptive has the widest range of non-significant scores, with a high median (around 0.75). Demographic also has a relatively high median, meaning many factors in demographic-related studies were found to be non-significant. Some themes (e.g., Perceptive and Demographic) show high variability in non-significant factor scores, while some report very low variability. The presence of outliers suggests that certain studies had unusually high numbers of non-significant factors. The outliers are factors with non-significant scores of higher than 0.5 (appearing more than half of their total frequency as non-significant).\n\nhead(df_merged_nonsigfac %&gt;% filter(Score_NOT_Sig &gt; 0.5) %&gt;% dplyr::select(FAC_NS, DecisionTheme))\n\n# A tibble: 6 × 2\n  FAC_NS   DecisionTheme\n  &lt;chr&gt;    &lt;chr&gt;        \n1 soc_val  demographic  \n2 cond_val demographic  \n3 trial    demographic  \n4 soc_val  demographic  \n5 pu_atm   perceptive   \n6 soc_val  psychological\n\n\nThese are: Social Value, Conditional Value, Triability (Demographic), Perceived Usefulness of ATM devices, Search Convenience and Evaluation Convenience (Perceptive), Social Value and Hedonic Experience (Psychological), Triability (Personal) and Convenience barriers (Market).\nAn interesting observation is social values being an outlier in demographic studies for both significance and non-significane score.\n\nglimpse(df %&gt;% filter(F1 == \"soc_val\" | F2 == \"soc_val\" | F3 == \"soc_val\" | F4 == \"soc_val\" | F5 == \"soc_val\" | F6 == \"soc_val\" | F7 == \"soc_val\" | F8 == \"soc_val\" | F9 == \"soc_val\"))\n\nRows: 2\nColumns: 63\n$ Reason_Theme       &lt;chr&gt; \"In agreement\", \"\"\n$ CiteKey            &lt;chr&gt; \"\", \"\"\n$ ID                 &lt;chr&gt; \"p2_37\", \"p2_143\"\n$ Algo_Theme         &lt;chr&gt; \"psychological, market, perceptive\", \"perceptive, p…\n$ DecisionTheme      &lt;chr&gt; \"demographic\", \"perceptive\"\n$ Man_Theme          &lt;chr&gt; \"personal, demographic\", \"perceptive\"\n$ Match              &lt;int&gt; 0, 1\n$ Title              &lt;chr&gt; \"consumption values and mobile banking services: un…\n$ Year               &lt;int&gt; 2021, 2024\n$ PublicationTitle   &lt;chr&gt; \"international journal of bank marketing\", \"spanish…\n$ Creators           &lt;chr&gt; \"heikki karjaluoto, richard glavee-geo, dineshwar r…\n$ Publisher          &lt;chr&gt; \"emerald\", \"emerald\"\n$ AffiliationCountry &lt;chr&gt; \"mauritius\", \"spain, macedonia\"\n$ K1                 &lt;chr&gt; \"technology acceptance model\", \"perceived value\"\n$ K2                 &lt;chr&gt; \"perceived value\", \"mobile\"\n$ K3                 &lt;chr&gt; \"discriminant validity\", \"services\"\n$ K4                 &lt;chr&gt; \"behavioral intention\", \"adoption\"\n$ K5                 &lt;chr&gt; \"initial trust\", \"acceptance\"\n$ K6                 &lt;chr&gt; \"social value\", \"commerce\"\n$ K7                 &lt;chr&gt; \"method bias\", \"drives\"\n$ K8                 &lt;chr&gt; \"adoption\", NA\n$ K9                 &lt;chr&gt; \"recommendations\", NA\n$ K10                &lt;chr&gt; \"identification\", NA\n$ Num_Factors        &lt;int&gt; 4, 5\n$ F1                 &lt;chr&gt; \"fun_val\", \"util_val\"\n$ F2                 &lt;chr&gt; \"epi_val\", \"hed_val\"\n$ F3                 &lt;chr&gt; \"emo_val\", \"soc_val\"\n$ F4                 &lt;chr&gt; \"soc_val\", \"epi_val\"\n$ F5                 &lt;chr&gt; \"\", \"pval\"\n$ F6                 &lt;chr&gt; \"\", \"\"\n$ F7                 &lt;chr&gt; \"\", \"\"\n$ F8                 &lt;chr&gt; \"\", \"\"\n$ F9                 &lt;chr&gt; \"\", \"\"\n$ F1_THEME           &lt;chr&gt; \"personal\", \"personal\"\n$ F2_THEME           &lt;chr&gt; \"personal\", \"personal\"\n$ F3_THEME           &lt;chr&gt; \"psychological\", \"social\"\n$ F4_THEME           &lt;chr&gt; \"social\", \"personal\"\n$ F5_THEME           &lt;chr&gt; \"\", \"perceptive\"\n$ F6_THEME           &lt;chr&gt; \"\", \"\"\n$ F7_THEME           &lt;chr&gt; \"\", \"\"\n$ F8_THEME           &lt;chr&gt; \"\", \"\"\n$ F9_THEME           &lt;chr&gt; \"\", \"\"\n$ NUM_FAC_NOTSIG     &lt;int&gt; 1, 1\n$ FNS1               &lt;chr&gt; \"soc_val\", \"hed_val\"\n$ FNS2               &lt;chr&gt; \"\", \"\"\n$ FNS3               &lt;chr&gt; \"\", \"\"\n$ FNS4               &lt;chr&gt; \"\", \"\"\n$ ResearchType       &lt;chr&gt; \"quantitative\", \"quantitative\"\n$ SampleSize         &lt;int&gt; 246, 252\n$ METHOD1            &lt;chr&gt; \"pls sem\", \"pls sem\"\n$ METHOD2            &lt;chr&gt; \"\", \"\"\n$ METHOD3            &lt;chr&gt; \"\", \"\"\n$ METHOD4            &lt;chr&gt; \"\", \"\"\n$ Tech               &lt;chr&gt; \"mobile banking\", \"mobile banking\"\n$ THEORY1            &lt;chr&gt; \"tcv\", \"sor\"\n$ THEORY2            &lt;chr&gt; \"\", \"tcv\"\n$ THEORY3            &lt;chr&gt; \"\", \"\"\n$ THEORY4            &lt;chr&gt; \"\", \"\"\n$ LIMIT1             &lt;chr&gt; \"sample_size\", \"loc_specific\"\n$ LIMIT2             &lt;chr&gt; \"loc_specific\", \"\"\n$ LIMIT3             &lt;chr&gt; \"\", \"\"\n$ Abstract           &lt;chr&gt; \"purpose this study develops a theoretical model of…\n$ SampleSizeBin      &lt;chr&gt; \"SQ1\", \"SQ1\"\n\n\nThis is because of the study of Karjaluoto @karjaluotoConsumptionValuesMobile2021b where they found that social values did not influence trust in m-banking directly. However, social value is a significant moderator between functional value and intention to m-bank. Other insights from this analysis are: Perceptive studies may either not be reliable or may be useful to frameworks to assess the significance of factors in. Since another reason for low scores of factors are due to them not appearing as frequently across the literature, we also highlight several of these overlooked factors across themes. Identifying these gaps can provide valuable insights for future research and broaden the understanding of m-banking adoption.\nI can test whether certain themes are more likely to have non-significant factors:\n\nkruskal.test(Score_NOT_Sig ~ DecisionTheme, data = df_merged_nonsigfac)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Score_NOT_Sig by DecisionTheme\nKruskal-Wallis chi-squared = 15.829, df = 6, p-value = 0.0147\n\n\nIf p &lt; 0.05, it confirms that certain themes are more likely to have non-significant factors.\nI can also identify which specific factors within perceptive or demographic themes are most non-significant. Check interaction effects (e.g., Does the study’s research method influence significance?).\n\nlonglongfactors &lt;- df %&gt;% pivot_longer(\n    cols = F1:F9,\n    names_to = \"NAMES\",\n    values_to = \"FAC\"\n) %&gt;% filter(FAC != \"\" & FAC != \" \")\n\nprint(\"Unique Decision Themes: \\n\") \n\n[1] \"Unique Decision Themes: \\n\"\n\nunique(df$DecisionTheme)\n\n[1] \"cultural\"      \"psychological\" \"demographic\"   \"perceptive\"   \n[5] \"social\"        \"market\"        \"personal\"     \n\n\n\nhead(longlongfactors %&gt;% \n    group_by(DecisionTheme) %&gt;%\n    count(FAC) %&gt;% arrange(desc(n)) %&gt;% \n    filter(DecisionTheme == \"personal\"))\n\n# A tibble: 6 × 3\n# Groups:   DecisionTheme [1]\n  DecisionTheme FAC           n\n  &lt;chr&gt;         &lt;chr&gt;     &lt;int&gt;\n1 personal      peou          4\n2 personal      pu            4\n3 personal      efex          3\n4 personal      habt          3\n5 personal      hed_motiv     3\n6 personal      peex          3\n\n\nIn the paper, I actually wanted to find the factors that appeared the least number of times. So, just take out the desc() arrangement and then look for the papers that have used certain factors (only once). For example, agreeableness was used only once. Let’s see which personal-themed paper used it:\n\ndf %&gt;% pivot_longer(\n    cols = F1:F9,\n    names_to = \"NAMES\",\n    values_to = \"THINGS\") %&gt;%\n    filter(DecisionTheme == \"personal\" &\n           THINGS == \"agrbns\") %&gt;%\n    dplyr::select(ID)\n\n# A tibble: 1 × 1\n  ID    \n  &lt;chr&gt; \n1 p2_106\n\n\nThis was then referenced in the paper I wrote.\nThe same entire process was repeated for Theory, Methods, Limitations, Factors and Non-significant Factors.\n\n\n\nFoundational Theories\n\ncounts_dflong &lt;- theory_long %&gt;% \n        filter(THEORY != \"\" & THEORY != \" \"\n            & !is.na(THEORY)) %&gt;% \n        count(THEORY) %&gt;% arrange(desc(n))\n    \ncounts_df_long &lt;- data.frame(\n    Group = rep(counts_dflong$THEORY, times = counts_dflong$n),\n    Value = unlist(lapply(counts_dflong$n, function(x) seq_len(x))))\n    \nanova_result &lt;- aov(Value ~ Group, data = counts_df_long)\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup        42  18420   438.6   3.632 1.82e-08 ***\nResiduals   120  14492   120.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGreat, There’s statistically significant differences in the distribution of Theories in the data. I follow the same idea above, except this time, use the theory_long in building the combined data. For example, for Year:\n\nbuild_combine_long &lt;- function(df, colName1, colName2){\n    data_combine &lt;- df %&gt;% group_by({{colName1}}) %&gt;% count({{colName2}}) \n\n    return(data_combine)\n}\n\n\nchisq.test(xtabs(\n    n ~ THEORY + Year, \n    data = build_combine_long(theory_long, THEORY, Year)))\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ THEORY + Year, data = build_combine_long(theory_long,     THEORY, Year))\nX-squared = 262.92, df = 252, p-value = 0.3054\n\n\nI do this for SampleSizeBin, ResearchType, and Tech too. The results are:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nYear\n262.92\n252\n0.305\n\n\nTech\n309.21\n294\n0.260\n\n\nSampleSizeBin\n112.35\n108\n0.368\n\n\nResearchType\n421.38\n$ 630$\n0.999\n\n\n\nFor the multiple column variables, I get:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nMethods\n315.09\n168\n&lt;.001^{***}\n\n\nLimits\n133.81\n78\n&lt;.001^{***}\n\n\nFactors\n1745.5\n1134\n&lt;.001^{***}\n\n\nNS.Factors\n247.63\n162\n&lt;.001^{***}\n\n\n\nLet’s dig deeper.\n\nTheory vs Method\nThere are a large number of unique methods in the data, 41 to be exact:\n\nn_distinct(method_long$METHOD)\n\n[1] 41\n\n\nLet’s visualize this.\n\nlonglong_theorymethod &lt;- theory_long %&gt;%\n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        filter(THINGS != \"\" & THINGS != \" \" & \n               THEORY != \" \" & THEORY != \"\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS),\n           THEORY = str_to_upper(THEORY))\n\nlonglong_theorymethodGroup &lt;- longlong_theorymethod %&gt;% \n    group_by(THEORY) %&gt;% count(THINGS) %&gt;%\n    arrange(desc(n))\n\nggplot(longlong_theorymethodGroup, \n    aes(area = n, fill = as.factor(THEORY), \n       label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0) +\n  #scale_fill_viridis_d(option = \"turbo\") + \n  scale_color_identity() +     \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7),  \n    legend.spacing.y = unit(0.1, \"cm\"), # row spaces \n    legend.key.height = unit(0.2, \"cm\"), \n  ) + guides(color = guide_legend(ncol = 7)) \n\n\n\n\n\n\n\n\nThe guides() pipe helps with the guild legend on the bottom of the figure (same color as the cells). This doesn’t help much, so I’ll set aside the top methods and the top theories:\n\nlonglongGroup2 &lt;- longlong_theorymethod %&gt;% \n    group_by(THINGS) %&gt;% filter(\n        THINGS != \"SEM\" & THINGS != \"PLS SEM\" & THINGS != \"PLS\"& \n        THEORY != \"TAM\" & THEORY != \"UTAUT\"\n    ) %&gt;% count(THEORY) %&gt;% arrange(desc(n)) \n\nggplot(longlongGroup2, \n       aes(area = n, fill = as.factor(THINGS),\n           label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0\n  ) +\n  #scale_fill_viridis_d(option = \"rocket\") + \n  scale_color_identity() +  \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7), \n    legend.spacing.y = unit(0.1, \"cm\"),   \n    legend.key.height = unit(0.2, \"cm\"),  \n  ) + guides(color = guide_legend(ncol = 7)) \n\n\n\n\n\n\n\n\nMuch more useful information from this. For example, we see that newer model theories like Elaborate Likelihood Method (ELM) incorporate moderation and mediation analysis. Whereas older models like TAM, DOI, TPB were the ones using confirmatory factor analysis and SEM.\n\n\nTheory vs Limits\nFor limits, I’m setting aside the least referenced (ttest) and the limit in scope that is captured in other limits. Additionally, let’s set aside the two top theories TAM and UTAUT.\n\nlonglonglimittheory &lt;- theory_long %&gt;% pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        filter(THINGS != \"\" & THINGS != \" \"&\n               THEORY != \" \" & THEORY != \"\" & \n               THEORY != \"tam\" & THEORY != \"utaut\" &\n               THINGS != \"ttest\" & THINGS != \"limit_scope\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS), \n           THEORY = str_to_upper(THEORY))\n\nlonglonglimittheoryGroup &lt;- longlonglimittheory %&gt;% \n    group_by(THEORY) %&gt;%\n    count(THINGS) %&gt;% arrange(desc(n))\n\n\nggplot(longlonglimittheoryGroup, \n    aes(x = THEORY, fill = as.factor(THINGS))) + \n    geom_bar(position = \"fill\", color = \"black\") +\n    theme_minimal() +\n    labs(fill = \"Limits\", x = \" \", y = \"Proportions\") +\n    fill_palette(\"Paired\") + \n    theme(\n        axis.text.y = element_text(angle = 0, hjust = 1, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\n\nLet’s see how many times which theories are used in papers which have limitations because of sample bias:\n\ntheory_long %&gt;% pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        filter(THINGS != \"\" & THINGS != \" \"&\n               THEORY != \" \" & THEORY != \"\") %&gt;%\n    mutate(THINGS = str_to_upper(THINGS), \n           THEORY = str_to_upper(THEORY)) %&gt;% \n        group_by(THINGS) %&gt;% \n        count(THEORY) %&gt;% arrange(desc(n)) %&gt;%\n        filter(THINGS == \"BIAS\") \n\n# A tibble: 5 × 3\n# Groups:   THINGS [1]\n  THINGS THEORY         n\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;\n1 BIAS   TAM            3\n2 BIAS   IRT            1\n3 BIAS   META_UTAUT     1\n4 BIAS   TPB            1\n5 BIAS   TT             1\n\n\n\n\nTheory vs Factors\nThere are way too many factors for even visualizations to have any meaningful results. So, it’s better to look at the factor themes and maybe get more specific only look up really interesting patterns.\n\nlonglongfactortheory &lt;- theory_long %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"NAMES\",\n    values_to = \"THINGS\"\n) %&gt;% filter(THINGS != \"\" & THINGS != \" \" &\n             THEORY != \" \" & THEORY != \"\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS),\n           THEORY = str_to_upper(THEORY))\n\nlonglongGroupfactortheory &lt;- longlongfactortheory %&gt;% \n    filter(THEORY != \"TAM\" & THEORY != \"UTAUT\" &\n           THINGS != \"EXTERNAL\") %&gt;% \n    group_by(THEORY) %&gt;% count(THINGS) %&gt;% arrange(desc(n))\n\n\nggplot(longlongGroupfactortheory, \n    aes(area = n, fill = as.factor(THINGS),\n       label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0\n  ) +\n  #scale_fill_viridis_d(option = \"plasma\") + \n  scale_color_identity() +  \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7),  \n    legend.spacing.y = unit(0.1, \"cm\"), \n    legend.key.height = unit(0.2, \"cm\")\n  ) \n\n\n\n\n\n\n\n\n\nggplot(longlongGroupfactortheory, \n    aes(x = as.factor(THEORY), fill = THINGS)) +\n    geom_bar(position = \"fill\") +\n    theme_minimal() +\n    labs(fill = \"Theme of Factors\", x = \"Theory\", y = \"Proportions\") +\n    fill_palette(\"Set1\") +\n    theme(\n        axis.text.y = element_text(angle = 0, hjust = 1, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"))\n\n\n\n\n\n\n\n\nLet’s see for example what type of theories are routinely used with market factors:\n\nlonglongfactortheory %&gt;% group_by(THINGS) %&gt;% \n    count(THEORY) %&gt;% arrange(desc(n)) %&gt;%\n    filter(THINGS == \"MARKET\")\n\n# A tibble: 3 × 3\n# Groups:   THINGS [1]\n  THINGS THEORY     n\n  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt;\n1 MARKET TAM        1\n2 MARKET TPB        1\n3 MARKET UTAUT2     1\n\n\n\n\nTheory vs Non-Significant Factors\n\nlonglongfactornstheory &lt;- theory_long %&gt;% pivot_longer(\n    cols = FNS1:FNS4,\n    names_to = \"NAMES\",\n    values_to = \"THINGS\"\n) %&gt;% filter(THINGS != \"\" & THINGS != \" \" &\n             THEORY != \" \" & THEORY != \"\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS),\n           THEORY = str_to_upper(THEORY))\n\nlonglongGroupfactorNStheory &lt;- longlongfactornstheory %&gt;%\n    group_by(THEORY) %&gt;% count(THINGS) %&gt;% \n    arrange(desc(n))\n\n\nggplot(longlongGroupfactorNStheory, \n    aes(area = n, fill = THEORY, \n        label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0\n  ) +\n  scale_fill_viridis_d(option = \"plasma\") + \n  scale_color_identity() +   \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7), \n    legend.spacing.y = unit(0.1, \"cm\"),\n    legend.key.height = unit(0.2, \"cm\")\n  ) \n\n\n\n\n\n\n\n\n\nlonglongfactornstheory %&gt;% group_by(THINGS) %&gt;% \n    count(THEORY) %&gt;% arrange(desc(n)) %&gt;% \n    filter(THINGS == \"PERSONAL\")\n\n# A tibble: 0 × 3\n# Groups:   THINGS [0]\n# ℹ 3 variables: THINGS &lt;chr&gt;, THEORY &lt;chr&gt;, n &lt;int&gt;",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_tm.html",
    "href": "study1_tm.html",
    "title": "Topic Modeling",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "study1_tm.html#part-0.-jupyter-notebook",
    "href": "study1_tm.html#part-0.-jupyter-notebook",
    "title": "Topic Modeling",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "study1_tm.html#part-1.",
    "href": "study1_tm.html#part-1.",
    "title": "Topic Modeling",
    "section": "Part 1.",
    "text": "Part 1.\n\n__requires__= 'scipy==1.12.0'\nimport scipy\n\nprint(scipy.__version__)\n\n\n# general python imports \nimport string\nimport os\nimport re\nimport pandas as pd\nimport numpy as np \nimport scipy\nimport itertools\nimport textract\n\n# NLT imports \nimport nltk\nfrom nltk import pos_tag\nfrom nltk.tokenize import regexp_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk.tokenize import RegexpTokenizer\n\n# SKLEARN \nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.naive_bayes import (\n    BernoulliNB,\n    ComplementNB,\n    MultinomialNB,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n\n# GENSIM imports \nimport gensim\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.corpora import MmCorpus\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import KeyedVectors\n\n\n# PyLDAvis imports \nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis\nimport pyLDAvis.gensim\nimport pyLDAvis.gensim_models\n\n\n\n# MISC imports \nfrom collections import Counter\nfrom collections import defaultdict\nfrom string import punctuation\nfrom pprint import pprint\nfrom numpy import triu\n#from scipy.linalg.special_matrices import triu\n#from scipy.sparse import csr_matrix\n\n\n\n# TRANSFORMERS \n#import torch\n#import tensorflow as tf\n#from transformers import BertTokenizer, BertModel\n#from tensorflow.keras.models import Sequential\n#from tensorflow.keras.preprocessing.text import Tokenizer\n#from tensorflow.keras.preprocessing.sequence import pad_sequences\n#from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n#from tensorflow.keras.layers import LeakyReLU\n\nimport fitz  # PyMuPDF\n\n\n# MATPLOT \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#%matplotlib inline\n\n\n# only run once\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\nnltk.download('omw-1.4')  # Optional for better language support\nnltk.download('averaged_perceptron_tagger')  # For POS tagging\nnltk.download('averaged_perceptron_tagger_eng')\n\n\nCLEANING AND PRE-PROCESSING DATA\nThe following procedures are implemented for Data Cleaning:\n\nTurn everything into lower case\nRemove stopwords + additional stopwords such as “bank”, “banking”, “banks”, “mobile”, “mbank”, “mbanking”, “m-bank”, “online”, “digital”, “adoption”, “theory”, “app”, “application”\nRemove punctuation\nLemming/Stemming\n\n\ndf = pd.read_csv(\"P2_AR_01.csv\")\ndf_copy = df.copy() # keeping a copy of the original data \ndf.head()\n\nGrabbing the file names of the pdf files:\n\npdf_directory = \"./pdfs/\"\nall_files = os.listdir(pdf_directory)\npdf_files = [file for file in all_files if file.endswith('.pdf')]\n\noutput_file = \"pdf_file_names.txt\"\nwith open(output_file, \"w\") as f:\n    for pdf in pdf_files:\n        f.write(pdf + \"\\n\")\n\nprint(f\"PDF file names have been saved to {output_file}\")\n\nThen saving them in a python dictionary:\n\nname_of_pdfs = {\n    'p2_101': \"Okocha and Awele Adibi - 2020 - Mobile banking adoption by business executives in .pdf\",\n    # ... \n}\n\nExtract text:\n\n#version one using PyMuPDF \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\n\ntext_of_pdfs_v1 = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    text_of_pdfs_v1[paper_id] = text\n\n# Example: Print the extracted text from the first PDF\nfor paper_id, text in text_of_pdfs_v1.items():\n    print(f\"Text from {paper_id} ({name_of_pdfs[paper_id]}):\")\n    print(text[:500])  # Print the first 500 characters of the text\n    break\n\n\nText from p2_101 (Okocha and Awele Adibi - 2020 - Mobile banking adoption by business executives in .pdf): Mobile banking adoption by business executives in Nigeria Foluke Olabisi Okocha1* and Vera Awele Adibi2 1Centre for Learning Resources, Landmark University, Nigeria 2Doctoral student, University of Ibadan, Nigeria *Corresponding author email: dada.foluke@lmu.edu.ng, folukedada@yahoo.com Challenges with the adoption of mobile banking technologies are best understood by studies on adoption. This however requires understanding the factors that inﬂuence its adoption in a given region. Technology Acc\n\nClean text:\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \"technology\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", #from journal information \n                   \"theory\", \"app\", \"application\", \"usage\", \"model\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\ndef preprocess_Dict(dct):\n    for k, v in dct.items():\n        if isinstance(v, list):\n            processed_list = []\n            for item in v:\n                item = item.lower()\n                item = re.sub(r'http\\S+www\\S+@\\S+', '', item)\n                item = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', item)\n                item = re.sub(r'[^a-z0-9\\s\\n]', '', item)\n                item = re.sub(r'\\s+', ' ', item).strip()\n                item = re.sub(r'\\d+', '', item).strip()\n                item = \" \".join([word for word in item.split() if word not in stop_words])\n                item = \" \".join([stemmer.stem(word) for word in item.split()])\n                item = item.replace('structural equation model', 'sem')\n                item = item.replace('technology acceptance model', 'tam')\n                item = item.replace('unified theory of acceptance and use of technology', 'utaut')\n                item = item.replace('diffusion of innovation', 'doi')\n                item = item.replace('partial least squares', 'pls')\n                item = item.replace('theory of planned behavior', 'tpb')\n                processed_list.append(item)\n            dct[k] = processed_list\n        else:\n            v = v.lower()\n            v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n            v = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', v)\n            v = re.sub(r'[^a-z0-9\\s\\n]', '', v)\n            v = re.sub(r'\\s+', ' ', v).strip()\n            v = re.sub(r'\\d+', '', v).strip()\n            v = \" \".join([word for word in v.split() if word not in stop_words])\n            v = \" \".join([stemmer.stem(word) for word in v.split()])\n            item = item.replace('structural equation model', 'sem')\n            item = item.replace('technology acceptance model', 'tam')\n            item = item.replace('unified theory of acceptance and use of technology', 'utaut')\n            item = item.replace('diffusion of innovation', 'doi')\n            item = item.replace('partial least squares', 'pls')\n            item = item.replace('theory of planned behavior', 'tpb')\n            dct[k] = v\n    return dct\n\nSentence Tokenizer:\n\ndef tokenizeToSentences(doc):\n    for k, v in doc.items():\n        \n        if isinstance(v, bytes):\n            v = v.decode('utf-8')\n          \n        v = v.lower()\n        v = v.replace('\\n', ' ')\n        v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n        #v = \" \".join([str(s) for s in v])\n\n        v = sent_tokenize(v)\n        doc[k] = v\n        \n    return doc\n\n\ntext_of_pdfs_uncleaned_tokenizedSentences_v1 = tokenizeToSentences(text_of_pdfs_v1)\n\nBuild uni and bi-grams:\n\ntext_of_pdfs_uncleaned_tokenize_words_v1 = {}\ntext_of_pdfs_uncleaned_tokenize_bigrams_v1 = {}\n\n\nfor k, v in text_of_pdfs_uncleaned_tokenizedSentences_v1.items():\n    #v is a list of sentences \n    text_of_pdfs_uncleaned_tokenize_words_v1[k] = [word_tokenize(s) for s in v] #list of lists \n    text_of_pdfs_uncleaned_tokenize_bigrams_v1[k] = [list(ngrams(sentence, 2)) for sentence in text_of_pdfs_uncleaned_tokenize_words_v1[k]] \n\nClean:\n\ntext_of_pdfs_cleaned_tokenize_words_v1 = {}\n\n\nfor k, v in text_of_pdfs_uncleaned_tokenize_words_v1.items():\n    # v is a list of lists - where each outer list is a sentence, and the inner list is the words in that sentence. \n    text_of_pdfs_cleaned_tokenize_words_v1[k] = preprocess_listOfLists(v)\n    \n\ntext_of_pdfs_cleaned_tokenize_bigrams_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_words_v1.items():\n    text_of_pdfs_cleaned_tokenize_bigrams_v1[k] = [list(ngrams(sentence, 2)) for sentence in v]\n\n\ntext_of_pdfs_cleaned_tokenize_words_v1['p2_01'][0][:3]\n\n\n[‘doi’, ‘jgim’, ‘global’]\n\n\ntext_of_pdfs_cleaned_tokenize_bigrams_v1['p2_01'][0][:3]\n\n\n[(‘doi’, ‘jgim’), (‘jgim’, ‘global’), (‘global’, ‘inform’)]\n\nStich the bi-grams together:\n\ntext_of_pdfs_cleaned_tokenize_bigrams_combined_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_bigrams_v1.items():\n    text_of_pdfs_cleaned_tokenize_bigrams_combined_v1[k] = [[f\"{a} {b}\" for a, b in sublist] for sublist in v]\n    \ntext_of_pdfs_cleaned_tokenize_bigrams_combined_v1['p2_01'][0][:3]\n\n\n[‘doi jgim’, ‘jgim global’, ‘global inform’]\n\nGenerate Dictionary and Corpuses for unigrams and bigrams\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n    \n    return dictionary, corpus\n\n\npapers_dictionary_unigrams_v1 = {}\npapers_corpus_unigrams_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_words_v1.items():\n    papers_dictionary_unigrams_v1[k] = generate_dictionary(v, 'mmcorpus_unigrams')[0]\n    papers_corpus_unigrams_v1[k] = generate_dictionary(v, 'mmcorpus_unigrams')[1]\n\n\npapers_dictionary_bigrams_v1 = {}\npapers_corpus_bigrams_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_bigrams_combined_v1.items():\n    papers_dictionary_bigrams_v1[k] = generate_dictionary(v, 'mmcorpus_bigrams')[0]\n    papers_corpus_bigrams_v1[k] = generate_dictionary(v, 'mmcorpus_bigrams')[1]\n\nAdditionally, I combine all the PDFs and run this for the entire Database.\n\nentire_database_listoflists_unigrams_v1 = []\n\nfor value in text_of_pdfs_cleaned_tokenize_words_v1.values():\n    entire_database_listoflists_unigrams_v1.extend(value)\n\nentire_database_listoflists_bigrams_v1 = []\n\nfor value in text_of_pdfs_cleaned_tokenize_bigrams_combined_v1.values():\n    entire_database_listoflists_bigrams_v1.extend(value)\n\n\n# database_dictionary_unigrams = {}\n# database_corpus_unigrams = {}\n\ndatabase_dictionary_unigrams_v1 = generate_dictionary(entire_database_listoflists_unigrams_v1, 'mmcorpus_Database_unigrams_v1')[0]\ndatabase_corpus_unigrams_v1 = generate_dictionary(entire_database_listoflists_unigrams_v1, 'mmcorpus_Database_unigrams_v1')[1]\n\n\ndatabase_dictionary_bigrams_v1 = generate_dictionary(entire_database_listoflists_bigrams_v1, 'mmcorpus_Database_bigrams_v1')[0]\ndatabase_corpus_bigrams_v1 = generate_dictionary(entire_database_listoflists_bigrams_v1, 'mmcorpus_Database_bigrams_v1')[1]\n\nPrinting top 50 words across the corpus:\n\n# ---------------------- START OF CHATGPT CODE\ndef print_top_50_words(corpus, dictionary):\n    total_word_count = defaultdict(int)\n    word_weights = defaultdict(float)\n\n    for word_id, word_count in itertools.chain.from_iterable(corpus):\n        total_word_count[word_id] += word_count\n\n    sorted_tota_words_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n\n    tfidf = TfidfModel(corpus)\n\n\n    for doc in corpus:\n        tfidf_weights = tfidf[doc]  # Calculate TF-IDF for the review\n        for term_id, weight in tfidf_weights:\n            word_weights[term_id] += weight  # Aggregate the weight for the term\n\n    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n\n    # Print the top 50 terms with their weights\n    top_50_words = [(dictionary.get(term_id), weight) for term_id, weight in sorted_word_weights[:50]]\n\n    for word, weight in top_50_words:\n        print(word, weight)\n\n# ---------------------- END OF CHATGPT CODE \n\nUni-grams over the entire database:\n\nprint_top_50_words(database_corpus_unigrams_v1, database_dictionary_unigrams_v1)\n\n\nuse 1710.4405813502553 al 1500.5918177863637 et 1495.2598944189729 studi 1254.889113401414 servic 1177.5025518831014 research 1155.4801330260996 model 1093.757883374598 intent 1085.622080362571 inform 1035.95718724093 market 1032.669725701611 manag 1020.5243612360091 custom 1011.465319080724 perceiv 975.0912634817644 consum 959.7309079460276 and many more\n\nBi-grams over the entire database:\n\nprint_top_50_words(database_corpus_bigrams_v1, database_dictionary_bigrams_v1)\n\n\net al 1065.4586868386625 intern market 424.5870797007975 inform manag 324.417783324221 http doiorg 272.8802285987675 inform system 259.07233958915 intent use 247.3467671477514 behavior intent 207.71672202444856 eas use 206.32538882113823 comput human 183.94284111390388 perceiv use 183.0881496709403 human behavior 179.3628870311971 and many more\n\nBuild an LDA model:\n\ndef build_lda_model(n_topic, corpus_, dictionary_):\n    lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus_,\n                                              num_topics = n_topic,\n                                              id2word = dictionary_,\n                                              random_state = 100,\n                                              update_every = 1,\n                                              chunksize = 1000,\n                                              passes = 10,\n                                              alpha = 'auto',\n                                              per_word_topics = True)\n    return lda_model\n\n\ndef train_models(corpus_, dictionary_):\n    list_to_hold_models = []\n    topic_n_to_try = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n    \n    for i in range(len(topic_n_to_try)):\n        list_to_hold_models.insert(i, build_lda_model(topic_n_to_try[i], corpus_, dictionary_))\n        \n    return list_to_hold_models\n\n\ndef calculate_perplexity(model, corpus_):\n    perplexity = model.log_perplexity(corpus_)\n    return perplexity\n\ndef calculate_coherence(model, text, dictionary_):\n    coherence_model_lda = CoherenceModel(model = model, texts = text, dictionary = dictionary_, coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n\n    return coherence_lda\n\n\ndef build_model_comparison_table(list_of_models, corpus_, dictionary_, data):\n    tracker = 5 \n    models_perplexity = []\n    models_coherence = []\n    models_topics = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n    \n    \n    for model in list_of_models:\n        models_perplexity.append(calculate_perplexity(model, corpus_))\n        models_coherence.append(calculate_coherence(model, data, dictionary_))\n        tracker += 1\n        \n    if tracker == 10:\n        print(\"Successfully generated model comparison table.\") \n        \n    models_df = pd.DataFrame({\n        'Num_Topics': models_topics,\n        'Coherence': models_coherence,\n        'Perplexity': models_perplexity,\n    })\n\n    return models_df\n\n\ndef find_best_model(models_df):\n    print(\"Number of topics with the maximum Coherence is \", models_df.loc[models_df['Coherence'].idxmax(), 'Num_Topics'])\n    print(\"Number of topics with the minimum Perplexity is \", models_df.loc[models_df['Perplexity'].idxmin(), 'Num_Topics'])\n    \n    if models_df.loc[models_df['Coherence'].idxmax(), 'Num_Topics'] == models_df.loc[models_df['Perplexity'].idxmin(), 'Num_Topics']:\n        best_model_row = models_df.loc[models_df['Perplexity'].idxmin()]\n        best_number_of_topics = best_model_row['Num_Topics']\n    else:\n        models_df['Normalized_Perplexity'] = (models_df['Perplexity'] - models_df['Perplexity'].min()) / (models_df['Perplexity'].max() - models_df['Perplexity'].min())\n        models_df['Normalized_Coherence'] = (models_df['Coherence'] - models_df['Coherence'].min()) / (models_df['Coherence'].max() - models_df['Coherence'].min())\n\n        models_df['Inverted_Perplexity'] = 1 - models_df['Normalized_Perplexity'] # because smaller is better\n\n        weight_preplexity = 0.5\n        weight_coherence = 0.5\n\n        models_df['Score'] = weight_coherence * models_df['Normalized_Coherence'] + weight_preplexity * models_df['Inverted_Perplexity']\n\n        best_model_row = models_df.loc[models_df['Score'].idxmax()]\n        best_number_of_topics = best_model_row['Num_Topics']\n\n    print(best_model_row)\n                                       \n    return best_model_row, best_number_of_topics\n                                                                                                     \n                                                                                                \ndef pick_best_model(num, m):\n    \"\"\" \n    Model inputs are: \n        num = best number of topics found according to find_best_model()\n        m = list of models \n    \"\"\"\n    model_index = num - 5 \n    model_index = int(model_index)\n                                       \n    best_model = m[model_index]\n                                       \n    return best_model  \n                                                                  \ndef print_topics(model, corpus):\n    pprint(model.print_topics())\n    doc_lda = model[corpus]\n    \n    return doc_lda\n\n\nTopic Modeling - Unigrams\n\nunigram_models_v1 = train_models(database_corpus_unigrams_v1, database_dictionary_unigrams_v1)\n\n\nunigram_model_comparison_v1 = build_model_comparison_table(unigram_models_v1, database_corpus_unigrams_v1, database_dictionary_unigrams_v1, entire_database_listoflists_unigrams_v1)\nunigram_model_comparison_v1\n\n\n\n\nNum_Topics\nCoherence\nPerplexity\n\n\n\n\n0\n5\n0.436565\n\n\n1\n6\n0.413618\n\n\n2\n7\n0.469700\n\n\n3\n8\n0.400105\n\n\n4\n9\n0.452116\n\n\n5\n10\n0.420971\n\n\n6\n11\n0.446276\n\n\n7\n12\n0.454530\n\n\n8\n13\n0.409933\n\n\n9\n14\n0.418211\n\n\n10\n15\n0.406770\n\n\n\n\nunigram_best_row_v1 = find_best_model(unigram_model_comparison_v1)[0]\nunigram_best_n_topics_v1 = find_best_model(unigram_model_comparison_v1)[1]\n\n\nNumber of topics with the maximum Coherence is 7 Number of topics with the minimum Perplexity is 15 Num_Topics 12.000000 Coherence 0.454530 Perplexity -9.011387 Normalized_Perplexity 0.636029 Normalized_Coherence 0.782020 Inverted_Perplexity 0.363971 Score 0.572996 Name: 7, dtype: float64 Number of topics with the maximum Coherence is 7 Number of topics with the minimum Perplexity is 15 Num_Topics 12.000000 Coherence 0.454530 Perplexity -9.011387 Normalized_Perplexity 0.636029 Normalized_Coherence 0.782020 Inverted_Perplexity 0.363971 Score 0.572996 Name: 7, dtype: float64\n\n\nunigram_best_model_v1 = pick_best_model(unigram_best_n_topics_v1, unigram_models_v1)\nprint(\"Best Unigram model is (V1):\", unigram_best_model_v1)\n\n\nBest Unigram model is (V1): LdaModel(num_terms=27200, num_topics=12, decay=0.5, chunksize=1000)\n\n\n\nTopic Modeling - Bigrams\n\nbigram_models_v1 = train_models(database_corpus_bigrams_v1, database_dictionary_bigrams_v1)\n\n\nbigram_model_comparison_v1 = build_model_comparison_table(bigram_models_v1, database_corpus_bigrams_v1, database_dictionary_bigrams_v1, entire_database_listoflists_bigrams_v1)\nbigram_model_comparison_v1\n\n\n\n\nNum_Topics\nCoherence\nPerplexity\n\n\n\n\n0\n5\n0.558434\n\n\n1\n6\n0.535400\n\n\n2\n7\n0.542287\n\n\n3\n8\n0.515335\n\n\n4\n9\n0.523767\n\n\n5\n10\n0.526290\n\n\n6\n11\n0.523879\n\n\n7\n12\n0.513803\n\n\n8\n13\n0.510867\n\n\n9\n14\n0.554809\n\n\n10\n15\n0.582336\n\n\n\n\nbigram_best_row_v1 = find_best_model(bigram_model_comparison_v1)[0]\nbigram_best_n_topics_v1 = find_best_model(bigram_model_comparison_v1)[1]\n\n\nNumber of topics with the maximum Coherence is 15 Number of topics with the minimum Perplexity is 15 Num_Topics 15.000000 Coherence 0.582336 Perplexity -24.581214 Name: 10, dtype: float64 Number of topics with the maximum Coherence is 15 Number of topics with the minimum Perplexity is 15 Num_Topics 15.000000 Coherence 0.582336 Perplexity -24.581214 Name: 10, dtype: float64\n\n\nbigram_best_model_v1 = pick_best_model(bigram_best_n_topics_v1, bigram_models_v1)\nprint(\"Best Unigram model is (V1):\", bigram_best_model_v1)\n\n\nBest Unigram model is (V1): LdaModel(num_terms=306163, num_topics=15, decay=0.5, chunksize=1000)\n\n\n\nPick Best Model\n\ndef model_score(p, c, wp = 0.5, wc = 0.5):\n    \"\"\" Calculates model score with 0.5 weights as default\"\"\"\n    score = (1 - p) * wp + c * wc \n    return score\n\n\nprint(\"Best unigram model's score is (V1):\", model_score(-9.011387,0.454530))\nprint(\"Best bigram model's score is (V1):\", model_score(-24.581214,0.582336))\n\n\n\n\nNGRAM\nPerplexity\nCoherence\n# of topics\nScore\n\n\n\n\nUNI\n-9.011387\n0.454530\n12\n5.2329585\n\n\nBI\n-24.581214\n0.582336\n15\n13.081775\n\n\n\nThe best model overall is therefore bigram_best_model.\n\nbest_topic_model_v1 = bigram_best_model_v1\nnumber_of_topics = 8\n\n\nprint_topics(best_topic_model_v1, database_corpus_bigrams_v1)\n\n\n\n\n\nTopic Modeling using Keywords\n\nkeywordsDf = df.loc[:,'K1':'K10']\n\nkeywords_across_db = keywordsDf.values.flatten().tolist()\nlen(keywords_across_db)\n\n\nimport math\n\nempty_or_na_count = sum(1 for x in keywords_across_db if x in [None, \"\", ' '] or (isinstance(x, float) and math.isnan(x)))\n\nprint(f\"Number of empty or NA values: {empty_or_na_count}\")\n\n\nkeywords_across_db = [x for x in keywords_across_db if x not in [None, \"\", ' '] and not (isinstance(x, float) and math.isnan(x))]\n\nkeywords_across_db_nodup = list(set(keywords_across_db))\n\n\nfrom transformers import BertTokenizer, BertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel_bert = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embedding(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\n\ndef print_clusters(n_clusters, list_of_words):\n    clusters = {i: [] for i in range(n_clusters)}\n    for word, label in zip(list_of_words, labels):\n        clusters[label].append(word)\n\n    for label, words in clusters.items():\n        print(f\"Cluster {label}:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n    # Explain clusters\n    print(\"Cluster explanations based on semantics and ideas:\")\n    for label, words in clusters.items():\n        print(f\"Cluster {label} might be related to:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n\nimport torch \n\nkeyword_embeddings = np.array([get_embedding(phrase) for phrase in keywords_across_db_nodup])\n\nn_clusters = number_of_topics\nkmeans = KMeans(n_clusters = n_clusters, random_state = 0)\nkmeans.fit(keyword_embeddings)\nlabels = kmeans.labels_\n\n\nprint_clusters(n_clusters, keywords_across_db_nodup)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "study2_DA.html",
    "href": "study2_DA.html",
    "title": "Data Analysis - CIUS 2020",
    "section": "",
    "text": "Little bit about the work I will do\nLittle bit about CIUS 2020\nData I need:\nds20 &lt;- read_dta(\"data/cius2020_2022nov18_en.dta\")\nds &lt;- ds20\nSelect only those who use smartphones:\nds &lt;- ds %&gt;% \n    mutate(\n        devSM = case_when(\n        dv_010a == 1 ~ 1, #yes\n        dv_010a == 2 ~ 0, #no\n        .default = -1, #any valid skip and not stated \n        )\n    )\n\nds &lt;- ds %&gt;% \n   filter(devSM == 1)\nFor Mobile banking:\nFor Smartphone Dependency:\nds &lt;- ds %&gt;%\n    mutate(\n        #timeline : past 3 months \n        mBanking = case_when(\n            ui_050d == 1 ~ 1, # Yes \n            ui_050d == 2 ~ 0, # No \n            .default = -1 # valid skip, don't know, refused, not stated \n        ),\n        \n        SD = case_when(\n            sm_030a == 1 ~ 6, # At least every 5 minutes \n            sm_030a == 2 ~ 5, # At least every 15 minutes \n            sm_030a == 3 ~ 4, # At least every 30 minutes  \n            sm_030a == 4 ~ 3, # One time per hour  \n            sm_030a == 5 ~ 2, # Once a day or a few times per day  \n            sm_030a == 6 ~ 1, # Less than one time per day\n            .default = 96 # Valid skip 96 , Don’t know 97 , Refusal  98, Not stated  99\n        )\n    )\n\nds &lt;- ds %&gt;% filter(SD &lt; 10)\nds &lt;- ds %&gt;% filter(mBanking != -1)\nFriendship Satisfaction:\nFamily Satisfaction:\nMental Health:\nds &lt;- ds %&gt;% mutate(\n    FRISAT = case_when(\n        ts_010a == 1 ~ 1, #completely dissatisfied \n        ts_010a == 2 ~ 2, \n        ts_010a == 3 ~ 3,\n        ts_010a == 4 ~ 4,\n        ts_010a == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    FAMSAT = case_when(\n        ts_010b == 1 ~ 1, #completely dissatisfied \n        ts_010b == 2 ~ 2, \n        ts_010b == 3 ~ 3,\n        ts_010b == 4 ~ 4,\n        ts_010b == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    MH = case_when(\n        fd_030a == 1 ~ 5, #excellent \n        fd_030a == 2 ~ 4, #very good \n        fd_030a == 3 ~ 3, #good \n        fd_030a == 4 ~ 2, #fair\n        fd_030a == 5 ~ 1, #poor\n        .default = 6\n    )\n)\n\nds &lt;- ds %&gt;% filter(\n    FRISAT &lt; 6,\n    FAMSAT &lt; 6,\n    MH &lt; 6\n)\nSocial Media use:\nds &lt;- ds %&gt;% mutate(\n    id = pumfid,\n    province = province, \n    #LOC = luc_rst, #rural, urban, PE! \n    AGE = as.integer(age_grp),\n    SEX = gender,\n    #ABO = g_abm, #is aboriginal \n    #LAN = lan_g01,\n    EMP = ifelse(\n        emp == 2,\n        0,\n        emp\n    ),\n    #STU = ed_g10, #is a student? --- don't include it! \n    EDU = g_edu,\n    #MINORITY = g_vismin, #is visible minority?\n    #DIS = dis_g10,\n    FAM = g_hcomp, #type of family: children under 18 \n    IMM = ifelse(\n        imm_gsta == 2,\n        0,\n        imm_gsta\n    ),\n    #HSIZE = g_hsize, #household size\n    INC = hincquin,\n    SNS = case_when(\n        ui_010c == 1 ~ 1, # yes \n        ui_010c == 2 ~ 0, # no \n        .default = 3\n    )\n    \n)\n\nds &lt;- ds %&gt;% filter(\n    SNS &lt; 3,\n    EMP &lt; 3,\n    FAM &lt; 5,\n    IMM &lt; 3\n)\nds &lt;- ds %&gt;% \n    dplyr::select(id, \n                  mBanking, SD, FAMSAT, FRISAT, MH, SNS,\n                  province, AGE, SEX, EMP, EDU,\n                  FAM, IMM, INC, wtpg)\nSize of the dataset:\ndim(ds)\n\n[1] 11176    16",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#screening",
    "href": "study2_DA.html#screening",
    "title": "Data Analysis - CIUS 2020",
    "section": "SCREENING",
    "text": "SCREENING\n\npsych::describe(ds, type = 2)\n\n         vars     n      mean      sd    median   trimmed     mad      min\nid          1 11176 108666.23 5048.86 108637.50 108660.43 6530.85 100001.0\nmBanking    2 11176      0.86    0.34      1.00      0.95    0.00      0.0\nSD          3 11176      3.39    1.22      3.00      3.32    1.48      1.0\nFAMSAT      4 11176      4.27    0.92      5.00      4.42    0.00      1.0\nFRISAT      5 11176      4.28    0.90      5.00      4.42    0.00      1.0\nMH          6 11176      3.59    1.04      4.00      3.65    1.48      1.0\nSNS         7 11176      0.79    0.41      1.00      0.86    0.00      0.0\nprovince    8 11176     32.33   15.79     35.00     31.72   17.79     10.0\nAGE*        9 11176      3.98    1.52      4.00      4.06    1.48      1.0\nSEX        10 11176      1.53    0.50      2.00      1.54    0.00      1.0\nEMP        11 11176      1.37    0.48      1.00      1.33    0.00      1.0\nEDU        12 11176      2.11    0.80      2.00      2.13    1.48      1.0\nFAM        13 11176      1.98    0.78      2.00      1.94    1.48      1.0\nIMM        14 11176      1.88    0.32      2.00      1.98    0.00      1.0\nINC        15 11176      3.10    1.37      3.00      3.12    1.48      1.0\nwtpg       16 11176   2054.12 2104.81   1470.54   1681.77 1375.86     18.2\n               max    range  skew kurtosis    se\nid       117407.00 17406.00  0.01    -1.22 47.76\nmBanking      1.00     1.00 -2.12     2.51  0.00\nSD            6.00     5.00  0.33    -0.70  0.01\nFAMSAT        5.00     4.00 -1.30     1.47  0.01\nFRISAT        5.00     4.00 -1.29     1.49  0.01\nMH            5.00     4.00 -0.39    -0.44  0.01\nSNS           1.00     1.00 -1.39    -0.07  0.00\nprovince     59.00    49.00  0.16    -1.13  0.15\nAGE*          6.00     5.00 -0.27    -1.02  0.01\nSEX           2.00     1.00 -0.11    -1.99  0.00\nEMP           2.00     1.00  0.56    -1.69  0.00\nEDU           9.00     8.00  0.27     2.35  0.01\nFAM           4.00     3.00  0.39    -0.37  0.01\nIMM           2.00     1.00 -2.35     3.52  0.00\nINC           5.00     4.00 -0.08    -1.22  0.01\nwtpg      23728.02 23709.82  2.67    11.27 19.91\n\n\nSeems like it’s kind of rare for those that use m-banking to have lower MH scores. Descriptive statistics:\n\nggplot(data    = ds,\n       aes(x   = SD,\n           y   = wtpg,\n           color = as.factor(MH)))+ \n  geom_point() +\n  geom_jitter() +  \n  labs( x = \"Smartphone Dependency\", \n        y = \"Weight\", \n        color = \"MH\") + \n  theme_minimal() \n\n\n\n\n\n\n\n\nChecking Na’s:\n\nsum(is.na(ds))\n\n[1] 0\n\n\n\nglimpse(ds)\n\nRows: 11,176\nColumns: 16\n$ id       &lt;dbl&gt; 100001, 100002, 100003, 100004, 100005, 100007, 100008, 10001…\n$ mBanking &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1…\n$ SD       &lt;dbl&gt; 2, 5, 4, 2, 5, 3, 4, 3, 2, 4, 4, 5, 2, 3, 4, 6, 4, 5, 4, 4, 2…\n$ FAMSAT   &lt;dbl&gt; 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 3, 4, 4, 4, 5, 5, 4…\n$ FRISAT   &lt;dbl&gt; 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 2, 4, 4, 4, 5, 3, 4…\n$ MH       &lt;dbl&gt; 4, 3, 4, 5, 3, 3, 5, 3, 4, 2, 3, 3, 3, 4, 2, 2, 4, 3, 4, 3, 4…\n$ SNS      &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ province &lt;dbl&gt; 59, 35, 35, 24, 48, 13, 48, 12, 35, 48, 13, 12, 24, 46, 24, 2…\n$ AGE      &lt;chr&gt; \"06\", \"02\", \"03\", \"06\", \"04\", \"04\", \"02\", \"02\", \"02\", \"05\", \"…\n$ SEX      &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ EMP      &lt;dbl&gt; 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1…\n$ EDU      &lt;dbl&gt; 2, 3, 3, 3, 1, 3, 2, 1, 2, 2, 2, 2, 3, 3, 3, 1, 3, 3, 2, 2, 1…\n$ FAM      &lt;dbl&gt; 2, 1, 2, 2, 3, 3, 1, 2, 1, 2, 2, 1, 3, 2, 1, 3, 3, 3, 1, 2, 2…\n$ IMM      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2…\n$ INC      &lt;dbl&gt; 4, 3, 2, 4, 2, 3, 2, 5, 4, 5, 2, 5, 2, 5, 2, 3, 4, 4, 4, 4, 3…\n$ wtpg     &lt;dbl&gt; 2524.87054, 3720.74855, 5802.87880, 1449.85202, 1155.15218, 1…",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#relationships-visualizations-contingency-correlations",
    "href": "study2_DA.html#relationships-visualizations-contingency-correlations",
    "title": "Data Analysis - CIUS 2020",
    "section": "RELATIONSHIPS: Visualizations, Contingency, Correlations",
    "text": "RELATIONSHIPS: Visualizations, Contingency, Correlations\nMental Health and Mbanking:\n\nggplot(data = ds, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(mBanking)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"Pastel1\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Frequencies\") + labs(fill = \"Mbanking\")\n\n\n\n\n\n\n\n\nMH and controls: AGE, SEX, EMP, EDU, FAM, IMM, INC\n\ngg_fam &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMILY') + fill_palette(\"Pastel1\")\n\ngg_age &lt;- ggplot(data = ds , aes(MH, fill = as.factor(AGE))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'AGE') + fill_palette(\"Pastel1\")\n\ngg_edu &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EDU))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EDU') + fill_palette(\"Pastel1\")\n\ngg_inc &lt;- ggplot(data = ds , aes(MH, fill = as.factor(INC))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'INC') + fill_palette(\"Pastel1\")\n\ngg_sex &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SEX))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SEX') + fill_palette(\"Pastel1\")\n\ngg_emp &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EMP))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EMP') + fill_palette(\"Pastel1\")\n\ngg_imm &lt;- ggplot(data = ds , aes(MH, fill = as.factor(IMM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'Immigrant') + fill_palette(\"Pastel1\")\n\n\nggarrange(\n    gg_fam, gg_age, gg_edu, gg_inc, gg_sex, gg_emp, gg_imm,\n    labels = c(\"FAM\", \"AGE\", \"EDU\", \"INC\", \"SEX\", \"EMP\", \"IMM\"),\n    ncol = 3,\n    nrow = 3\n    \n) \n\n\n\n\n\n\n\n\nMH and other variables: SD, FAMSAT, FRISAT, SNS\n\ngg_frisat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FRISAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FRISAT') + fill_palette(\"Pastel1\")\n\ngg_famsat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAMSAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMSAT') + fill_palette(\"Pastel1\")\n\ngg_sd &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SD))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SD') + fill_palette(\"Pastel1\")\n\ngg_sns &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SNS))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SNS') + fill_palette(\"Pastel1\")\n\nggarrange(\n    gg_frisat, gg_famsat, gg_sd, gg_sns,\n    labels = c(\"FRISAT\", \"FAMSAT\", \"SD\", \"SNS\"),\n    ncol = 2,\n    nrow = 2\n    \n) \n\n\n\n\n\n\n\n\n\nsle &lt;- ds %&gt;% dplyr::select(mBanking, SD, FAMSAT, FRISAT, SNS, AGE, SEX, EMP, EDU, FAM, INC, MH)\ncorM &lt;- Hmisc::rcorr(as.matrix(sle))\nreg_corM &lt;- as.matrix(corM$r)\n\ncolnames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \nrownames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \ncorrplot::corrplot(reg_corM, p.mat = corM$P, method = \"color\", type = \"upper\", insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, order = 'AOE', tl.col = \"black\", tl.cex = 1, diag = F, col = corrplot::COL2('PuOr'))\n\n\n\n\n\n\n\n\nFRISAT and FAMSAT are obviously highly correlated. I’ll combine them into a new variable called RS for Relationship satisfaction.\n\nds &lt;- ds %&gt;% mutate(\n    RS = FAMSAT + FRISAT\n)\n\n\nggplot(data  = ds, aes(x = MH, y = SD))+ geom_point(size = 1.2, alpha = .5, position = \"jitter\") + \n    geom_smooth(method = lm,\n              se     = FALSE, \n              col    = \"red\",\n              size   = 2, \n              alpha  = .8)+ # to add regression line\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal() + scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"red\", \"lightblue\"))\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"darkred\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = AGE, #CHANGE THIS &lt;&lt;&lt;\n           y   = MH, #CHANGE THIS &lt;&lt;&lt;\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"grey70\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\nds1 &lt;- ds %&gt;% filter(mBanking == 1)\nds2 &lt;- ds %&gt;% filter(mBanking == 0)\n\ngg5 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\ngg6 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\nggarrange(gg5, gg6, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\n\ngg7 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\ngg8 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\nggarrange(gg7, gg8, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\n\ngg9 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\ngg10 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\nggarrange(gg9, gg10, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2.html",
    "href": "study2.html",
    "title": "Mental Health and Mobile Banking",
    "section": "",
    "text": "In this chapter, I look into how mental health might affect whether people choose to adopt mobile banking or not. While doing my review of past studies (refer to study 1 ), I noticed that a lot of research talks about how things like trust, risk, and perceived ease of use affect adoption. But very few studies look at how a person’s mental and emotional state influences their behavior. Especially because some studies actively showed how not accounting for people’s emotions were sometiems giving them strange results.\nI also found that even if the literature did talk about emotions, it was mostly about emotions as a result of the adoption/use of a technology. Not about how your general mood or feelings or struggles impact how you behave and interact with technology. Like I know when I’m stressed out or dreading something, the doom scrolling gets worse! But social media apps are built to be addictive and drag you in and make you want it more. Until you actually stop enjoying it and being online is no longer fun. In fact, I read just recently how engagement and use of social media apps has dropped in recent years! I’ll find the citation for it later! Anyway, I digress… Point is, I think your mood definitely affects how you interact with the world and technology is part of that world. So, what about people who have special moods? That is, certain behavioral or mental challenges? Not exactly disabilities, but hidden challenges often ignored!\nI wanted to explore whether people who are feeling mentally well are more or less likely to use mobile banking. My guess going in was that people with better mental health might feel more confident and more open to using this tech. But I found out it was actually the opposite! Then I got to thinking… why is that? Well, Here’s my guess:\n\nMaybe people with poorer mental health actually rely on mobile banking to avoid going out and talking to people, or dealing with stress in traditional banking.\nMaybe people who are happy don’t bank and don’t stress about money!\nIt’s not a causation, and I don’t know which comes first - are people who bank less happier or is it that happy people don’t bank as often? This is not the study to answer that. But that would be a cool study!\n\nI used Canadian survey data and looked at three things related to mental health:\n\nHow satisfied are you in your relationships (RS)\nHow dependent are you on your smartphones (SD)\nWhether or not you use social media (SNS)\n\nI also tested whether these three things change (or “moderate”) how mental health affects mobile banking adoption. This chapter is where I introduce mental health as a new factor in mobile banking adoption models. I show how mental and emotional states might be just as important as the usual stuff like trust or app features. The results surprised me, and hopefully, they’ll give banks and researchers something new to think about too.\nBy the way, this chapter was already published in .",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health"
    ]
  },
  {
    "objectID": "study2_DA.html#exploring-data",
    "href": "study2_DA.html#exploring-data",
    "title": "Data Analysis - CIUS 2020",
    "section": "EXPLORING DATA",
    "text": "EXPLORING DATA\n\nSCREENING\n\npsych::describe(ds, type = 2)\n\n         vars     n      mean      sd    median   trimmed     mad      min\nid          1 11176 108666.23 5048.86 108637.50 108660.43 6530.85 100001.0\nmBanking    2 11176      0.86    0.34      1.00      0.95    0.00      0.0\nSD          3 11176      3.39    1.22      3.00      3.32    1.48      1.0\nFAMSAT      4 11176      4.27    0.92      5.00      4.42    0.00      1.0\nFRISAT      5 11176      4.28    0.90      5.00      4.42    0.00      1.0\nMH          6 11176      3.59    1.04      4.00      3.65    1.48      1.0\nSNS         7 11176      0.79    0.41      1.00      0.86    0.00      0.0\nprovince    8 11176     32.33   15.79     35.00     31.72   17.79     10.0\nAGE         9 11176      3.98    1.52      4.00      4.06    1.48      1.0\nSEX        10 11176      1.53    0.50      2.00      1.54    0.00      1.0\nEMP        11 11176      0.63    0.48      1.00      0.67    0.00      0.0\nEDU        12 11176      2.11    0.80      2.00      2.13    1.48      1.0\nFAM        13 11176      1.98    0.78      2.00      1.94    1.48      1.0\nIMM        14 11176      0.12    0.32      0.00      0.02    0.00      0.0\nINC        15 11176      3.10    1.37      3.00      3.12    1.48      1.0\nwtpg       16 11176   2054.12 2104.81   1470.54   1681.77 1375.86     18.2\n               max    range  skew kurtosis    se\nid       117407.00 17406.00  0.01    -1.22 47.76\nmBanking      1.00     1.00 -2.12     2.51  0.00\nSD            6.00     5.00  0.33    -0.70  0.01\nFAMSAT        5.00     4.00 -1.30     1.47  0.01\nFRISAT        5.00     4.00 -1.29     1.49  0.01\nMH            5.00     4.00 -0.39    -0.44  0.01\nSNS           1.00     1.00 -1.39    -0.07  0.00\nprovince     59.00    49.00  0.16    -1.13  0.15\nAGE           6.00     5.00 -0.27    -1.02  0.01\nSEX           2.00     1.00 -0.11    -1.99  0.00\nEMP           1.00     1.00 -0.56    -1.69  0.00\nEDU           9.00     8.00  0.27     2.35  0.01\nFAM           4.00     3.00  0.39    -0.37  0.01\nIMM           1.00     1.00  2.35     3.52  0.00\nINC           5.00     4.00 -0.08    -1.22  0.01\nwtpg      23728.02 23709.82  2.67    11.27 19.91\n\n\nSeems like it’s kind of rare for those that use m-banking to have lower MH scores. Descriptive statistics:\n\nggplot(data    = ds,\n       aes(x   = SD,\n           y   = wtpg,\n           color = as.factor(MH)))+ \n  geom_point() +\n  geom_jitter() +  \n  labs( x = \"Smartphone Dependency\", \n        y = \"Weight\", \n        color = \"MH\") + \n  theme_minimal() \n\n\n\n\n\n\n\n\nChecking Na’s:\n\nsum(is.na(ds))\n\n[1] 0\n\n\n\nglimpse(ds)\n\nRows: 11,176\nColumns: 16\n$ id       &lt;dbl&gt; 100001, 100002, 100003, 100004, 100005, 100007, 100008, 10001…\n$ mBanking &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1…\n$ SD       &lt;dbl&gt; 2, 5, 4, 2, 5, 3, 4, 3, 2, 4, 4, 5, 2, 3, 4, 6, 4, 5, 4, 4, 2…\n$ FAMSAT   &lt;dbl&gt; 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 3, 4, 4, 4, 5, 5, 4…\n$ FRISAT   &lt;dbl&gt; 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 2, 4, 4, 4, 5, 3, 4…\n$ MH       &lt;dbl&gt; 4, 3, 4, 5, 3, 3, 5, 3, 4, 2, 3, 3, 3, 4, 2, 2, 4, 3, 4, 3, 4…\n$ SNS      &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ province &lt;dbl&gt; 59, 35, 35, 24, 48, 13, 48, 12, 35, 48, 13, 12, 24, 46, 24, 2…\n$ AGE      &lt;int&gt; 6, 2, 3, 6, 4, 4, 2, 2, 2, 5, 5, 4, 2, 6, 2, 1, 6, 3, 3, 4, 5…\n$ SEX      &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ EMP      &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1…\n$ EDU      &lt;dbl&gt; 2, 3, 3, 3, 1, 3, 2, 1, 2, 2, 2, 2, 3, 3, 3, 1, 3, 3, 2, 2, 1…\n$ FAM      &lt;dbl&gt; 2, 1, 2, 2, 3, 3, 1, 2, 1, 2, 2, 1, 3, 2, 1, 3, 3, 3, 1, 2, 2…\n$ IMM      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0…\n$ INC      &lt;dbl&gt; 4, 3, 2, 4, 2, 3, 2, 5, 4, 5, 2, 5, 2, 5, 2, 3, 4, 4, 4, 4, 3…\n$ wtpg     &lt;dbl&gt; 2524.87054, 3720.74855, 5802.87880, 1449.85202, 1155.15218, 1…\n\n\n\n\nRELATIONSHIPS: Visualizations, Contingency, Correlations\nMental Health and Mbanking:\n\nggplot(data = ds, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(mBanking)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"Pastel1\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Frequencies\") + labs(fill = \"Mbanking\")\n\n\n\n\n\n\n\n\nMH and controls: AGE, SEX, EMP, EDU, FAM, IMM, INC\n\ngg_fam &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMILY') + fill_palette(\"Pastel1\")\n\ngg_age &lt;- ggplot(data = ds , aes(MH, fill = as.factor(AGE))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'AGE') + fill_palette(\"Pastel1\")\n\ngg_edu &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EDU))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EDU') + fill_palette(\"Pastel1\")\n\ngg_inc &lt;- ggplot(data = ds , aes(MH, fill = as.factor(INC))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'INC') + fill_palette(\"Pastel1\")\n\ngg_sex &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SEX))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SEX') + fill_palette(\"Pastel1\")\n\ngg_emp &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EMP))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EMP') + fill_palette(\"Pastel1\")\n\ngg_imm &lt;- ggplot(data = ds , aes(MH, fill = as.factor(IMM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'Immigrant') + fill_palette(\"Pastel1\")\n\n\nggarrange(\n    gg_fam, gg_age, gg_edu, gg_inc, gg_sex, gg_emp, gg_imm,\n    labels = c(\"FAM\", \"AGE\", \"EDU\", \"INC\", \"SEX\", \"EMP\", \"IMM\"),\n    ncol = 3,\n    nrow = 3\n    \n) \n\n\n\n\n\n\n\n\nMH and other variables: SD, FAMSAT, FRISAT, SNS\n\ngg_frisat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FRISAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FRISAT') + fill_palette(\"Pastel1\")\n\ngg_famsat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAMSAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMSAT') + fill_palette(\"Pastel1\")\n\ngg_sd &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SD))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SD') + fill_palette(\"Pastel1\")\n\ngg_sns &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SNS))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SNS') + fill_palette(\"Pastel1\")\n\nggarrange(\n    gg_frisat, gg_famsat, gg_sd, gg_sns,\n    labels = c(\"FRISAT\", \"FAMSAT\", \"SD\", \"SNS\"),\n    ncol = 2,\n    nrow = 2\n    \n) \n\n\n\n\n\n\n\n\n\nsle &lt;- ds %&gt;% dplyr::select(mBanking, SD, FAMSAT, FRISAT, SNS, AGE, SEX, EMP, EDU, FAM, INC, MH)\ncorM &lt;- Hmisc::rcorr(as.matrix(sle))\nreg_corM &lt;- as.matrix(corM$r)\n\ncolnames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \nrownames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \ncorrplot::corrplot(reg_corM, p.mat = corM$P, method = \"color\", type = \"upper\", insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, order = 'AOE', tl.col = \"black\", tl.cex = 1, diag = F, col = corrplot::COL2('PuOr'))\n\n\n\n\n\n\n\n\nFRISAT and FAMSAT are obviously highly correlated. I’ll combine them into a new variable called RS for Relationship satisfaction.\n\nds &lt;- ds %&gt;% mutate(\n    RS = FAMSAT + FRISAT\n)\n\n\nShowing that it’s ok to replace FAMSAT + FRISAT with RS\nUsing a Likelihood Ratio Test:\n\nmodel_famsatfrisat &lt;- glm(mBanking ~ FAMSAT + FRISAT, \n                          family = \"binomial\",\n                          data = ds)\n\nmodel_rs &lt;- glm(mBanking ~ RS, \n                          family = \"binomial\",\n                          data = ds)   \n\nanova(model_rs, model_famsatfrisat, test = \"Chisq\")               \n\nAnalysis of Deviance Table\n\nModel 1: mBanking ~ RS\nModel 2: mBanking ~ FAMSAT + FRISAT\n  Resid. Df Resid. Dev Df  Deviance Pr(&gt;Chi)\n1     11174     8890.9                      \n2     11173     8890.9  1 0.0045091   0.9465\n\n\nSince p &gt; 0.05, there is no significant loss in model performance. It’s better to keep one variable instead of two.\n\n\n\nSome Visualizations\n\nggplot(data  = ds, aes(x = MH, y = SD))+ geom_point(size = 1.2, alpha = .5, position = \"jitter\") + \n    geom_smooth(method = lm,\n              se     = FALSE, \n              col    = \"red\",\n              size   = 2, \n              alpha  = .8)+ # to add regression line\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal() + scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"red\", \"lightblue\"))\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"darkred\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = AGE, #CHANGE THIS &lt;&lt;&lt;\n           y   = MH, #CHANGE THIS &lt;&lt;&lt;\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"grey70\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\nds1 &lt;- ds %&gt;% filter(mBanking == 1)\nds2 &lt;- ds %&gt;% filter(mBanking == 0)\n\ngg5 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\ngg6 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\nggarrange(gg5, gg6, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\n\ngg7 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\ngg8 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\nggarrange(gg7, gg8, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\n\ngg9 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\ngg10 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\nggarrange(gg9, gg10, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\n\n\nModeling\nPreparing the data for modeling:\n\nds &lt;- ds %&gt;% mutate(\n    MH_c = MH - mean(MH), \n    SD_c = SD - mean(SD),\n    SNS_f = as.factor(SNS),\n    RS_c = RS - mean(RS),\n    AGE_c = AGE - mean(AGE),\n    SEX_f = as.factor(SEX),\n    EMP_f = as.factor(EMP),\n    EDU_c = EDU - mean(EDU),\n    FAM_f = as.factor(FAM),\n    INC_c = INC - mean(INC),\n    IMM_f = as.factor(IMM),\n    PRVNC = as.factor(province)\n)\n\nds &lt;- ds %&gt;% \n    mutate(\n        # SEX \n        SEX_factor_Fem = relevel(SEX_f, ref = '2'),\n        SEX_factor_Mal = relevel(SEX_f, ref = '1'),\n        # EMP\n        EMP_factor_not = relevel(EMP_f, ref = '0'),\n        EMP_factor_Emp = relevel(EMP_f, ref = '1'),\n        # FAM \n        FAM_factor_1 = relevel(FAM_f, ref = '1'),\n        FAM_factor_2 = relevel(FAM_f, ref = '2'),\n        FAM_factor_3 = relevel(FAM_f, ref = '3'),\n        FAM_factor_4 = relevel(FAM_f, ref = '4'),\n        # IMM\n        IMM_factor_Imm = relevel(IMM_f, ref = '1'),\n        IMM_factor_non = relevel(IMM_f, ref = '0'),\n        # SNS \n        SNS_factor_notuse = relevel(SNS_f, ref = '0'),\n        SNS_factor_use = relevel(SNS_f, ref = '1')\n    )\n\nSo, I believe there may be some variation due to the sampling method (clusters on provinces).\n\nds &lt;- ds %&gt;% \n    mutate(\n        province_f_coded = fct_recode(\n            PRVNC,\n            'NL' = '10',\n            'NS' = '12', \n            'NB' = '13',\n            'QC' = '24',\n            'ON' = '35', \n            'MB' = '46', \n            'SK' = '47', \n            'AB' = '48', \n            'BC' = '59'\n        )\n    )\n\n\nggplot(ds, aes(province_f_coded, mBanking, color = as.factor(MH))) +\n                  stat_summary(fun = mean, geom = \"point\") +\n                  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.4) +\n                  theme_set(theme_bw(base_size = 10)) +\n                  theme(legend.position = \"top\") +\n                  labs(x = \"Province\", y = \"Observed Probabilty of mobile banking\", color = \"MH\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nprov_ &lt;- c('NL', 'NS', 'NB', 'QC', 'ON', 'MB', 'SK', 'AB', 'BC')\nprov_n &lt;- c(10, 12, 13, 24, 35, 46, 47, 48, 59)\n\n# random effects are from model 2 \nranefs_ &lt;- c(-0.029445800, 0.001523515,-0.034782017, 0.308732844,-0.132427568,-0.129533645,-0.047807245,-0.001728474,0.053000416)\nranefs_ &lt;- round(ranefs_, 4)\n\nd_graph &lt;- cbind(prov_, prov_n, ranefs_)\nd_graph &lt;- as.data.frame(d_graph)\n\nprovs_fullnames &lt;- c('Newfoundland and Labrador', 'Nova Scotia', 'New Brunswick','Quebec', 'Ontario', 'Manitoba', 'Saskatchewan', 'Alberta', 'British Columbia')\n\n\nggplot(data = d_graph, aes(x = prov_n, y = ranefs_, label = c('NL', 'NS', 'NB', 'QC', 'ON', 'MB', 'SK', 'AB', 'BC'))) +   \n    geom_point(size = 2, alpha = .5) + \n    geom_text(check_overlap = TRUE) + \n    labs(\n        x = \"Province Code\",\n        y = \"Random Effect\",\n        fill = \"Province\"\n    ) + geom_label(aes(fill = provs_fullnames), colour = \"white\", fontface = \"bold\") + geom_line(linetype = \"dashed\") + \n    scale_color_manual(values = provs_fullnames, name = \"province\")\n\n\n\n\n\n\n\n\nFollowing the paper, I have these models:\n\nModel 1. Standard Logistic Regression with RS \n\\begin{equation*}\n\\begin{split}\n& \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\ \\beta_0 + \\beta_{1} \\ MH + \\ \\beta_2 \\ SD + \\ \\beta_3 \\ SNS + \\ \\beta_4 \\ AGE \\ + \\ \\beta_5 \\ SEX \\\\\n&  + \\ \\beta_6 \\ EMP \\ + \\ \\beta_7 \\ EDU + \\ \\beta_8 \\ INC \\ + \\ \\beta_9 \\ D_{FAM_1} + \\ \\beta_{10} \\ D_{FAM_3}\\\\\n& + \\ \\beta_{11} \\ D_{FAM_4} \\ + \\ \\beta_{12} \\ IMM + \\ \\beta_{13} \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\n\n\nmodel1 &lt;- glm(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c,\n    data = ds,\n    family = \"binomial\"\n)\n\n\nModel 2. Fixed Effect Logistic Regression \n\\begin{equation*}\n\\begin{split}\n& \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\\\\n& \\ \\gamma_{0,j} + u_{0,j} + \\beta_{1} \\ MH + \\ \\beta_2 \\ SD + \\ \\beta_3 \\ SNS + \\ \\beta_4 \\ AGE \\ + \\ \\beta_5 \\ SEX\\\\\n&  + \\ \\beta_6 \\ EMP \\ + \\ \\beta_7 \\ EDU + \\ \\beta_8 \\ INC \\ + \\ \\beta_9 \\ D_{FAM_1} + \\ \\beta_{10} \\ D_{FAM_3}\\\\\n& + \\ \\beta_{11} \\ D_{FAM_4} \\ + \\ \\beta_{12} \\ IMM + \\ \\beta_{13} \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\n\n\nmodel2 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 | province),\n    data = ds,\n    family = binomial(),\n    control = glmerControl(optimizer = \"bobyqa\")\n)\n\n\nModel 3. Random Effect Logistic Regression\n\n\n\\begin{equation*}\n\\begin{split}\n   & \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\\\\n   & \\ \\gamma_{0,0} + u_{0,j} + (\\gamma_{1,0} + u_{1,j}) \\ MH + \\ (\\gamma_{2,0} + u_{2,j}) \\ SD \\ + \\ (\\gamma_{3,0} + u_{3,j}) \\ SNS \\\\\n   & + \\ (\\gamma_{4,0} + u_{4,j}) \\ AGE \\ + \\ (\\gamma_{5,0} + u_{5,j}) \\ SEX \\ + \\ (\\gamma_{6,0} + u_{6,j}) EMP \\\\\n   & + \\ (\\gamma_{7,0} + u_{7,j}) \\ EDU \\ + \\ (\\gamma_{8,0} + u_{8,j}) \\ INC \\\n    + \\ (\\gamma_{9,0} + u_{9,j}) \\ D_{FAM_1} \\\\\n   & + \\ (\\gamma_{10,0} + u_{10,j}) \\ D_{FAM_3} \\ + \\ (\\gamma_{11,0} + u_{11,j}) \\ D_{FAM_4} \\\\\n   & + \\ (\\gamma_{12,0} + u_{12,j}) \\ IMM_1 \\ + \\ (\\gamma_{13,0} + u_{13,j}) \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\n\nmodel3 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + \n    (1 + MH_c + SD_c + SNS + RS_c + AGE_c + SEX_f + EMP + EDU_c + FAM_2 + IMM_n + \n    INC_c | province),\n    data = ds,\n    family = binomial(link = \"logit\"),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nFor faster speeds, I’ll test everything with this model (random):\n\nmodel4 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + \n    (1 + MH_c + SD_c + SNS_factor_use + RS_c | province),\n    data = ds,\n    family = binomial(link = \"logit\"),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nComparing models:\n\nLikelihood Ratio Test model1 vs model2\nIf model2 wins, we need the Hausman test to see if model3 is better\nIf model1 wins, we should just use that\n\n\ntest_performance(model1, model2)\n\nName   |    Model |     BF | df | df_diff |  Chi2 |      p\n----------------------------------------------------------\nmodel1 |      glm |        | 14 |         |       |       \nmodel2 | glmerMod | 333.29 | 15 |    1.00 | 20.94 | &lt; .001\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\nHausman test for fixed effects in R is a bit tricky, so, I did it manually following the formula:\n\nExtract per-group (province) coefficients for both models and convert them to data frames\n\n\ncoefs_fixed &lt;- coef(model2)\ncoefs_rando &lt;- coef(model4)\n\ncoefs_fixed_df &lt;- as.data.frame(coefs_fixed$province)\ncoefs_rando_df &lt;- as.data.frame(coefs_rando$province)\n\n\nSubtract Random Effects model coefficients from Fixed Effect model, per province. This gives you the core term of the Hausman test:\n\n\ncoefs_diff &lt;- coefs_fixed_df - coefs_rando_df\n\ncoef_diffs_matrix &lt;- as.matrix(coefs_diff)\n\n\nEstimate the difference in the variance-covariance matrices of the coefficient estimates.\n\n\nV_diff &lt;- as.matrix(vcov(model2) - vcov(model4))\n\nWhat I need is:\n\nH = (\\hat{\\beta_{F}} - \\hat{\\beta_{R}}) \\cdot V^{-1} \\cdot (\\hat{\\beta_{F}} - \\hat{\\beta_{R}})^T\n\nJust checking that the matrix multiplications make sense:\n\ndim(t(coef_diffs_matrix))\n\n[1] 14 10\n\ndim(solve(V_diff))\n\n[1] 14 14\n\ndim(coef_diffs_matrix)\n\n[1] 10 14\n\n\nThey do! So, calculate H:\n\nH &lt;- coef_diffs_matrix %*% solve(V_diff) %*% t(coef_diffs_matrix)\nH\n\n           10          11          12          13         24         35\n10 -0.9216314 -1.14821418 -0.62853586 -1.27216633 -0.9614259 -1.1000217\n11 -1.1482142  0.06629823 -1.36003254 -0.52961709  1.1793393 -1.5173996\n12 -0.6285359 -1.36003254 -1.82447922  0.04347552 -3.1847200  0.4852484\n13 -1.2721663 -0.52961709  0.04347552 -2.20670478  2.2638636 -2.8475033\n24 -0.9614259  1.17933926 -3.18472004  2.26386362 -1.8142023  1.1087512\n35 -1.1000217 -1.51739956  0.48524845 -2.84750329  1.1087512 -2.7374940\n46 -0.5906471 -2.26035734 -1.59868327 -1.09977349 -1.5358419 -0.7031878\n47 -0.3433758 -2.20315977 -2.31495024  0.30606569 -4.8434098  1.2976268\n48 -0.6326137 -1.46623618 -1.50959981 -0.17644778 -3.4875978  0.4707913\n59 -0.6693772 -0.92849590 -2.53030848  0.56902412 -2.1646448  0.4002808\n           46         47         48         59\n10 -0.5906471 -0.3433758 -0.6326137 -0.6693772\n11 -2.2603573 -2.2031598 -1.4662362 -0.9284959\n12 -1.5986833 -2.3149502 -1.5095998 -2.5303085\n13 -1.0997735  0.3060657 -0.1764478  0.5690241\n24 -1.5358419 -4.8434098 -3.4875978 -2.1646448\n35 -0.7031878  1.2976268  0.4707913  0.4002808\n46 -4.4001880 -2.6590794 -0.7335810 -3.5244974\n47 -2.6590794 -3.1458358 -1.5682187 -4.0309963\n48 -0.7335810 -1.5682187 -1.2939189 -2.0536598\n59 -3.5244974 -4.0309963 -2.0536598 -3.4398903\n\n\nThis is the actual critical \\chi^2 value at degrees of freedom 13 (for 14 covariates), in fact, I can check:\n\nqr(V_diff)$rank\n\n[1] 14\n\n\n\nchisq_critical &lt;- qchisq(p = .05, df = 13, lower.tail = FALSE)\nchisq_critical\n\n[1] 22.36203\n\n\nIf H1 &gt; \\chi^2 then reject the null hypothesis that says the fixed model is better.\n\nH &gt; chisq_critical #reject H0: the fixed model is better.  \n\n      10    11    12    13    24    35    46    47    48    59\n10 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n11 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n12 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n13 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n24 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n35 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n46 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n47 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n48 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n59 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nThe p-value:\n\npchisq(H, df = 13, lower.tail = FALSE)\n\n   10        11 12        13        24        35 46        47 48        59\n10  1 1.0000000  1 1.0000000 1.0000000 1.0000000  1 1.0000000  1 1.0000000\n11  1 1.0000000  1 1.0000000 0.9999896 1.0000000  1 1.0000000  1 1.0000000\n12  1 1.0000000  1 1.0000000 1.0000000 1.0000000  1 1.0000000  1 1.0000000\n13  1 1.0000000  1 1.0000000 0.9995475 1.0000000  1 1.0000000  1 0.9999999\n24  1 0.9999896  1 0.9995475 1.0000000 0.9999928  1 1.0000000  1 1.0000000\n35  1 1.0000000  1 1.0000000 0.9999928 1.0000000  1 0.9999816  1 1.0000000\n46  1 1.0000000  1 1.0000000 1.0000000 1.0000000  1 1.0000000  1 1.0000000\n47  1 1.0000000  1 1.0000000 1.0000000 0.9999816  1 1.0000000  1 1.0000000\n48  1 1.0000000  1 1.0000000 1.0000000 1.0000000  1 1.0000000  1 1.0000000\n59  1 1.0000000  1 0.9999999 1.0000000 1.0000000  1 1.0000000  1 1.0000000\n\n\nOk, we can’t reject this hypothesis - therefore, the fixed model is better. Another way to check:\n\nanova(model2, model4)\n\nData: ds\nModels:\nmodel2: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 | province)\nmodel4: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 + MH_c + SD_c + SNS_factor_use + RS_c | province)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmodel2   15 8176.0 8285.8 -4073.0   8146.0                     \nmodel4   29 8202.8 8415.1 -4072.4   8144.8 1.2569 14          1\n\n\nOk, best model is model2. Now adding interaction terms:\n\nmodel2_int &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem \n    + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c \n    + MH_c:RS_c + MH_c:SD_c + MH_c:SNS_factor_use \n    + (1 | province),\n    data = ds,\n    family = binomial(),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nPrinting both odds ratios and log-odds versions:\n\nsumm(\n    model2,\n    scale = F,\n    pvals = T,\n    exp = T, \n    digits = 3,\n    #part.corr = T, #Print partial (labeled \"partial.r\") and semipartial (labeled \"part.r\")\n    #confint = getOption(\"summ-confint\", FALSE),\n    #ci.width = getOption(\"summ-ci.width\", 0.95),\n    #vifs = T\n)\n\n\n\n\n\nObservations\n11176\n\n\nDependent variable\nmBanking\n\n\nType\nMixed effects generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\nAIC\n8176.019\n\n\nBIC\n8285.842\n\n\nPseudo-R² (fixed effects)\n0.143\n\n\nPseudo-R² (total)\n0.148\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nexp(Est.)\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n13.580\n0.081\n32.201\n0.000\n\n\nMH_c\n0.920\n0.030\n-2.753\n0.006\n\n\nSD_c\n1.195\n0.027\n6.622\n0.000\n\n\nSNS_factor_use0\n0.377\n0.064\n-15.308\n0.000\n\n\nRS_c\n1.020\n0.018\n1.084\n0.279\n\n\nAGE_c\n1.026\n0.024\n1.084\n0.278\n\n\nSEX_factor_Fem1\n0.893\n0.059\n-1.924\n0.054\n\n\nEMP_factor_Emp0\n0.557\n0.063\n-9.313\n0.000\n\n\nEDU_c\n1.570\n0.039\n11.661\n0.000\n\n\nFAM_factor_21\n0.786\n0.079\n-3.031\n0.002\n\n\nFAM_factor_23\n0.947\n0.078\n-0.702\n0.483\n\n\nFAM_factor_24\n1.006\n0.186\n0.034\n0.973\n\n\nIMM_factor_non1\n0.810\n0.094\n-2.256\n0.024\n\n\nINC_c\n1.050\n0.024\n2.013\n0.044\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nprovince\n(Intercept)\n0.142\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\n\nprovince\n10\n0.006\n\n\n\n\n\n\n\nsummary(model2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nmBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem +  \n    EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non +  \n    INC_c + (1 | province)\n   Data: ds\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  8176.0   8285.8  -4073.0   8146.0    11161 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-16.2213   0.2326   0.3067   0.4159   1.1380 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n province (Intercept) 0.02003  0.1415  \nNumber of obs: 11176, groups:  province, 10\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      2.608591   0.081009  32.201  &lt; 2e-16 ***\nMH_c            -0.082966   0.030135  -2.753  0.00590 ** \nSD_c             0.178056   0.026889   6.622 3.54e-11 ***\nSNS_factor_use0 -0.974642   0.063671 -15.308  &lt; 2e-16 ***\nRS_c             0.019430   0.017930   1.084  0.27852    \nAGE_c            0.025561   0.023579   1.084  0.27834    \nSEX_factor_Fem1 -0.113372   0.058935  -1.924  0.05439 .  \nEMP_factor_Emp0 -0.586068   0.062928  -9.313  &lt; 2e-16 ***\nEDU_c            0.450903   0.038666  11.661  &lt; 2e-16 ***\nFAM_factor_21   -0.240512   0.079361  -3.031  0.00244 ** \nFAM_factor_23   -0.054847   0.078169  -0.702  0.48290    \nFAM_factor_24    0.006349   0.186493   0.034  0.97284    \nIMM_factor_non1 -0.210966   0.093525  -2.256  0.02409 *  \nINC_c            0.048330   0.024007   2.013  0.04410 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsumm(\n    model2_int,\n    scale = F,\n    pvals = T,\n    exp = T, \n    digits = 3,\n    #part.corr = T, #Print partial (labeled \"partial.r\") and semipartial (labeled \"part.r\")\n    #confint = getOption(\"summ-confint\", FALSE),\n    #ci.width = getOption(\"summ-ci.width\", 0.95),\n    #vifs = T\n)\n\n\n\n\n\nObservations\n11176\n\n\nDependent variable\nmBanking\n\n\nType\nMixed effects generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\nAIC\n8178.438\n\n\nBIC\n8310.225\n\n\nPseudo-R² (fixed effects)\n0.144\n\n\nPseudo-R² (total)\n0.149\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nexp(Est.)\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n13.584\n0.081\n32.058\n0.000\n\n\nMH_c\n0.888\n0.036\n-3.246\n0.001\n\n\nSD_c\n1.193\n0.027\n6.560\n0.000\n\n\nSNS_factor_use0\n0.372\n0.064\n-15.449\n0.000\n\n\nRS_c\n1.020\n0.018\n1.081\n0.279\n\n\nAGE_c\n1.026\n0.024\n1.104\n0.269\n\n\nSEX_factor_Fem1\n0.894\n0.059\n-1.900\n0.057\n\n\nEMP_factor_Emp0\n0.556\n0.063\n-9.305\n0.000\n\n\nEDU_c\n1.570\n0.039\n11.667\n0.000\n\n\nFAM_factor_21\n0.787\n0.079\n-3.023\n0.003\n\n\nFAM_factor_23\n0.948\n0.078\n-0.688\n0.491\n\n\nFAM_factor_24\n1.003\n0.187\n0.017\n0.987\n\n\nIMM_factor_non1\n0.812\n0.094\n-2.228\n0.026\n\n\nINC_c\n1.049\n0.024\n2.011\n0.044\n\n\nMH_c:RS_c\n1.002\n0.016\n0.116\n0.908\n\n\nMH_c:SD_c\n1.016\n0.023\n0.685\n0.493\n\n\nMH_c:SNS_factor_use0\n1.121\n0.061\n1.874\n0.061\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nprovince\n(Intercept)\n0.142\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\n\nprovince\n10\n0.006\n\n\n\n\n\n\n\nsummary(model2_int)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nmBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem +  \n    EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non +  \n    INC_c + MH_c:RS_c + MH_c:SD_c + MH_c:SNS_factor_use + (1 |      province)\n   Data: ds\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  8178.4   8310.2  -4071.2   8142.4    11158 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-16.1594   0.2314   0.3066   0.4157   1.1163 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n province (Intercept) 0.0201   0.1418  \nNumber of obs: 11176, groups:  province, 10\n\nFixed effects:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           2.608913   0.081381  32.058  &lt; 2e-16 ***\nMH_c                 -0.118271   0.036438  -3.246  0.00117 ** \nSD_c                  0.176551   0.026915   6.560 5.39e-11 ***\nSNS_factor_use0      -0.989512   0.064050 -15.449  &lt; 2e-16 ***\nRS_c                  0.019530   0.018059   1.081  0.27950    \nAGE_c                 0.026100   0.023634   1.104  0.26944    \nSEX_factor_Fem1      -0.112071   0.058973  -1.900  0.05738 .  \nEMP_factor_Emp0      -0.586152   0.062994  -9.305  &lt; 2e-16 ***\nEDU_c                 0.451151   0.038668  11.667  &lt; 2e-16 ***\nFAM_factor_21        -0.240028   0.079401  -3.023  0.00250 ** \nFAM_factor_23        -0.053828   0.078218  -0.688  0.49134    \nFAM_factor_24         0.003118   0.186707   0.017  0.98668    \nIMM_factor_non1      -0.208543   0.093581  -2.228  0.02585 *  \nINC_c                 0.048288   0.024016   2.011  0.04436 *  \nMH_c:RS_c             0.001805   0.015611   0.116  0.90795    \nMH_c:SD_c             0.016050   0.023429   0.685  0.49332    \nMH_c:SNS_factor_use0  0.114107   0.060903   1.874  0.06099 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlso calculating the confidence interval for the variances of each model:\n\nround(confint(model2),3)\n\n\nAnd the random effects for provinces (for visualization):\n\nranef(model2)\n\n$province\n    (Intercept)\n10 -0.035824280\n11  0.075928582\n12 -0.001101379\n13 -0.034281082\n24  0.287969394\n35 -0.133822629\n46 -0.139594708\n47 -0.060600346\n48 -0.012451359\n59  0.040405663\n\nwith conditional variances for \"province\" \n\n\n\n\nMarginal Effects\nFirst, let’s see which model is better:\n\nanova(model2, model2_int)\n\nData: ds\nModels:\nmodel2: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 | province)\nmodel2_int: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + MH_c:RS_c + MH_c:SD_c + MH_c:SNS_factor_use + (1 | province)\n           npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmodel2       15 8176.0 8285.8 -4073.0   8146.0                     \nmodel2_int   18 8178.4 8310.2 -4071.2   8142.4 3.5812  3     0.3104\n\n\n\ntest_performance(model2, model2_int)\n\nName       |    Model |      BF | df | df_diff | Chi2 |     p\n-------------------------------------------------------------\nmodel2     | glmerMod |         | 15 |         |      |      \nmodel2_int | glmerMod | &lt; 0.001 | 18 |    3.00 | 3.58 | 0.310\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\n\nprint(predict_response(model2,\n      terms = c(\"MH_c\", \"province\"), margin = \"empirical\"), \n      n = Inf)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2.html#whats-the-connection",
    "href": "study2.html#whats-the-connection",
    "title": "Mental Health and Mobile Banking",
    "section": "",
    "text": "In this chapter, I look into how mental health might affect whether people choose to adopt mobile banking or not. While doing my review of past studies (refer to study 1 ), I noticed that a lot of research talks about how things like trust, risk, and perceived ease of use affect adoption. But very few studies look at how a person’s mental and emotional state influences their behavior. Especially because some studies actively showed how not accounting for people’s emotions were sometiems giving them strange results.\nI also found that even if the literature did talk about emotions, it was mostly about emotions as a result of the adoption/use of a technology. Not about how your general mood or feelings or struggles impact how you behave and interact with technology. Like I know when I’m stressed out or dreading something, the doom scrolling gets worse! But social media apps are built to be addictive and drag you in and make you want it more. Until you actually stop enjoying it and being online is no longer fun. In fact, I read just recently how engagement and use of social media apps has dropped in recent years! I’ll find the citation for it later! Anyway, I digress… Point is, I think your mood definitely affects how you interact with the world and technology is part of that world. So, what about people who have special moods? That is, certain behavioral or mental challenges? Not exactly disabilities, but hidden challenges often ignored!\nI wanted to explore whether people who are feeling mentally well are more or less likely to use mobile banking. My guess going in was that people with better mental health might feel more confident and more open to using this tech. But I found out it was actually the opposite! Then I got to thinking… why is that? Well, Here’s my guess:\n\nMaybe people with poorer mental health actually rely on mobile banking to avoid going out and talking to people, or dealing with stress in traditional banking.\nMaybe people who are happy don’t bank and don’t stress about money!\nIt’s not a causation, and I don’t know which comes first - are people who bank less happier or is it that happy people don’t bank as often? This is not the study to answer that. But that would be a cool study!\n\nI used Canadian survey data and looked at three things related to mental health:\n\nHow satisfied are you in your relationships (RS)\nHow dependent are you on your smartphones (SD)\nWhether or not you use social media (SNS)\n\nI also tested whether these three things change (or “moderate”) how mental health affects mobile banking adoption. This chapter is where I introduce mental health as a new factor in mobile banking adoption models. I show how mental and emotional states might be just as important as the usual stuff like trust or app features. The results surprised me, and hopefully, they’ll give banks and researchers something new to think about too.\nBy the way, this chapter was already published in .",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci",
    "href": "study2_DA.html#mh_c-predicted-95-ci",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.87 | 0.86, 0.89 -0.59 | 0.86 | 0.85, 0.88 0.41 | 0.85 | 0.84, 0.87 1.41 | 0.84 | 0.83, 0.86\nprovince: 12",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-1",
    "href": "study2_DA.html#mh_c-predicted-95-ci-1",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.89 | 0.87, 0.90 -1.59 | 0.88 | 0.86, 0.89 -0.59 | 0.87 | 0.86, 0.88 0.41 | 0.86 | 0.85, 0.87 1.41 | 0.85 | 0.83, 0.87\nprovince: 13",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-2",
    "href": "study2_DA.html#mh_c-predicted-95-ci-2",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.87 | 0.85, 0.89 -0.59 | 0.86 | 0.85, 0.88 0.41 | 0.85 | 0.84, 0.87 1.41 | 0.84 | 0.82, 0.86\nprovince: 24",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-3",
    "href": "study2_DA.html#mh_c-predicted-95-ci-3",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.91 | 0.90, 0.93 -1.59 | 0.90 | 0.89, 0.92 -0.59 | 0.90 | 0.89, 0.91 0.41 | 0.89 | 0.88, 0.90 1.41 | 0.88 | 0.87, 0.90\nprovince: 35",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-4",
    "href": "study2_DA.html#mh_c-predicted-95-ci-4",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.87 | 0.85, 0.89 -1.59 | 0.86 | 0.84, 0.88 -0.59 | 0.85 | 0.84, 0.87 0.41 | 0.84 | 0.83, 0.86 1.41 | 0.83 | 0.81, 0.85\nprovince: 46",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-5",
    "href": "study2_DA.html#mh_c-predicted-95-ci-5",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.87 | 0.85, 0.89 -1.59 | 0.86 | 0.85, 0.88 -0.59 | 0.85 | 0.84, 0.87 0.41 | 0.84 | 0.83, 0.86 1.41 | 0.83 | 0.82, 0.85\nprovince: 47",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-6",
    "href": "study2_DA.html#mh_c-predicted-95-ci-6",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.87 | 0.85, 0.89 -0.59 | 0.86 | 0.85, 0.88 0.41 | 0.85 | 0.84, 0.87 1.41 | 0.84 | 0.82, 0.86\nprovince: 48",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-7",
    "href": "study2_DA.html#mh_c-predicted-95-ci-7",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.88 | 0.86, 0.89 -0.59 | 0.87 | 0.85, 0.88 0.41 | 0.86 | 0.84, 0.87 1.41 | 0.85 | 0.83, 0.87\nprovince: 59",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-8",
    "href": "study2_DA.html#mh_c-predicted-95-ci-8",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.89 | 0.87, 0.91 -1.59 | 0.88 | 0.87, 0.90 -0.59 | 0.87 | 0.86, 0.89 0.41 | 0.86 | 0.85, 0.88 1.41 | 0.85 | 0.84, 0.87\nMargins at different provinces:\n\nhead(margins_summary(model2, \n     at = list(province = c(10, 12, 13, 24, 35, 46, 47, 48, 59))))\n\n factor province    AME     SE      z      p   lower  upper\n  AGE_c  10.0000 0.0029 0.0027 1.0837 0.2785 -0.0023 0.0081\n  AGE_c  12.0000 0.0028 0.0026 1.0837 0.2785 -0.0023 0.0079\n  AGE_c  13.0000 0.0029 0.0027 1.0837 0.2785 -0.0023 0.0081\n  AGE_c  24.0000 0.0023 0.0021 1.0835 0.2786 -0.0019 0.0065\n  AGE_c  35.0000 0.0031 0.0028 1.0838 0.2784 -0.0025 0.0086\n  AGE_c  46.0000 0.0031 0.0028 1.0838 0.2784 -0.0025 0.0086\n\n\nSummary Table of Margins:\n\nmargins_summary(model2)\n\n          factor     AME     SE        z      p   lower   upper\n           AGE_c  0.0028 0.0026   1.0838 0.2785 -0.0022  0.0078\n           EDU_c  0.0490 0.0044  11.1029 0.0000  0.0403  0.0576\n EMP_factor_Emp0 -0.0663 0.0076  -8.6816 0.0000 -0.0813 -0.0513\n   FAM_factor_21 -0.0269 0.0091  -2.9500 0.0032 -0.0447 -0.0090\n   FAM_factor_23 -0.0058 0.0083  -0.6964 0.4862 -0.0220  0.0105\n   FAM_factor_24  0.0007 0.0192   0.0341 0.9728 -0.0370  0.0383\n IMM_factor_non1 -0.0241 0.0113  -2.1415 0.0322 -0.0462 -0.0020\n           INC_c  0.0053 0.0026   2.0105 0.0444  0.0001  0.0104\n            MH_c -0.0090 0.0033  -2.7429 0.0061 -0.0155 -0.0026\n            RS_c  0.0021 0.0020   1.0827 0.2790 -0.0017  0.0059\n            SD_c  0.0193 0.0030   6.5080 0.0000  0.0135  0.0252\n SEX_factor_Fem1 -0.0123 0.0064  -1.9175 0.0552 -0.0250  0.0003\n SNS_factor_use0 -0.1254 0.0100 -12.5093 0.0000 -0.1450 -0.1057",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  }
]